
from globals import normalize_path
from langchain_community.embeddings import SparkLLMTextEmbeddings
from langchain_openai import OpenAIEmbeddings

from llm.call_llm import parse_llm_api_key
from llm.call_llm import parse_llm_api_base
from globals import EMBEDDING_MODEL_DICT, models_cache, default_model_dir
from langchain_community.embeddings import BaichuanTextEmbeddings
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_huggingface import HuggingFaceEmbeddings
import torch
import subprocess, os
from embedding.HuggingFaceEmbeddings2 import HuggingFaceEmbeddings2
from utils.checkPort import getValidPort
import time
from globals import OLLAMA_PORT_MAP
from langchain_ollama import OllamaEmbeddings
from llm.model_to_llm import download_ollama_model,download_huggingface_model

def get_embedding(embedding_type:str=None,embedding: str=None, embedding_key: str=None,embedding_base:str=None,
                  spark_app_id:str=None, spark_api_secret:str=None,
                  wenxin_secret:str=None,  embedding_dir:str=None):
    if embedding_type == "HuggingFaceEmbedding":
    # ä½¿ç”¨HuggingFaceæœ¬åœ°embeddingæ¨¡å‹
        embedding=get_huggingface_embedding(embedding, embedding_dir)
        return embedding, \
        embedding_key, embedding_base, spark_app_id, spark_api_secret, wenxin_secret

    elif embedding_type == "OllamaEmbedding":
# ä½¿ç”¨Ollamaæœ¬åœ°embeddingæ¨¡å‹
        embedding=get_ollama_embedding(embedding, embedding_dir)

        return embedding, \
            embedding_key, embedding_base, spark_app_id, spark_api_secret, wenxin_secret

    if not embedding_key: #""å’ŒNoneéƒ½å¯ä»¥

        if embedding_type == "SPARK":
            spark_app_id, spark_api_secret, embedding_key = parse_llm_api_key(embedding_type, embedding)
            wenxin_secret=""
        elif embedding_type=="WENXIN":
            embedding_key,wenxin_secret = parse_llm_api_key(embedding_type,embedding)

            spark_app_id = ""
            spark_api_secret = ""
        else:
            embedding_key = parse_llm_api_key(embedding_type, embedding)
            spark_app_id = ""
            spark_api_secret = ""
            wenxin_secret = ""


    if not embedding_base  :
        embedding_base = parse_llm_api_base(embedding_type,embedding)


    # å› ä¸ºSPARKå’ŒOPENAIåªæ”¯æŒ ä¸€ç§å…è´¹çš„åµŒå…¥æ¨¡å‹è°ƒç”¨ï¼Œæ‰€ä»¥è¿™é‡Œå•ç‹¬å†™ï¼Œè€Œä¸ä¼ å…¥æ¨¡å‹åç§°ï¼›
    #å› ä¸ºå»ºç«‹æ•°æ®åº“æ—¶ï¼Œåœ¨å…ƒæ•°æ®ä¿å­˜äº†keyçš„ä¿¡æ¯ï¼Œä¾¿äºä¸‹æ¬¡è°ƒç”¨ï¼›æ‰€ä»¥è¿™é‡Œè¿”å›keyæ˜¯ä¸ºäº†create_dbçš„add_Filesï¼Œè·å–keyä¿¡æ¯ã€‚
    if embedding_type == "SPARK":
        return SparkLLMTextEmbeddings(spark_app_id=spark_app_id, spark_api_secret=spark_api_secret,
                                      spark_api_key=embedding_key,base_url=embedding_base),\
            embedding_key,embedding_base,spark_app_id,spark_api_secret,wenxin_secret
    elif embedding_type =="OPENAI":
        return OpenAIEmbeddings(model=embedding,openai_api_key=embedding_key,
                                openai_api_base=embedding_base),\
            embedding_key,embedding_base,spark_app_id,spark_api_secret, wenxin_secret
    elif embedding_type =="BAICHUAN":
      #åªæ”¯æŒé»˜è®¤çš„url
        return BaichuanTextEmbeddings(
            baichuan_api_key=embedding_key,
            model_name=embedding,
        ),\
            embedding_key,embedding_base,spark_app_id,spark_api_secret,wenxin_secret
    elif embedding_type =="QWEN":
        #åªæ”¯æŒé»˜è®¤url
        return  DashScopeEmbeddings(
            model=embedding, dashscope_api_key=embedding_key
        ),\
            embedding_key,embedding_base,spark_app_id,spark_api_secret,wenxin_secret

    else:
        return OpenAIEmbeddings(
            model=embedding,
            openai_api_key=embedding_key,
            openai_api_base=embedding_base),\
            embedding_key,embedding_base,spark_app_id,spark_api_secret,wenxin_secret








def download_embedding(
    embedding_type: str = "OllamaEmbedding",
    embedding: str = None,
    embedding_dir: str = None,

):

    """
    é€šç”¨ Embedding æ¨¡å‹ä¸‹è½½å‡½æ•°
    æ”¯æŒ ModelScopeã€HuggingFaceã€Ollama

    Args:
        embedding_type: ["ModelScope", "HuggingFace", "OllamaEmbedding"]
        embedding: æ¨¡å‹åç§°ï¼Œä¾‹å¦‚ "mxbai-embed-large" æˆ– "iic/nlp_gte_sentence-embedding-base-zh"
        embedding_dir: æ¨¡å‹ä¸‹è½½ä¿å­˜è·¯å¾„
        port: Ollama æœåŠ¡ç«¯å£ï¼ˆä»… Ollama æ¨¡å‹ç”¨ï¼‰

    Returns:
        str: æœ¬åœ°æ¨¡å‹è·¯å¾„
    """
    from modelscope import snapshot_download
    if embedding is None:
        raise ValueError("embedding å‚æ•°ä¸èƒ½ä¸ºç©º")
    if embedding_type == "HuggingFaceEmbedding":
        return download_huggingface_model(embedding,embedding_dir)

    # ------------------- Ollama -------------------
    elif embedding_type=="OllamaEmbedding":
        return download_ollama_model(embedding,embedding_dir)


def get_huggingface_embedding(model_name: str, model_dir: str = None):
    """
    è·å–HuggingFaceæœ¬åœ°embeddingæ¨¡å‹
    
    Args:
        model_name: æ¨¡å‹åç§°
        model_dir: è‡ªå®šä¹‰æ¨¡å‹ç›®å½•
        is_download: æ˜¯å¦è‡ªåŠ¨ä¸‹è½½æ¨¡å‹
        
    Returns:
        HuggingFaceEmbeddings: embeddingæ¨¡å‹å®ä¾‹
    """
    try:
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç¼“å­˜
        model_dir = normalize_path(model_dir)
        if model_name in models_cache["HuggingFaceEmbedding"].get(model_dir,{}):
            return models_cache["HuggingFaceEmbedding"][model_dir][model_name][0]

        # åˆ›å»ºembeddingæ¨¡å‹
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model_path=os.path.join(model_dir,model_name)

        #ä½¿ç”¨é‡æ–°å°è£…çš„HuggingFaceEmbeddings2ï¼Œæ˜¯å› ä¸ºChromaæ•°æ®åº“åˆå§‹åŒ–å¿…é¡»ä¼ å…¥embeddingFunctionï¼Œ
        # ç›´æ¥ç”¨HuggingFaceEmbeddingsä¼šç«‹åˆ»åŠ è½½æ¨¡å‹ï¼Œå ç”¨å¤§é‡æ˜¾å­˜
        #è¿™é‡Œä½¿ç”¨é‡æ–°å°è£…çš„HuggingFaceEmbeddings2ï¼Œåªæœ‰åœ¨çœŸæ­£ä½¿ç”¨æ—¶æ¨¡å‹æ‰ä¼šåŠ è½½
        embedding_model = HuggingFaceEmbeddings2(
            model_name=model_path,
            model_kwargs={'device': device},
            encode_kwargs={'normalize_embeddings': True}  # æ ‡å‡†åŒ–embeddingå‘é‡
        )

        return embedding_model
        
    except Exception as e:
        raise ValueError(f"åŠ è½½HuggingFace embeddingæ¨¡å‹å¤±è´¥: {str(e)}")



def get_ollama_embedding(model: str,model_dir

                       ):
    """
    å¯åŠ¨æŒ‡å®šç›®å½•ä¸‹çš„ Ollama æœåŠ¡å¹¶è¿”å› Embedding æ¨¡å‹å®ä¾‹

    Args:
        model: Ollama embedding æ¨¡å‹åç§° (ä¾‹å¦‚ "nomic-embed-text")
        model_dir: æ¨¡å‹å­˜æ”¾è·¯å¾„
        port: èµ·å§‹ç«¯å£å· (é»˜è®¤ 11510)
        base_port: ç”¨äºè‡ªåŠ¨åˆ†é…ç«¯å£çš„èµ·å§‹å€¼
        max_port: è‡ªåŠ¨ç«¯å£åˆ†é…çš„æœ€å¤§èŒƒå›´

    Returns:
        OllamaEmbeddings å®ä¾‹
    """

    env = os.environ.copy()
    model_dir=normalize_path(model_dir)
    if model in models_cache["OllamaEmbedding"].get(model_dir, {}):
        return models_cache["OllamaEmbedding"][model_dir ][model][0]
    else:
        # --- Step 1. æ£€æŸ¥è¯¥ç›®å½•æ˜¯å¦å·²æœ‰è®°å½•çš„ç«¯å£ ---
        if model_dir in OLLAMA_PORT_MAP:
            port = OLLAMA_PORT_MAP[model_dir]

        else:
            # --- Step 2. è‡ªåŠ¨åˆ†é…ç©ºé—²ç«¯å£ ---
            port=getValidPort(port=11500,max_port=11600)
            # --- Step 3. å¯åŠ¨æ–°çš„ Ollama æœåŠ¡ ---
            print(f"ğŸš€ å¯åŠ¨æ–°çš„ Ollama æœåŠ¡ï¼ˆæ¨¡å‹ç›®å½•: {model_dir}, ç«¯å£: {port}ï¼‰")
            env["OLLAMA_MODELS"] = model_dir
            env["OLLAMA_HOST"] = f"127.0.0.1:{port}"
            OLLAMA_PORT_MAP[model_dir] = port
            subprocess.Popen(["ollama", "serve"], env=env,
                             stdout=subprocess.DEVNULL,  # å±è”½æ ‡å‡†è¾“å‡º
                             stderr=subprocess.DEVNULL  # å±è”½é”™è¯¯è¾“å‡º
                             )
            time.sleep(2)  # ç­‰å¾…æœåŠ¡å¯åŠ¨

        base_url = f"127.0.0.1:{port}"
        os.environ["OLLAMA_HOST"] = base_url  # ä¼˜å…ˆçº§æ¯”è¾“å…¥å‚æ•°base_urlæ›´é«˜
        embedding_model = OllamaEmbeddings(
            model=model,
            base_url=base_url
        )
            # å¯åŠ¨ä¸€ä¸ªåå°è¿›ç¨‹å¹¶ä¿æŒè¿è¡Œ


        return embedding_model



