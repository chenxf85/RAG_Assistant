2025-03-25 20:34:07,370 - root - ERROR - �ο͵�¼��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\user\log_in.py", line 47, in guest_login
    state["logged_in"] = True
TypeError: 'str' object does not support item assignment
-guest_login
2025-03-25 20:34:48,372 - root - ERROR - �ο͵�¼��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\user\log_in.py", line 47, in guest_login
    state["logged_in"] = True
TypeError: 'str' object does not support item assignment
-guest_login
2025-03-26 12:05:06,121 - root - ERROR - ע�ᷢ������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\user\sign_up.py", line 38, in register
    users = load_users()
  File "B:\notebook\Rag_Assistant\project\user\sign_up.py", line 19, in load_users
    return json.load(f)
  File "D:\Anaconda\lib\json\__init__.py", line 293, in load
    return loads(fp.read(),
  File "D:\Anaconda\lib\json\__init__.py", line 346, in loads
    return _default_decoder.decode(s)
  File "D:\Anaconda\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "D:\Anaconda\lib\json\decoder.py", line 355, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
-register
2025-03-26 12:05:34,217 - root - ERROR - ע�ᷢ������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\user\sign_up.py", line 38, in register
    users = load_users()
  File "B:\notebook\Rag_Assistant\project\user\sign_up.py", line 19, in load_users
    return json.load(f)
  File "D:\Anaconda\lib\json\__init__.py", line 293, in load
    return loads(fp.read(),
  File "D:\Anaconda\lib\json\__init__.py", line 346, in loads
    return _default_decoder.decode(s)
  File "D:\Anaconda\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "D:\Anaconda\lib\json\decoder.py", line 355, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
-register
2025-03-26 12:14:29,392 - root - ERROR - ע�ᷢ������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\user\sign_up.py", line 38, in register
    users = load_users()
  File "B:\notebook\Rag_Assistant\project\user\sign_up.py", line 19, in load_users
    return json.load(f)
  File "D:\Anaconda\lib\json\__init__.py", line 293, in load
    return loads(fp.read(),
  File "D:\Anaconda\lib\json\__init__.py", line 346, in loads
    return _default_decoder.decode(s)
  File "D:\Anaconda\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "D:\Anaconda\lib\json\decoder.py", line 355, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
-register
2025-05-08 14:02:57,564 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 69, in chat_qa_chain_self_answer
    for chunk in output_stream:
TypeError: 'async_generator' object is not iterable
-chat_qa_chain_self_answer
2025-05-08 14:04:46,380 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 70, in chat_qa_chain_self_answer
    yield chunk, chain.history
AttributeError: 'Chat_QA_chain_self' object has no attribute 'history'
-chat_qa_chain_self_answer
2025-05-08 14:21:32,261 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 69, in chat_qa_chain_self_answer
    async for chunk in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 220, in answer_with_history
    async for chunk in convo_qa_chain.astream({
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 624, in _atransform
    async for chunk in for_passthrough:
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\branch.py", line 454, in astream
    async for chunk in self.default.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 900, in _astream
    response = await self.async_client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 1927, in create
    return await self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1562, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'bad response status code 400    (request id: B20250508142120137451162xxCkCUXW) (request id: 2025050814212044557943bAiL16bM) (request id: 20250508142119905556740IucXnrdn)', 'type': 'new_api_error', 'param': '400', 'code': 'bad_response_status_code'}}
-chat_qa_chain_self_answer
2025-05-08 14:22:36,659 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 69, in chat_qa_chain_self_answer
    async for chunk in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 220, in answer_with_history
    async for chunk in convo_qa_chain.astream({
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 624, in _atransform
    async for chunk in for_passthrough:
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\branch.py", line 454, in astream
    async for chunk in self.default.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 900, in _astream
    response = await self.async_client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 1927, in create
    return await self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1547, in _request
    return await self._retry_request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1594, in _retry_request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1547, in _request
    return await self._retry_request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1594, in _retry_request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1562, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.InternalServerError: Error code: 503 - {'error': {'message': '��ǰ���� default �¶���ģ�� o1 �޿������� (request id: 20250508142236479262396QtJBU463) (request id: 20250508142236244854549QqmhqDx3)', 'type': 'new_api_error', 'param': '', 'code': None}}
-chat_qa_chain_self_answer
2025-05-08 14:31:16,493 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-1259' coro=<py_anext.<locals>.anext_impl() done, defined at D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py:71> exception=KeyError('context')>-default_exception_handler
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4988, in atransform
    async for output in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4952, in _atransform
    output = await acall_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4927, in f
    return await run_in_executor(config, func, *args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 616, in run_in_executor
    return await asyncio.get_running_loop().run_in_executor(
  File "D:\Anaconda\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 607, in wrapper
    return func(*args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4921, in func
    return call_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "D:\Anaconda\lib\site-packages\langchain\chains\combine_documents\stuff.py", line 86, in format_docs
    for doc in inputs[document_variable_name]
KeyError: 'context'
2025-05-08 14:31:16,496 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-1302' coro=<py_anext.<locals>.anext_impl() done, defined at D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py:71> exception=KeyError('context')>-default_exception_handler
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4988, in atransform
    async for output in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4952, in _atransform
    output = await acall_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4927, in f
    return await run_in_executor(config, func, *args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 616, in run_in_executor
    return await asyncio.get_running_loop().run_in_executor(
  File "D:\Anaconda\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 607, in wrapper
    return func(*args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4921, in func
    return call_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "D:\Anaconda\lib\site-packages\langchain\chains\combine_documents\stuff.py", line 86, in format_docs
    for doc in inputs[document_variable_name]
KeyError: 'context'
2025-05-08 14:44:51,670 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 69, in chat_qa_chain_self_answer
    async for chunk in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 220, in answer_with_history
    async for chunk in convo_qa_chain.astream({
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 624, in _atransform
    async for chunk in for_passthrough:
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\branch.py", line 454, in astream
    async for chunk in self.default.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 900, in _astream
    response = await self.async_client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 1927, in create
    return await self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1562, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.PermissionDeniedError: Error code: 403 - {'error': {'message': '���ƶ�Ȳ��� (request id: 20250508144452718356496Ou0CjfxC) (request id: 20250508144452596073849prV0NyHK)', 'type': 'nebstudio_api_error', 'param': '', 'code': 'pre_consume_token_quota_failed'}}
-chat_qa_chain_self_answer
2025-05-08 14:48:41,522 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 69, in chat_qa_chain_self_answer
    async for chunk in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 220, in answer_with_history
    async for chunk in convo_qa_chain.astream({
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 624, in _atransform
    async for chunk in for_passthrough:
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\branch.py", line 454, in astream
    async for chunk in self.default.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 900, in _astream
    response = await self.async_client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 1927, in create
    return await self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1547, in _request
    return await self._retry_request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1594, in _retry_request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1562, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': '[sk-Ugq***323] �����ƶ�����þ� !token.UnlimitedQuota && token.RemainQuota = -25120 (request id: 20250508144842571142361ak3wjYKZ) (request id: 20250508144842448893384jLULeOcN)', 'type': 'nebstudio_api_error', 'param': '', 'code': None}}
-chat_qa_chain_self_answer
2025-05-08 14:54:47,680 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 69, in chat_qa_chain_self_answer
    async for chunk in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 220, in answer_with_history
    async for chunk in convo_qa_chain.astream({
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 624, in _atransform
    async for chunk in for_passthrough:
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\branch.py", line 454, in astream
    async for chunk in self.default.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 900, in _astream
    response = await self.async_client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 1927, in create
    return await self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1562, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'Unrecognized request argument supplied: max_completion_tokens  (request id: B202505081454315436926851eNnbSIw) (request id: 20250508145431437353560hFmzjJPV) (request id: 20250508145431341436718VNywSStq)', 'type': 'new_api_error', 'param': '', 'code': None}}
-chat_qa_chain_self_answer
2025-05-08 15:09:56,900 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 69, in chat_qa_chain_self_answer
    async for chunk in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 220, in answer_with_history
    async for chunk in convo_qa_chain.astream({
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 908, in _astream
    generation_chunk = self._convert_chunk_to_generation_chunk(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 668, in _convert_chunk_to_generation_chunk
    _create_usage_metadata(token_usage) if token_usage else None
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2569, in _create_usage_metadata
    total_tokens = oai_token_usage.get("total_tokens", input_tokens + output_tokens)
TypeError: unsupported operand type(s) for +: 'NoneType' and 'NoneType'
-chat_qa_chain_self_answer
2025-05-08 15:13:21,477 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 69, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 220, in answer_with_history
    async for chunk in convo_qa_chain.astream({
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 900, in _astream
    response = await self.async_client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 1927, in create
    return await self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1562, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': '[sk-Ugq***323] �����ƶ�����þ� !token.UnlimitedQuota && token.RemainQuota = -25120 (request id: 20250508151322536348744fMlT8Zxc) (request id: 20250508151322417169143TIxc2TLY)', 'type': 'nebstudio_api_error', 'param': '', 'code': None}}
-chat_qa_chain_self_answer
2025-05-08 15:16:13,065 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 69, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 220, in answer_with_history
    async for chunk in convo_qa_chain.astream({
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 624, in _atransform
    async for chunk in for_passthrough:
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\branch.py", line 454, in astream
    async for chunk in self.default.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 900, in _astream
    response = await self.async_client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 1927, in create
    return await self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1562, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': '[sk-Ugq***323] �����ƶ�����þ� !token.UnlimitedQuota && token.RemainQuota = -25120 (request id: 20250508151614121695533hyM26kDb) (request id: 20250508151613947344454ZZzm1HJ0)', 'type': 'nebstudio_api_error', 'param': '', 'code': None}}
-chat_qa_chain_self_answer
2025-05-08 15:17:16,725 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-486' coro=<py_anext.<locals>.anext_impl() done, defined at D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py:71> exception=KeyError('context')>-default_exception_handler
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4988, in atransform
    async for output in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4952, in _atransform
    output = await acall_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4927, in f
    return await run_in_executor(config, func, *args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 616, in run_in_executor
    return await asyncio.get_running_loop().run_in_executor(
  File "D:\Anaconda\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 607, in wrapper
    return func(*args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4921, in func
    return call_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "D:\Anaconda\lib\site-packages\langchain\chains\combine_documents\stuff.py", line 86, in format_docs
    for doc in inputs[document_variable_name]
KeyError: 'context'
2025-05-08 15:17:39,767 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 69, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 220, in answer_with_history
    async for chunk in convo_qa_chain.astream({
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 624, in _atransform
    async for chunk in for_passthrough:
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\branch.py", line 454, in astream
    async for chunk in self.default.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 900, in _astream
    response = await self.async_client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 1927, in create
    return await self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1562, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': '[sk-qSA***3E8] �����ƶ�����þ� !token.UnlimitedQuota && token.RemainQuota = -723495 (request id: 20250508151740831927898zfult2D7) (request id: 20250508151740718569371pSedkL9z)', 'type': 'nebstudio_api_error', 'param': '', 'code': None}}
-chat_qa_chain_self_answer
2025-05-08 15:18:17,165 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-1174' coro=<py_anext.<locals>.anext_impl() done, defined at D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py:71> exception=KeyError('context')>-default_exception_handler
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4988, in atransform
    async for output in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4952, in _atransform
    output = await acall_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4927, in f
    return await run_in_executor(config, func, *args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 616, in run_in_executor
    return await asyncio.get_running_loop().run_in_executor(
  File "D:\Anaconda\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 607, in wrapper
    return func(*args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4921, in func
    return call_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "D:\Anaconda\lib\site-packages\langchain\chains\combine_documents\stuff.py", line 86, in format_docs
    for doc in inputs[document_variable_name]
KeyError: 'context'
2025-05-08 15:18:18,593 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 69, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 220, in answer_with_history
    async for chunk in convo_qa_chain.astream({
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 900, in _astream
    response = await self.async_client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 1927, in create
    return await self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1562, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': '[sk-eCj***e37] �����ƶ�����þ� !token.UnlimitedQuota && token.RemainQuota = -748238 (request id: 202505081518195913743415s4865f3) (request id: 20250508151819310993858XaLdlsT6)', 'type': 'nebstudio_api_error', 'param': '', 'code': None}}
-chat_qa_chain_self_answer
2025-05-08 16:35:00,694 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 69, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 220, in answer_with_history
    async for chunk in convo_qa_chain.astream({
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 624, in _atransform
    async for chunk in for_passthrough:
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\branch.py", line 454, in astream
    async for chunk in self.default.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 900, in _astream
    response = await self.async_client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 1927, in create
    return await self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1562, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': '[sk-eCj***e37] �����ƶ�����þ� !token.UnlimitedQuota && token.RemainQuota = -748238 (request id: 20250508163501765315767ZMqaHory) (request id: 20250508163501636779986SNsARwND)', 'type': 'nebstudio_api_error', 'param': '', 'code': None}}
-chat_qa_chain_self_answer
2025-05-08 16:35:44,943 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-1665' coro=<py_anext.<locals>.anext_impl() done, defined at D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py:71> exception=KeyError('context')>-default_exception_handler
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4988, in atransform
    async for output in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4952, in _atransform
    output = await acall_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4927, in f
    return await run_in_executor(config, func, *args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 616, in run_in_executor
    return await asyncio.get_running_loop().run_in_executor(
  File "D:\Anaconda\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 607, in wrapper
    return func(*args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4921, in func
    return call_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "D:\Anaconda\lib\site-packages\langchain\chains\combine_documents\stuff.py", line 86, in format_docs
    for doc in inputs[document_variable_name]
KeyError: 'context'
2025-05-08 16:47:53,790 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\serve\run_rag_assistant.py", line 84, in create_db_info
    return DB.create_db_info(files,embedding_type,embeddings), gr.update(choices=DB.files.get(embeddings, []),value=None),DB
  File "B:\notebook\Rag_Assistant\project\database\create_db.py", line 180, in create_db_info
    new_files,exist_files=self.add_files(files,embedding_type,embeddings)
  File "B:\notebook\Rag_Assistant\project\database\create_db.py", line 202, in add_files
    =self.document_process(files,embedding_type,embeddings)
  File "B:\notebook\Rag_Assistant\project\database\create_db.py", line 253, in document_process
    subdocs: list[Document] = loader[1].load()
  File "D:\Anaconda\lib\site-packages\langchain_core\document_loaders\base.py", line 32, in load
    return list(self.lazy_load())
  File "D:\Anaconda\lib\site-packages\langchain_community\document_loaders\unstructured.py", line 107, in lazy_load
    elements = self._get_elements()
  File "D:\Anaconda\lib\site-packages\langchain_community\document_loaders\word_document.py", line 137, in _get_elements
    from unstructured.partition.docx import partition_docx
  File "D:\PyCharm 2022.3.3\plugins\python\helpers\pydev\_pydev_bundle\pydev_import_hook.py", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File "C:\Users\chenx\AppData\Roaming\Python\Python39\site-packages\unstructured\partition\docx.py", line 33, in <module>
    from docx.text.hyperlink import Hyperlink
  File "D:\PyCharm 2022.3.3\plugins\python\helpers\pydev\_pydev_bundle\pydev_import_hook.py", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
ModuleNotFoundError: No module named 'docx.text.hyperlink'
-create_db_info
2025-05-08 16:48:06,405 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\serve\run_rag_assistant.py", line 84, in create_db_info
    return DB.create_db_info(files,embedding_type,embeddings), gr.update(choices=DB.files.get(embeddings, []),value=None),DB
  File "B:\notebook\Rag_Assistant\project\database\create_db.py", line 180, in create_db_info
    new_files,exist_files=self.add_files(files,embedding_type,embeddings)
  File "B:\notebook\Rag_Assistant\project\database\create_db.py", line 202, in add_files
    =self.document_process(files,embedding_type,embeddings)
  File "B:\notebook\Rag_Assistant\project\database\create_db.py", line 253, in document_process
    subdocs: list[Document] = loader[1].load()
  File "D:\Anaconda\lib\site-packages\langchain_core\document_loaders\base.py", line 32, in load
    return list(self.lazy_load())
  File "D:\Anaconda\lib\site-packages\langchain_community\document_loaders\unstructured.py", line 107, in lazy_load
    elements = self._get_elements()
  File "D:\Anaconda\lib\site-packages\langchain_community\document_loaders\unstructured.py", line 216, in _get_elements
    from unstructured.partition.auto import partition
  File "D:\PyCharm 2022.3.3\plugins\python\helpers\pydev\_pydev_bundle\pydev_import_hook.py", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File "C:\Users\chenx\AppData\Roaming\Python\Python39\site-packages\unstructured\partition\auto.py", line 37, in <module>
    from unstructured.partition.doc import partition_doc
  File "D:\PyCharm 2022.3.3\plugins\python\helpers\pydev\_pydev_bundle\pydev_import_hook.py", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File "C:\Users\chenx\AppData\Roaming\Python\Python39\site-packages\unstructured\partition\doc.py", line 14, in <module>
    from unstructured.partition.docx import partition_docx
  File "D:\PyCharm 2022.3.3\plugins\python\helpers\pydev\_pydev_bundle\pydev_import_hook.py", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File "C:\Users\chenx\AppData\Roaming\Python\Python39\site-packages\unstructured\partition\docx.py", line 33, in <module>
    from docx.text.hyperlink import Hyperlink
  File "D:\PyCharm 2022.3.3\plugins\python\helpers\pydev\_pydev_bundle\pydev_import_hook.py", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
ModuleNotFoundError: No module named 'docx.text.hyperlink'
-create_db_info
2025-05-08 16:52:55,819 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 69, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 216, in answer_with_history
    scores = self.get_scores(question, **kwargs)
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 282, in get_scores
    docs_with_scores = self.vectordb.similarity_search_with_relevance_scores(
  File "D:\Anaconda\lib\site-packages\langchain_core\vectorstores\base.py", line 556, in similarity_search_with_relevance_scores
    docs_and_similarities = self._similarity_search_with_relevance_scores(
  File "D:\Anaconda\lib\site-packages\langchain_core\vectorstores\base.py", line 504, in _similarity_search_with_relevance_scores
    docs_and_scores = self.similarity_search_with_score(query, k, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_chroma\vectorstores.py", line 701, in similarity_search_with_score
    results = self.__query_collection(
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\utils.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_chroma\vectorstores.py", line 393, in __query_collection
    return self._collection.query(
  File "D:\Anaconda\lib\site-packages\chromadb\api\models\Collection.py", line 218, in query
    query_results = self._client._query(
  File "D:\Anaconda\lib\site-packages\chromadb\api\rust.py", line 507, in _query
    rust_response = self.bindings.query(
chromadb.errors.InternalError: Error executing plan: Internal error: Error finding id
-chat_qa_chain_self_answer
2025-05-08 16:53:11,813 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 69, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 220, in answer_with_history
    async for chunk in convo_qa_chain.astream({
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 624, in _atransform
    async for chunk in for_passthrough:
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\branch.py", line 454, in astream
    async for chunk in self.default.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 900, in _astream
    response = await self.async_client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 1927, in create
    return await self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1562, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': '[sk-E5I***d81] �����ƶ�����þ� !token.UnlimitedQuota && token.RemainQuota = -840921 (request id: 202505081653128768677526N3YHU39) (request id: 20250508165312721657811hJcgeveb)', 'type': 'nebstudio_api_error', 'param': '', 'code': None}}
-chat_qa_chain_self_answer
2025-05-08 18:21:54,458 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 69, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 264, in answer_with_history
    async for chunk in convo_qa_chain.astream({
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 900, in _astream
    response = await self.async_client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 1927, in create
    return await self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1562, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': '[sk-eCj***e37] �����ƶ�����þ� !token.UnlimitedQuota && token.RemainQuota = -748238 (request id: 20250508182155555648288Ue3IXlHI) (request id: 20250508182155392964137t7FKxtyh)', 'type': 'nebstudio_api_error', 'param': '', 'code': None}}
-chat_qa_chain_self_answer
2025-05-08 18:23:04,169 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 101, in map_httpcore_exceptions
    yield
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 271, in __aiter__
    async for part in self._httpcore_stream:
  File "D:\Anaconda\lib\site-packages\httpcore\_async\connection_pool.py", line 407, in __aiter__
    raise exc from None
  File "D:\Anaconda\lib\site-packages\httpcore\_async\connection_pool.py", line 403, in __aiter__
    async for part in self._stream:
  File "D:\Anaconda\lib\site-packages\httpcore\_async\http11.py", line 342, in __aiter__
    raise exc
  File "D:\Anaconda\lib\site-packages\httpcore\_async\http11.py", line 334, in __aiter__
    async for chunk in self._connection._receive_response_body(**kwargs):
  File "D:\Anaconda\lib\site-packages\httpcore\_async\http11.py", line 203, in _receive_response_body
    event = await self._receive_event(timeout=timeout)
  File "D:\Anaconda\lib\site-packages\httpcore\_async\http11.py", line 214, in _receive_event
    event = self._h11_state.next_event()
  File "D:\Anaconda\lib\contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\Anaconda\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 69, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 264, in answer_with_history
    async for chunk in convo_qa_chain.astream({
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 905, in _astream
    async for chunk in response:
  File "D:\Anaconda\lib\site-packages\openai\_streaming.py", line 147, in __aiter__
    async for item in self._iterator:
  File "D:\Anaconda\lib\site-packages\openai\_streaming.py", line 160, in __stream__
    async for sse in iterator:
  File "D:\Anaconda\lib\site-packages\openai\_streaming.py", line 151, in _iter_events
    async for sse in self._decoder.aiter_bytes(self.response.aiter_bytes()):
  File "D:\Anaconda\lib\site-packages\openai\_streaming.py", line 302, in aiter_bytes
    async for chunk in self._aiter_chunks(iterator):
  File "D:\Anaconda\lib\site-packages\openai\_streaming.py", line 313, in _aiter_chunks
    async for chunk in iterator:
  File "D:\Anaconda\lib\site-packages\httpx\_models.py", line 997, in aiter_bytes
    async for raw_bytes in self.aiter_raw():
  File "D:\Anaconda\lib\site-packages\httpx\_models.py", line 1055, in aiter_raw
    async for raw_stream_bytes in self.stream:
  File "D:\Anaconda\lib\site-packages\httpx\_client.py", line 176, in __aiter__
    async for chunk in self._stream:
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 272, in __aiter__
    yield part
  File "D:\Anaconda\lib\contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)
-chat_qa_chain_self_answer
2025-05-09 16:34:11,889 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 151, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 164, in chat_answer
    self.chat_history[-1][1]+=answer
TypeError: 'tuple' object does not support item assignment
-chat_qa_chain_self_answer
2025-05-09 16:36:34,247 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 147, in answer
    async for info, answer in rag_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 217, in rag_answer
    async for info,answer in self.rag_answer_stream(question, rag_chain, **kwargs):
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 222, in rag_answer_stream
    scores = get_scores(question, **kwargs)
TypeError: get_scores() missing 1 required positional argument: 'vectordb'
-chat_qa_chain_self_answer
2025-05-09 16:40:55,284 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 147, in answer
    async for info, answer in rag_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 217, in rag_answer
    async for info,answer in self.rag_answer_stream(question, rag_chain, **kwargs):
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 228, in rag_answer_stream
    async for chunk in rag_chain.astream(chain_config):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1024, in astream
    yield await self.ainvoke(input, config, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 242, in ainvoke
    return await self._acall_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1981, in _acall_with_config
    output: Output = await coro_with_context(coro, context)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 196, in _aformat_prompt_with_error_handling
    return await self.aformat_prompt(**_inner_input)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\chat.py", line 737, in aformat_prompt
    messages = await self.aformat_messages(**kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\chat.py", line 1214, in aformat_messages
    message = await message_template.aformat_messages(**kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\message.py", line 55, in aformat_messages
    return self.format_messages(**kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\chat.py", line 183, in format_messages
    raise ValueError(msg)  # noqa: TRY004
ValueError: variable chat_history should be a list of base messages, got None of type <class 'NoneType'>
-chat_qa_chain_self_answer
2025-05-09 16:55:21,531 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 151, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 162, in chat_answer
    async for answer in chat_chain.astream(chain_config):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 900, in _astream
    response = await self.async_client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 1927, in create
    return await self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1547, in _request
    return await self._retry_request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1594, in _retry_request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1547, in _request
    return await self._retry_request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1594, in _retry_request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1562, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.InternalServerError: Error code: 503 - {'error': {'message': '��ǰ���� default �¶���ģ�� claude-3-5-haiku �޿������� (request id: 20250509165522802290279Tdy8s1mQ) (request id: 20250509165522659409477iDJcvLjQ)', 'type': 'new_api_error', 'param': '', 'code': None}}
-chat_qa_chain_self_answer
2025-05-11 23:04:48,532 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 150, in answer
    abstract_answer=self.abstract_answer()
TypeError: abstract_answer() missing 1 required positional argument: 'files'
-chat_qa_chain_self_answer
2025-05-12 00:44:23,780 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 156, in answer
    async for info, answer in rag_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 262, in rag_answer
    async for info,answer in self.rag_answer_stream(question, rag_chain, **kwargs):
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 271, in rag_answer_stream
    chain_config["chat_history"] = self.get_chat_history_LCEL(kwargs["history_len"]) if kwargs[
KeyError: 'is_with_history'
-chat_qa_chain_self_answer
2025-05-12 00:46:49,671 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 156, in answer
    async for info, answer in rag_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 262, in rag_answer
    async for info,answer in self.rag_answer_stream(question, rag_chain, **kwargs):
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 271, in rag_answer_stream
    chain_config["chat_history"] = self.get_chat_history_LCEL(kwargs["chat_mode"]["history_len"]) if kwargs[
KeyError: 'is_with_history'
-chat_qa_chain_self_answer
2025-05-12 12:44:16,188 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 160, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 207, in chat_answer
    async for answer in chat_chain.astream(chain_config):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 900, in _astream
    response = await self.async_client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 1927, in create
    return await self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1562, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - [{'error': {'code': 400, 'message': 'User location is not supported for the API use.', 'status': 'FAILED_PRECONDITION'}}]
-chat_qa_chain_self_answer
2025-05-12 12:53:32,282 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 151, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 175, in abstract_answer
    loaders = get_docs()
TypeError: get_docs() missing 3 required positional arguments: 'vectordb', 'files', and 'path'
-chat_qa_chain_self_answer
2025-05-12 12:53:46,921 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 151, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 175, in abstract_answer
    loaders = get_docs()
TypeError: get_docs() missing 3 required positional arguments: 'vectordb', 'files', and 'path'
-chat_qa_chain_self_answer
2025-05-12 12:57:43,171 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 151, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 173, in abstract_answer
    file_loader(file, loaders)
TypeError: file_loader() missing 1 required positional argument: 'loaders'
-chat_qa_chain_self_answer
2025-05-12 12:58:30,872 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 65, in chat_qa_chain_self_answer
    self.chat_history.append(chain.chat_history[-1]) #Ӧ�ð�chainͳһ��agent�Ĵ洢��
IndexError: list index out of range
-chat_qa_chain_self_answer
2025-05-12 13:00:27,933 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 65, in chat_qa_chain_self_answer
    self.chat_history.append(chain.chat_history[-1]) #Ӧ�ð�chainͳһ��agent�Ĵ洢��
IndexError: list index out of range
-chat_qa_chain_self_answer
2025-05-12 13:00:41,117 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 151, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 176, in abstract_answer
    loaders = get_docs()
TypeError: get_docs() missing 3 required positional arguments: 'vectordb', 'files', and 'path'
-chat_qa_chain_self_answer
2025-05-12 15:18:48,249 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 151, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 176, in abstract_answer
    loaders = get_docs()
TypeError: get_docs() missing 3 required positional arguments: 'vectordb', 'files', and 'path'
-chat_qa_chain_self_answer
2025-05-12 15:26:17,172 - backoff - INFO - Backing off send_request(...) for 0.8s (requests.exceptions.ProxyError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(10054, 'Զ������ǿ�ȹر���һ�����е����ӡ�', None, 10054, None))))-_log_backoff
2025-05-12 15:26:17,992 - backoff - INFO - Backing off send_request(...) for 0.1s (requests.exceptions.ProxyError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(10054, 'Զ������ǿ�ȹر���һ�����е����ӡ�', None, 10054, None))))-_log_backoff
2025-05-12 15:26:18,149 - backoff - INFO - Backing off send_request(...) for 1.1s (requests.exceptions.ProxyError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(10054, 'Զ������ǿ�ȹر���һ�����е����ӡ�', None, 10054, None))))-_log_backoff
2025-05-12 15:26:18,642 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 101, in map_httpcore_exceptions
    yield
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 250, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\Anaconda\lib\site-packages\httpcore\_sync\connection_pool.py", line 256, in handle_request
    raise exc from None
  File "D:\Anaconda\lib\site-packages\httpcore\_sync\connection_pool.py", line 236, in handle_request
    response = connection.handle_request(
  File "D:\Anaconda\lib\site-packages\httpcore\_sync\http_proxy.py", line 316, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\Anaconda\lib\site-packages\httpcore\_sync\http11.py", line 376, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "D:\Anaconda\lib\site-packages\httpcore\_backends\sync.py", line 170, in start_tls
    raise exc
  File "D:\Anaconda\lib\contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\Anaconda\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [WinError 10054] Զ������ǿ�ȹر���һ�����е����ӡ�

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 955, in _request
    response = self._client.send(
  File "D:\Anaconda\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "D:\Anaconda\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "D:\Anaconda\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "D:\Anaconda\lib\site-packages\httpx\_client.py", line 1014, in _send_single_request
    response = transport.handle_request(request)
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 250, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\Anaconda\lib\contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] Զ������ǿ�ȹر���һ�����е����ӡ�

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\serve\run_rag_assistant.py", line 93, in create_db_info
    return DB.create_db_info(files, embedding_type, embeddings), gr.update(choices=DB.files.get(embeddings, []),
  File "B:\notebook\Rag_Assistant\project\database\create_db.py", line 180, in create_db_info
    new_files,exist_files=self.add_files(files,embedding_type,embeddings)
  File "B:\notebook\Rag_Assistant\project\database\create_db.py", line 205, in add_files
    vectordb = Chroma.from_documents(
  File "D:\Anaconda\lib\site-packages\langchain_chroma\vectorstores.py", line 1234, in from_documents
    return cls.from_texts(
  File "D:\Anaconda\lib\site-packages\langchain_chroma\vectorstores.py", line 1187, in from_texts
    chroma_collection.add_texts(
  File "D:\Anaconda\lib\site-packages\langchain_chroma\vectorstores.py", line 527, in add_texts
    embeddings = self._embedding_function.embed_documents(texts)
  File "D:\Anaconda\lib\site-packages\langchain_openai\embeddings\base.py", line 588, in embed_documents
    return self._get_len_safe_embeddings(texts, engine=engine)
  File "D:\Anaconda\lib\site-packages\langchain_openai\embeddings\base.py", line 483, in _get_len_safe_embeddings
    response = self.client.create(
  File "D:\Anaconda\lib\site-packages\openai\resources\embeddings.py", line 128, in create
    return self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 919, in request
    return self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 979, in _request
    return self._retry_request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1057, in _retry_request
    return self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 979, in _request
    return self._retry_request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1057, in _retry_request
    return self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 989, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
-create_db_info
2025-05-12 15:26:19,253 - backoff - ERROR - Giving up send_request(...) after 4 tries (requests.exceptions.ProxyError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(10054, 'Զ������ǿ�ȹر���һ�����е����ӡ�', None, 10054, None))))-_log_giveup
2025-05-12 15:29:28,121 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 151, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 186, in abstract_answer
    async for answer in chain.astream({"context": loaders}):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4988, in atransform
    async for output in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4952, in _atransform
    output = await acall_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4927, in f
    return await run_in_executor(config, func, *args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 616, in run_in_executor
    return await asyncio.get_running_loop().run_in_executor(
  File "D:\Anaconda\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 607, in wrapper
    return func(*args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4921, in func
    return call_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "D:\Anaconda\lib\site-packages\langchain\chains\combine_documents\stuff.py", line 84, in format_docs
    return document_separator.join(
  File "D:\Anaconda\lib\site-packages\langchain\chains\combine_documents\stuff.py", line 85, in <genexpr>
    format_document(doc, _document_prompt)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 446, in format_document
    return prompt.format(**_get_document_info(doc, prompt))
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 395, in _get_document_info
    base_info = {"page_content": doc.page_content, **doc.metadata}
AttributeError: 'numpy.ndarray' object has no attribute 'page_content'
-chat_qa_chain_self_answer
2025-05-12 15:29:41,506 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 151, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 186, in abstract_answer
    async for answer in chain.astream({"context": loaders}):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4988, in atransform
    async for output in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4952, in _atransform
    output = await acall_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4927, in f
    return await run_in_executor(config, func, *args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 616, in run_in_executor
    return await asyncio.get_running_loop().run_in_executor(
  File "D:\Anaconda\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 607, in wrapper
    return func(*args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4921, in func
    return call_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "D:\Anaconda\lib\site-packages\langchain\chains\combine_documents\stuff.py", line 84, in format_docs
    return document_separator.join(
  File "D:\Anaconda\lib\site-packages\langchain\chains\combine_documents\stuff.py", line 85, in <genexpr>
    format_document(doc, _document_prompt)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 446, in format_document
    return prompt.format(**_get_document_info(doc, prompt))
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 395, in _get_document_info
    base_info = {"page_content": doc.page_content, **doc.metadata}
AttributeError: 'PyMuPDFLoader' object has no attribute 'page_content'
-chat_qa_chain_self_answer
2025-05-12 15:30:48,700 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 151, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 186, in abstract_answer
    async for answer in chain.astream({"context": loaders}):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4988, in atransform
    async for output in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4952, in _atransform
    output = await acall_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4927, in f
    return await run_in_executor(config, func, *args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 616, in run_in_executor
    return await asyncio.get_running_loop().run_in_executor(
  File "D:\Anaconda\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 607, in wrapper
    return func(*args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4921, in func
    return call_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "D:\Anaconda\lib\site-packages\langchain\chains\combine_documents\stuff.py", line 84, in format_docs
    return document_separator.join(
  File "D:\Anaconda\lib\site-packages\langchain\chains\combine_documents\stuff.py", line 85, in <genexpr>
    format_document(doc, _document_prompt)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 446, in format_document
    return prompt.format(**_get_document_info(doc, prompt))
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 395, in _get_document_info
    base_info = {"page_content": doc.page_content, **doc.metadata}
AttributeError: 'numpy.ndarray' object has no attribute 'page_content'
-chat_qa_chain_self_answer
2025-05-12 16:15:28,204 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 151, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 183, in abstract_answer
    docs = get_docs(self.vectordb,files,self.sql_path)
  File "B:\notebook\Rag_Assistant\project\utils\fileProcess.py", line 60, in get_docs
    cursor.execute(query, seq_ids)
sqlite3.ProgrammingError: Incorrect number of bindings supplied. The current statement uses 1, and there are 136 supplied.
-chat_qa_chain_self_answer
2025-05-12 16:16:24,312 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 151, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 180, in abstract_answer
    docs[file_name].extend(docs)
TypeError: list indices must be integers or slices, not str
-chat_qa_chain_self_answer
2025-05-12 16:22:28,429 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 151, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 198, in abstract_answer
    async for answer in chain.astream({"context": doc}):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4988, in atransform
    async for output in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4952, in _atransform
    output = await acall_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4927, in f
    return await run_in_executor(config, func, *args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 616, in run_in_executor
    return await asyncio.get_running_loop().run_in_executor(
  File "D:\Anaconda\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 607, in wrapper
    return func(*args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4921, in func
    return call_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "D:\Anaconda\lib\site-packages\langchain\chains\combine_documents\stuff.py", line 84, in format_docs
    return document_separator.join(
  File "D:\Anaconda\lib\site-packages\langchain\chains\combine_documents\stuff.py", line 85, in <genexpr>
    format_document(doc, _document_prompt)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 446, in format_document
    return prompt.format(**_get_document_info(doc, prompt))
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 395, in _get_document_info
    base_info = {"page_content": doc.page_content, **doc.metadata}
AttributeError: 'str' object has no attribute 'page_content'
-chat_qa_chain_self_answer
2025-05-12 16:35:22,357 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 151, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 198, in abstract_answer
    async for answer in chain.astream({"context": doc}):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 900, in _astream
    response = await self.async_client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 1927, in create
    return await self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1562, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.PermissionDeniedError: Error code: 403 - {'error': {'message': '���API����ģ������tokenС��4096�����и������������ https://api.chatanywhere.tech/#/shop ���򸶷�API��The number of prompt tokens for free accounts is limited to 4096. If you have additional requirements, please visit https://api.chatanywhere.tech/#/shop to purchase a premium key.(��ǰ����ʹ�õ�ApiKey: sk-Hsr****fpgt)', 'type': 'chatanywhere_error', 'param': None, 'code': '403 FORBIDDEN'}}
-chat_qa_chain_self_answer
2025-05-12 16:36:21,581 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 151, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 198, in abstract_answer
    self.chat_history[-1][1] += answer
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 900, in _astream
    response = await self.async_client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 1927, in create
    return await self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1562, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - [{'error': {'code': 400, 'message': '* GenerateContentRequest.contents: contents is not specified\n', 'status': 'INVALID_ARGUMENT'}}]
-chat_qa_chain_self_answer
2025-05-12 16:37:18,796 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 151, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 198, in abstract_answer
    self.chat_history[-1][1] += answer
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 900, in _astream
    response = await self.async_client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 1927, in create
    return await self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1562, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - [{'error': {'code': 400, 'message': '* GenerateContentRequest.contents: contents is not specified\n', 'status': 'INVALID_ARGUMENT'}}]
-chat_qa_chain_self_answer
2025-05-12 17:11:06,612 - backoff - INFO - Backing off send_request(...) for 0.8s (requests.exceptions.ProxyError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(10054, 'Զ������ǿ�ȹر���һ�����е����ӡ�', None, 10054, None))))-_log_backoff
2025-05-12 17:11:07,454 - backoff - INFO - Backing off send_request(...) for 1.7s (requests.exceptions.ProxyError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(10054, 'Զ������ǿ�ȹر���һ�����е����ӡ�', None, 10054, None))))-_log_backoff
2025-05-12 17:11:09,216 - backoff - INFO - Backing off send_request(...) for 2.5s (requests.exceptions.ProxyError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(10054, 'Զ������ǿ�ȹر���һ�����е����ӡ�', None, 10054, None))))-_log_backoff
2025-05-12 17:11:09,934 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 101, in map_httpcore_exceptions
    yield
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 394, in handle_async_request
    resp = await self._pool.handle_async_request(req)
  File "D:\Anaconda\lib\site-packages\httpcore\_async\connection_pool.py", line 256, in handle_async_request
    raise exc from None
  File "D:\Anaconda\lib\site-packages\httpcore\_async\connection_pool.py", line 236, in handle_async_request
    response = await connection.handle_async_request(
  File "D:\Anaconda\lib\site-packages\httpcore\_async\http_proxy.py", line 316, in handle_async_request
    stream = await stream.start_tls(**kwargs)
  File "D:\Anaconda\lib\site-packages\httpcore\_async\http11.py", line 376, in start_tls
    return await self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "D:\Anaconda\lib\site-packages\httpcore\_backends\anyio.py", line 79, in start_tls
    raise exc
  File "D:\Anaconda\lib\contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\Anaconda\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1500, in _request
    response = await self._client.send(
  File "D:\Anaconda\lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
  File "D:\Anaconda\lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
  File "D:\Anaconda\lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
  File "D:\Anaconda\lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 394, in handle_async_request
    resp = await self._pool.handle_async_request(req)
  File "D:\Anaconda\lib\contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 151, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 198, in abstract_answer
    self.chat_history[-1][1] += answer
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 900, in _astream
    response = await self.async_client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 1927, in create
    return await self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1524, in _request
    return await self._retry_request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1594, in _retry_request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1524, in _request
    return await self._retry_request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1594, in _retry_request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1534, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
-chat_qa_chain_self_answer
2025-05-12 17:11:11,712 - backoff - ERROR - Giving up send_request(...) after 4 tries (requests.exceptions.ProxyError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(10054, 'Զ������ǿ�ȹر���һ�����е����ӡ�', None, 10054, None))))-_log_giveup
2025-05-12 17:11:28,955 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 101, in map_httpcore_exceptions
    yield
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 394, in handle_async_request
    resp = await self._pool.handle_async_request(req)
  File "D:\Anaconda\lib\site-packages\httpcore\_async\connection_pool.py", line 256, in handle_async_request
    raise exc from None
  File "D:\Anaconda\lib\site-packages\httpcore\_async\connection_pool.py", line 236, in handle_async_request
    response = await connection.handle_async_request(
  File "D:\Anaconda\lib\site-packages\httpcore\_async\http_proxy.py", line 316, in handle_async_request
    stream = await stream.start_tls(**kwargs)
  File "D:\Anaconda\lib\site-packages\httpcore\_async\http11.py", line 376, in start_tls
    return await self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "D:\Anaconda\lib\site-packages\httpcore\_backends\anyio.py", line 79, in start_tls
    raise exc
  File "D:\Anaconda\lib\contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\Anaconda\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: TLS/SSL connection has been closed (EOF) (_ssl.c:1133)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1500, in _request
    response = await self._client.send(
  File "D:\Anaconda\lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
  File "D:\Anaconda\lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
  File "D:\Anaconda\lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
  File "D:\Anaconda\lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 394, in handle_async_request
    resp = await self._pool.handle_async_request(req)
  File "D:\Anaconda\lib\contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: TLS/SSL connection has been closed (EOF) (_ssl.c:1133)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 160, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 221, in chat_answer
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 900, in _astream
    response = await self.async_client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 1927, in create
    return await self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1524, in _request
    return await self._retry_request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1594, in _retry_request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1524, in _request
    return await self._retry_request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1594, in _retry_request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1534, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
-chat_qa_chain_self_answer
2025-05-12 17:12:06,153 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 151, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 198, in abstract_answer
    self.chat_history[-1][1] += answer
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 900, in _astream
    response = await self.async_client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 1927, in create
    return await self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1562, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - [{'error': {'code': 400, 'message': '* GenerateContentRequest.contents: contents is not specified\n', 'status': 'INVALID_ARGUMENT'}}]
-chat_qa_chain_self_answer
2025-05-12 17:16:59,091 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 151, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 198, in abstract_answer
    self.chat_history[-1][1] += answer
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 900, in _astream
    response = await self.async_client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 1927, in create
    return await self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1562, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'code': '1214', 'message': 'messages �����Ƿ��������ĵ���'}}
-chat_qa_chain_self_answer
2025-05-12 17:18:06,059 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 151, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 198, in abstract_answer
    self.chat_history[-1][1] += answer
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_community\chat_models\baichuan.py", line 540, in _astream
    async for sse in event_source.aiter_sse():
  File "D:\Anaconda\lib\site-packages\httpx_sse\_api.py", line 37, in aiter_sse
    self._check_content_type()
  File "D:\Anaconda\lib\site-packages\httpx_sse\_api.py", line 18, in _check_content_type
    raise SSEError(
httpx_sse._exceptions.SSEError: Expected response header Content-Type to contain 'text/event-stream', got 'application/json'
-chat_qa_chain_self_answer
2025-05-12 17:18:34,051 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 151, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 198, in abstract_answer
    self.chat_history[-1][1] += answer
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 900, in _astream
    response = await self.async_client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 1927, in create
    return await self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1562, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_argument', 'message': 'the role of last message must be user or tool', 'type': 'invalid_request_error'}, 'id': 'as-9f78h4ewct'}
-chat_qa_chain_self_answer
2025-05-12 19:07:53,835 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 158, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 191, in abstract_answer
    file_names=",".join(list[docs.keys()])
TypeError: can only join an iterable
-chat_qa_chain_self_answer
2025-05-12 19:09:23,682 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 158, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 194, in abstract_answer
    summarizer = LCELSummarizer(self.llm, mode)
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 57, in __init__
    self._create_chains()
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 70, in _create_chains
    self.stuff_chain=create_stuff_documents_chain(self.llm, self.stuff_prompt)
  File "D:\Anaconda\lib\site-packages\langchain\chains\combine_documents\stuff.py", line 79, in create_stuff_documents_chain
    _validate_prompt(prompt, document_variable_name)
  File "D:\Anaconda\lib\site-packages\langchain\chains\combine_documents\base.py", line 27, in _validate_prompt
    raise ValueError(
ValueError: Prompt must accept context as an input variable. Received prompt with input variables: ['text']
-chat_qa_chain_self_answer
2025-05-12 19:17:27,636 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 158, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 207, in abstract_answer
    async for info ,answer in summarizer.astream_summary(docs,self.chat_history):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 151, in astream_summary
    async for answer in chain.astream({"context": doc}):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 900, in _astream
    response = await self.async_client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 1927, in create
    return await self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1562, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.PermissionDeniedError: Error code: 403 - {'error': {'message': '���API����ģ������tokenС��4096�����и������������ https://api.chatanywhere.tech/#/shop ���򸶷�API��The number of prompt tokens for free accounts is limited to 4096. If you have additional requirements, please visit https://api.chatanywhere.tech/#/shop to purchase a premium key.(��ǰ����ʹ�õ�ApiKey: sk-Hsr****fpgt)', 'type': 'chatanywhere_error', 'param': None, 'code': '403 FORBIDDEN'}}
-chat_qa_chain_self_answer
2025-05-12 19:19:08,365 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 158, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 207, in abstract_answer
    async for info ,answer in summarizer.astream_summary(docs,self.chat_history):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 145, in astream_summary
    chain = self._get_chain()
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 140, in _get_chain
    raise ValueError(f"Unsupported method: {self.method}")
ValueError: Unsupported method: map_reduce
-chat_qa_chain_self_answer
2025-05-12 19:19:31,870 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 158, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 207, in abstract_answer
    async for info ,answer in summarizer.astream_summary(docs,self.chat_history):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 152, in astream_summary
    chat_history[-1][1] += answer
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\utils.py", line 518, in __radd__
    chunk = AddableDict(other)
ValueError: dictionary update sequence element #0 has length 1; 2 is required
-chat_qa_chain_self_answer
2025-05-12 19:22:57,789 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 158, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 207, in abstract_answer
    async for info ,answer in summarizer.astream_summary(docs,self.chat_history):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 145, in astream_summary
    chain = self._get_chain()
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 140, in _get_chain
    raise ValueError(f"Unsupported method: {self.method}")
ValueError: Unsupported method: map_reduce
-chat_qa_chain_self_answer
2025-05-12 19:31:00,186 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 158, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 207, in abstract_answer
    async for info ,answer in summarizer.astream_summary(docs,self.chat_history):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 151, in astream_summary
    async for answer in chain.astream({"context": doc}):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 900, in _astream
    response = await self.async_client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 1927, in create
    return await self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1562, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.PermissionDeniedError: Error code: 403 - {'error': {'message': '���API����ģ������tokenС��4096�����и������������ https://api.chatanywhere.tech/#/shop ���򸶷�API��The number of prompt tokens for free accounts is limited to 4096. If you have additional requirements, please visit https://api.chatanywhere.tech/#/shop to purchase a premium key.(��ǰ����ʹ�õ�ApiKey: sk-Hsr****fpgt)', 'type': 'chatanywhere_error', 'param': None, 'code': '403 FORBIDDEN'}}
-chat_qa_chain_self_answer
2025-05-12 19:32:09,922 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 158, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 207, in abstract_answer
    async for info ,answer in summarizer.astream_summary(docs,self.chat_history):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 151, in astream_summary
    async for answer in chain.astream({"context": doc}):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 900, in _astream
    response = await self.async_client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 1927, in create
    return await self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1562, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 8192', 'type': 'invalid_request_error'}}
-chat_qa_chain_self_answer
2025-05-12 19:32:45,407 - root - ERROR - chat_process��������
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 158, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 207, in abstract_answer
    async for info ,answer in summarizer.astream_summary(docs,self.chat_history):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 151, in astream_summary
    async for answer in chain.astream({"context": doc}):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4988, in atransform
    async for output in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4891, in _atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4988, in atransform
    async for output in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4952, in _atransform
    output = await acall_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4927, in f
    return await run_in_executor(config, func, *args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 616, in run_in_executor
    return await asyncio.get_running_loop().run_in_executor(
  File "D:\Anaconda\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 607, in wrapper
    return func(*args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4921, in func
    return call_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 93, in <lambda>
    summaries=lambda x: [
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 94, in <listcomp>
    self.map_chain.invoke({"context": context})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3034, in invoke
    input = context.run(step.invoke, input, config)
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 369, in invoke
    self.generate_prompt(
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 946, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 765, in generate
    self._generate_with_cache(
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 1011, in _generate_with_cache
    result = self._generate(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 781, in _generate
    return generate_from_stream(stream_iter)
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 153, in generate_from_stream
    generation = next(stream, None)
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 732, in _stream
    response = self.client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\_utils\_utils.py", line 279, in wrapper
    return func(*args, **kwargs)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 879, in create
    return self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 919, in request
    return self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1008, in _request
    return self._retry_request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1057, in _retry_request
    return self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1008, in _request
    return self._retry_request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1057, in _retry_request
    return self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1023, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Your account org-b58fa3daaec74bbea30dcf9357471395<ak-ey4g5asgsu6111ej6i6i> request reached organization max RPM: 3, please try again after 1 seconds', 'type': 'rate_limit_reached_error'}}
-chat_qa_chain_self_answer
2025-05-12 20:56:54,635 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 158, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 207, in abstract_answer
    async for info ,answer in summarizer.astream_summary(docs,self.chat_history):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 155, in astream_summary
    async for answer in chain.astream({"context": doc}):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1024, in astream
    yield await self.ainvoke(input, config, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain\chains\base.py", line 221, in ainvoke
    raise e
  File "D:\Anaconda\lib\site-packages\langchain\chains\base.py", line 210, in ainvoke
    self._validate_inputs(inputs)
  File "D:\Anaconda\lib\site-packages\langchain\chains\base.py", line 290, in _validate_inputs
    raise ValueError(f"Missing some input keys: {missing_keys}")
ValueError: Missing some input keys: {'input_documents'}
-chat_qa_chain_self_answer
2025-05-12 21:05:12,181 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 158, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 207, in abstract_answer
    async for info ,answer in summarizer.astream_summary(docs,self.chat_history):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 158, in astream_summary
    chain = self._get_chain()
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 135, in _get_chain
    chain=load_summarize_chain(self.llm, chain_type=self.method,verbose=True,prompt=self.stuff_prompt)
  File "D:\Anaconda\lib\site-packages\langchain\chains\summarize\chain.py", line 167, in load_summarize_chain
    return loader_mapping[chain_type](llm, verbose=verbose, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain\chains\summarize\chain.py", line 36, in _load_stuff_chain
    return StuffDocumentsChain(
  File "D:\Anaconda\lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\load\serializable.py", line 130, in __init__
    super().__init__(*args, **kwargs)
  File "D:\Anaconda\lib\site-packages\pydantic\main.py", line 214, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
pydantic_core._pydantic_core.ValidationError: 1 validation error for StuffDocumentsChain
  Value error, document_variable_name text was not found in llm_chain input_variables: ['context'] [type=value_error, input_value={'llm_chain': LLMChain(ve...'text', 'verbose': True}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/value_error
-chat_qa_chain_self_answer
2025-05-12 21:18:21,537 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 158, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 207, in abstract_answer
    async for info ,answer in summarizer.astream_summary(docs,self.chat_history):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 169, in astream_summary
    async for answer in chain.astream({"context": doc}):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1024, in astream
    yield await self.ainvoke(input, config, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain\chains\base.py", line 221, in ainvoke
    raise e
  File "D:\Anaconda\lib\site-packages\langchain\chains\base.py", line 210, in ainvoke
    self._validate_inputs(inputs)
  File "D:\Anaconda\lib\site-packages\langchain\chains\base.py", line 290, in _validate_inputs
    raise ValueError(f"Missing some input keys: {missing_keys}")
ValueError: Missing some input keys: {'input_documents'}
-chat_qa_chain_self_answer
2025-05-13 01:08:04,000 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 158, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 194, in abstract_answer
    summarizer = LCELSummarizer(self.llm, mode)
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 22, in __init__
    self.map_prompt = ChatPromptTemplate.from_message(
  File "D:\Anaconda\lib\site-packages\pydantic\_internal\_model_construction.py", line 264, in __getattr__
    raise AttributeError(item)
AttributeError: from_message
-chat_qa_chain_self_answer
2025-05-13 01:28:33,212 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 158, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 207, in abstract_answer
    async for info ,answer in summarizer.astream_summary(docs,self.chat_history):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 157, in astream_summary
    async for answer in chain.astream({"context": doc}):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4988, in atransform
    async for output in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4891, in _atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4988, in atransform
    async for output in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4952, in _atransform
    output = await acall_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4927, in f
    return await run_in_executor(config, func, *args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 616, in run_in_executor
    return await asyncio.get_running_loop().run_in_executor(
  File "D:\Anaconda\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 607, in wrapper
    return func(*args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4921, in func
    return call_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 100, in <lambda>
    summaries=lambda x: asyncio.gather(*[
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 101, in <listcomp>
    self.map_chain.invoke_async({"context": context})
  File "D:\Anaconda\lib\site-packages\pydantic\main.py", line 891, in __getattr__
    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
AttributeError: 'RunnableSequence' object has no attribute 'invoke_async'
-chat_qa_chain_self_answer
2025-05-13 01:33:01,427 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 158, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 194, in abstract_answer
    summarizer = LCELSummarizer(self.llm, mode)
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 65, in __init__
    self._create_chains()
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 110, in _create_chains
    | RunnableMap(lambda x: self.map_prompt.invoke_async(x))  # 첽 map_prompt
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3562, in __init__
    merged = {**steps__} if steps__ is not None else {}
TypeError: 'function' object is not a mapping
-chat_qa_chain_self_answer
2025-05-13 01:52:03,918 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 158, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 207, in abstract_answer
    async for info ,answer in summarizer.astream_summary(docs,self.chat_history):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 197, in astream_summary
    async for answer in chain.astream({"context": doc}):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4988, in atransform
    async for output in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4952, in _atransform
    output = await acall_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4927, in f
    return await run_in_executor(config, func, *args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 616, in run_in_executor
    return await asyncio.get_running_loop().run_in_executor(
  File "D:\Anaconda\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 607, in wrapper
    return func(*args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4921, in func
    return call_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 121, in <lambda>
    texts=lambda x: [doc.page_content for doc in x["documents"]],
KeyError: 'documents'
-chat_qa_chain_self_answer
2025-05-13 14:57:34,350 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 158, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 207, in abstract_answer
    async for info ,answer in summarizer.astream_summary(docs,self.chat_history):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 101, in astream_summary
    chat_history[-1][1] += answer
IndexError: list index out of range
-chat_qa_chain_self_answer
2025-05-13 17:18:52,477 - root - ERROR - chat_process
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 101, in map_httpcore_exceptions
    yield
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 271, in __aiter__
    async for part in self._httpcore_stream:
  File "D:\Anaconda\lib\site-packages\httpcore\_async\connection_pool.py", line 407, in __aiter__
    raise exc from None
  File "D:\Anaconda\lib\site-packages\httpcore\_async\connection_pool.py", line 403, in __aiter__
    async for part in self._stream:
  File "D:\Anaconda\lib\site-packages\httpcore\_async\http11.py", line 342, in __aiter__
    raise exc
  File "D:\Anaconda\lib\site-packages\httpcore\_async\http11.py", line 334, in __aiter__
    async for chunk in self._connection._receive_response_body(**kwargs):
  File "D:\Anaconda\lib\site-packages\httpcore\_async\http11.py", line 203, in _receive_response_body
    event = await self._receive_event(timeout=timeout)
  File "D:\Anaconda\lib\site-packages\httpcore\_async\http11.py", line 214, in _receive_event
    event = self._h11_state.next_event()
  File "D:\Anaconda\lib\contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\Anaconda\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 167, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 228, in chat_answer
    "֮ǰġ"
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 43, in _atransform
    async for chunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 905, in _astream
    async for chunk in response:
  File "D:\Anaconda\lib\site-packages\openai\_streaming.py", line 147, in __aiter__
    async for item in self._iterator:
  File "D:\Anaconda\lib\site-packages\openai\_streaming.py", line 160, in __stream__
    async for sse in iterator:
  File "D:\Anaconda\lib\site-packages\openai\_streaming.py", line 151, in _iter_events
    async for sse in self._decoder.aiter_bytes(self.response.aiter_bytes()):
  File "D:\Anaconda\lib\site-packages\openai\_streaming.py", line 302, in aiter_bytes
    async for chunk in self._aiter_chunks(iterator):
  File "D:\Anaconda\lib\site-packages\openai\_streaming.py", line 313, in _aiter_chunks
    async for chunk in iterator:
  File "D:\Anaconda\lib\site-packages\httpx\_models.py", line 997, in aiter_bytes
    async for raw_bytes in self.aiter_raw():
  File "D:\Anaconda\lib\site-packages\httpx\_models.py", line 1055, in aiter_raw
    async for raw_stream_bytes in self.stream:
  File "D:\Anaconda\lib\site-packages\httpx\_client.py", line 176, in __aiter__
    async for chunk in self._stream:
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 272, in __aiter__
    yield part
  File "D:\Anaconda\lib\contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)
-chat_qa_chain_self_answer
2025-05-13 19:14:48,697 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 166, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 204, in abstract_answer
    async for info ,answer in summarizer.astream_summary(docs,self.chat_history):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 104, in astream_summary
    async for answer in chain.astream({"context": doc}):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 185, in astream
    yield result['generate_final_summary']['final_summary']
KeyError: 'generate_final_summary'
-chat_qa_chain_self_answer
2025-05-13 20:13:41,837 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 166, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 204, in abstract_answer
    async for info ,answer in summarizer.astream_summary(docs,self.chat_history):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 104, in astream_summary
    async for answer in chain.astream({"context": doc}):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 185, in astream
    yield result['generate_final_summary']['final_summary']
KeyError: 'generate_final_summary'
-chat_qa_chain_self_answer
2025-05-13 20:16:03,839 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 166, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 204, in abstract_answer
    async for info ,answer in summarizer.astream_summary(docs,self.chat_history):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 104, in astream_summary
    async for answer in chain.astream({"context": doc}):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 185, in astream
    yield result['generate_final_summary']['final_summary']
KeyError: 'generate_final_summary'
-chat_qa_chain_self_answer
2025-05-13 20:22:19,479 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 166, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 204, in abstract_answer
    async for info ,answer in summarizer.astream_summary(docs,self.chat_history):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 104, in astream_summary
    async for answer in chain.astream({"context": doc}):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 185, in astream
    yield result['generate_final_summary']['final_summary']
KeyError: 'generate_final_summary'
-chat_qa_chain_self_answer
2025-05-14 00:12:21,228 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 166, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 204, in abstract_answer
    async for info ,answer in summarizer.astream_summary(docs,self.chat_history):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 104, in astream_summary
    async for answer in chain.astream({"context": doc}):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 182, in astream
    async for result in self.map_reduce_chain.astream(
  File "D:\Anaconda\lib\site-packages\langgraph\pregel\__init__.py", line 2781, in astream
    raise GraphRecursionError(msg)
langgraph.errors.GraphRecursionError: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.
For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT
-chat_qa_chain_self_answer
2025-05-14 00:30:37,583 - root - ERROR - chat_process
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 101, in map_httpcore_exceptions
    yield
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 271, in __aiter__
    async for part in self._httpcore_stream:
  File "D:\Anaconda\lib\site-packages\httpcore\_async\connection_pool.py", line 407, in __aiter__
    raise exc from None
  File "D:\Anaconda\lib\site-packages\httpcore\_async\connection_pool.py", line 403, in __aiter__
    async for part in self._stream:
  File "D:\Anaconda\lib\site-packages\httpcore\_async\http11.py", line 342, in __aiter__
    raise exc
  File "D:\Anaconda\lib\site-packages\httpcore\_async\http11.py", line 334, in __aiter__
    async for chunk in self._connection._receive_response_body(**kwargs):
  File "D:\Anaconda\lib\site-packages\httpcore\_async\http11.py", line 203, in _receive_response_body
    event = await self._receive_event(timeout=timeout)
  File "D:\Anaconda\lib\site-packages\httpcore\_async\http11.py", line 214, in _receive_event
    event = self._h11_state.next_event()
  File "D:\Anaconda\lib\contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\Anaconda\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 166, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 204, in abstract_answer
    async for info ,answer in summarizer.astream_summary(docs,self.chat_history):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 104, in astream_summary
    async for answer in chain.astream({"context": doc}):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 182, in astream
    async for result in self.map_reduce_chain.astream(
  File "D:\Anaconda\lib\site-packages\langgraph\pregel\__init__.py", line 2759, in astream
    async for _ in runner.atick(
  File "D:\Anaconda\lib\site-packages\langgraph\pregel\runner.py", line 283, in atick
    await arun_with_retry(
  File "D:\Anaconda\lib\site-packages\langgraph\pregel\retry.py", line 128, in arun_with_retry
    return await task.proc.ainvoke(task.input, config)
  File "D:\Anaconda\lib\site-packages\langgraph\utils\runnable.py", line 676, in ainvoke
    input = await step.ainvoke(input, config, **kwargs)
  File "D:\Anaconda\lib\site-packages\langgraph\utils\runnable.py", line 440, in ainvoke
    ret = await self.afunc(*args, **kwargs)
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 244, in collapse_summaries
    results = []
  File "D:\Anaconda\lib\site-packages\langchain\chains\combine_documents\reduce.py", line 114, in acollapse_docs
    result = await combine_document_func(docs, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3075, in ainvoke
    input = await coro_with_context(part(), context, create_task=True)
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 391, in ainvoke
    llm_result = await self.agenerate_prompt(
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 957, in agenerate_prompt
    return await self.agenerate(
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 915, in agenerate
    raise exceptions[0]
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 1083, in _agenerate_with_cache
    result = await self._agenerate(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 949, in _agenerate
    return await agenerate_from_stream(stream_iter)
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 180, in agenerate_from_stream
    chunks = [chunk async for chunk in stream]
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 180, in <listcomp>
    chunks = [chunk async for chunk in stream]
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 905, in _astream
    async for chunk in response:
  File "D:\Anaconda\lib\site-packages\openai\_streaming.py", line 147, in __aiter__
    async for item in self._iterator:
  File "D:\Anaconda\lib\site-packages\openai\_streaming.py", line 160, in __stream__
    async for sse in iterator:
  File "D:\Anaconda\lib\site-packages\openai\_streaming.py", line 151, in _iter_events
    async for sse in self._decoder.aiter_bytes(self.response.aiter_bytes()):
  File "D:\Anaconda\lib\site-packages\openai\_streaming.py", line 302, in aiter_bytes
    async for chunk in self._aiter_chunks(iterator):
  File "D:\Anaconda\lib\site-packages\openai\_streaming.py", line 313, in _aiter_chunks
    async for chunk in iterator:
  File "D:\Anaconda\lib\site-packages\httpx\_models.py", line 997, in aiter_bytes
    async for raw_bytes in self.aiter_raw():
  File "D:\Anaconda\lib\site-packages\httpx\_models.py", line 1055, in aiter_raw
    async for raw_stream_bytes in self.stream:
  File "D:\Anaconda\lib\site-packages\httpx\_client.py", line 176, in __aiter__
    async for chunk in self._stream:
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 272, in __aiter__
    yield part
  File "D:\Anaconda\lib\contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)
-chat_qa_chain_self_answer
2025-05-14 01:00:05,012 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 166, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 202, in abstract_answer
    summarizer = LCELSummarizer(self.llm, mode)
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 75, in __init__
    self._create_chains()
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 81, in _create_chains
    self.map_reduce_chain = MapReduceChain(self.llm,self.token_max,self.map_prompt,self.reduce_prompt,
TypeError: __init__() got multiple values for argument 'document_variable_name'
-chat_qa_chain_self_answer
2025-05-14 01:37:12,841 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 166, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 204, in abstract_answer
    async for info ,answer in summarizer.astream_summary(docs,self.chat_history):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 104, in astream_summary
    async for answer in chain.astream({"context": doc}):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 182, in astream
    async for result in self.map_reduce_chain.astream(
  File "D:\Anaconda\lib\site-packages\langgraph\pregel\__init__.py", line 2759, in astream
    async for _ in runner.atick(
  File "D:\Anaconda\lib\site-packages\langgraph\pregel\runner.py", line 392, in atick
    _panic_or_proceed(
  File "D:\Anaconda\lib\site-packages\langgraph\pregel\runner.py", line 499, in _panic_or_proceed
    raise exc
  File "D:\Anaconda\lib\site-packages\langgraph\pregel\retry.py", line 128, in arun_with_retry
    return await task.proc.ainvoke(task.input, config)
  File "D:\Anaconda\lib\site-packages\langgraph\utils\runnable.py", line 676, in ainvoke
    input = await step.ainvoke(input, config, **kwargs)
  File "D:\Anaconda\lib\site-packages\langgraph\utils\runnable.py", line 440, in ainvoke
    ret = await self.afunc(*args, **kwargs)
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 217, in generate_summary
    response = await self.map_chain.ainvoke(new_state)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3075, in ainvoke
    input = await coro_with_context(part(), context, create_task=True)
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 391, in ainvoke
    llm_result = await self.agenerate_prompt(
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 957, in agenerate_prompt
    return await self.agenerate(
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 915, in agenerate
    raise exceptions[0]
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 1083, in _agenerate_with_cache
    result = await self._agenerate(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 949, in _agenerate
    return await agenerate_from_stream(stream_iter)
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 180, in agenerate_from_stream
    chunks = [chunk async for chunk in stream]
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 180, in <listcomp>
    chunks = [chunk async for chunk in stream]
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 900, in _astream
    response = await self.async_client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 1927, in create
    return await self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1562, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - [{'error': {'code': 400, 'message': '* GenerateContentRequest.contents: contents is not specified\n', 'status': 'INVALID_ARGUMENT'}}]
-chat_qa_chain_self_answer
2025-05-14 01:57:28,983 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 166, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 202, in abstract_answer
    summarizer = LCELSummarizer(self.llm, mode)
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 84, in __init__
    self._create_chains()
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 93, in _create_chains
    self.refine_chain = create_refine_chain(self.llm, self.initial_refine_prompt, self.refine_prompt,
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 133, in create_refine_chain
    return RefineChain(llm, initial_refine_prompt, refine_prompt, document_variable_name=document_variable_name)
NameError: name 'RefineChain' is not defined
-chat_qa_chain_self_answer
2025-05-14 02:31:11,775 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 166, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 204, in abstract_answer
    async for info ,answer in summarizer.astream_summary(docs,self.chat_history):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 118, in astream_summary
    chat_history[-1][1] += answer
  File "D:\Anaconda\lib\site-packages\langgraph\pregel\io.py", line 107, in __radd__
    return other | self
TypeError: unsupported operand type(s) for |: 'str' and 'AddableValuesDict'
-chat_qa_chain_self_answer
2025-05-14 02:37:34,417 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 166, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 204, in abstract_answer
    async for info ,answer in summarizer.astream_summary(docs,self.chat_history):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 117, in astream_summary
    async for answer in chain.astream({"context": doc}):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 326, in astream
    async for step in self.refine_chain.astream(input_dict,stream_mode="values"):
  File "D:\Anaconda\lib\site-packages\langgraph\pregel\__init__.py", line 2759, in astream
    async for _ in runner.atick(
  File "D:\Anaconda\lib\site-packages\langgraph\pregel\runner.py", line 283, in atick
    await arun_with_retry(
  File "D:\Anaconda\lib\site-packages\langgraph\pregel\retry.py", line 128, in arun_with_retry
    return await task.proc.ainvoke(task.input, config)
  File "D:\Anaconda\lib\site-packages\langgraph\utils\runnable.py", line 676, in ainvoke
    input = await step.ainvoke(input, config, **kwargs)
  File "D:\Anaconda\lib\site-packages\langgraph\utils\runnable.py", line 440, in ainvoke
    ret = await self.afunc(*args, **kwargs)
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 340, in generate_initial_summary
    summary = await self.initial_summary_chain.ainvoke(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3075, in ainvoke
    input = await coro_with_context(part(), context, create_task=True)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 242, in ainvoke
    return await self._acall_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1981, in _acall_with_config
    output: Output = await coro_with_context(coro, context)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 195, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 183, in _validate_input
    raise KeyError(
KeyError: "Input to ChatPromptTemplate is missing variables {'context'}.  Expected: ['context'] Received: []\nNote: if you intended {context} to be part of the string and not a variable, please escape it with double curly braces like: '{{context}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
-chat_qa_chain_self_answer
2025-05-14 02:42:12,178 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 166, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 204, in abstract_answer
    async for info ,answer in summarizer.astream_summary(docs,self.chat_history):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 117, in astream_summary
    async for answer in chain.astream({"context": doc}):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 329, in astream
    async for step in self.refine_chain.astream(input_dict,stream_mode="values"):
  File "D:\Anaconda\lib\site-packages\langgraph\pregel\__init__.py", line 2759, in astream
    async for _ in runner.atick(
  File "D:\Anaconda\lib\site-packages\langgraph\pregel\runner.py", line 283, in atick
    await arun_with_retry(
  File "D:\Anaconda\lib\site-packages\langgraph\pregel\retry.py", line 128, in arun_with_retry
    return await task.proc.ainvoke(task.input, config)
  File "D:\Anaconda\lib\site-packages\langgraph\utils\runnable.py", line 676, in ainvoke
    input = await step.ainvoke(input, config, **kwargs)
  File "D:\Anaconda\lib\site-packages\langgraph\utils\runnable.py", line 440, in ainvoke
    ret = await self.afunc(*args, **kwargs)
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 353, in refine_summary
    summary = await self.refine_summary_chain.ainvoke(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3075, in ainvoke
    input = await coro_with_context(part(), context, create_task=True)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 242, in ainvoke
    return await self._acall_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1981, in _acall_with_config
    output: Output = await coro_with_context(coro, context)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 195, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 183, in _validate_input
    raise KeyError(
KeyError: "Input to ChatPromptTemplate is missing variables {'existing_summary', 'new_text'}.  Expected: ['existing_summary', 'new_text'] Received: ['existing_answer', 'context']\nNote: if you intended {existing_summary} to be part of the string and not a variable, please escape it with double curly braces like: '{{existing_summary}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
-chat_qa_chain_self_answer
2025-05-14 02:44:37,238 - root - ERROR - chat_process
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 101, in map_httpcore_exceptions
    yield
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 271, in __aiter__
    async for part in self._httpcore_stream:
  File "D:\Anaconda\lib\site-packages\httpcore\_async\connection_pool.py", line 407, in __aiter__
    raise exc from None
  File "D:\Anaconda\lib\site-packages\httpcore\_async\connection_pool.py", line 403, in __aiter__
    async for part in self._stream:
  File "D:\Anaconda\lib\site-packages\httpcore\_async\http11.py", line 342, in __aiter__
    raise exc
  File "D:\Anaconda\lib\site-packages\httpcore\_async\http11.py", line 334, in __aiter__
    async for chunk in self._connection._receive_response_body(**kwargs):
  File "D:\Anaconda\lib\site-packages\httpcore\_async\http11.py", line 203, in _receive_response_body
    event = await self._receive_event(timeout=timeout)
  File "D:\Anaconda\lib\site-packages\httpcore\_async\http11.py", line 214, in _receive_event
    event = self._h11_state.next_event()
  File "D:\Anaconda\lib\contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\Anaconda\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 166, in answer
    async for info, answer in abstract_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 204, in abstract_answer
    async for info ,answer in summarizer.astream_summary(docs,self.chat_history):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 117, in astream_summary
    async for answer in chain.astream({"context": doc}):
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 329, in astream
    async for step in self.refine_chain.astream(input_dict,stream_mode="values"):
  File "D:\Anaconda\lib\site-packages\langgraph\pregel\__init__.py", line 2759, in astream
    async for _ in runner.atick(
  File "D:\Anaconda\lib\site-packages\langgraph\pregel\runner.py", line 283, in atick
    await arun_with_retry(
  File "D:\Anaconda\lib\site-packages\langgraph\pregel\retry.py", line 128, in arun_with_retry
    return await task.proc.ainvoke(task.input, config)
  File "D:\Anaconda\lib\site-packages\langgraph\utils\runnable.py", line 676, in ainvoke
    input = await step.ainvoke(input, config, **kwargs)
  File "D:\Anaconda\lib\site-packages\langgraph\utils\runnable.py", line 440, in ainvoke
    ret = await self.afunc(*args, **kwargs)
  File "B:\notebook\Rag_Assistant\project\qa_chain\summarize_chain.py", line 353, in refine_summary
    summary = await self.refine_summary_chain.ainvoke(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3075, in ainvoke
    input = await coro_with_context(part(), context, create_task=True)
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 391, in ainvoke
    llm_result = await self.agenerate_prompt(
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 957, in agenerate_prompt
    return await self.agenerate(
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 915, in agenerate
    raise exceptions[0]
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 1083, in _agenerate_with_cache
    result = await self._agenerate(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 949, in _agenerate
    return await agenerate_from_stream(stream_iter)
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 180, in agenerate_from_stream
    chunks = [chunk async for chunk in stream]
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 180, in <listcomp>
    chunks = [chunk async for chunk in stream]
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 905, in _astream
    async for chunk in response:
  File "D:\Anaconda\lib\site-packages\openai\_streaming.py", line 147, in __aiter__
    async for item in self._iterator:
  File "D:\Anaconda\lib\site-packages\openai\_streaming.py", line 160, in __stream__
    async for sse in iterator:
  File "D:\Anaconda\lib\site-packages\openai\_streaming.py", line 151, in _iter_events
    async for sse in self._decoder.aiter_bytes(self.response.aiter_bytes()):
  File "D:\Anaconda\lib\site-packages\openai\_streaming.py", line 302, in aiter_bytes
    async for chunk in self._aiter_chunks(iterator):
  File "D:\Anaconda\lib\site-packages\openai\_streaming.py", line 313, in _aiter_chunks
    async for chunk in iterator:
  File "D:\Anaconda\lib\site-packages\httpx\_models.py", line 997, in aiter_bytes
    async for raw_bytes in self.aiter_raw():
  File "D:\Anaconda\lib\site-packages\httpx\_models.py", line 1055, in aiter_raw
    async for raw_stream_bytes in self.stream:
  File "D:\Anaconda\lib\site-packages\httpx\_client.py", line 176, in __aiter__
    async for chunk in self._stream:
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 272, in __aiter__
    yield part
  File "D:\Anaconda\lib\contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)
-chat_qa_chain_self_answer
2025-05-17 11:51:09,864 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 52, in chat_qa_chain_self_answer
    Chat_QA_chain_self(model_type=model_type, model=model,local_llm=local_llm,
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 68, in __init__
    self.summarizer = LCELBrowser(
TypeError: __init__() missing 1 required positional argument: 'llm'
-chat_qa_chain_self_answer
2025-05-17 11:56:38,464 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 208, in answer
    > LLM_WINDOW[self.model][self.model] * 0.9:  # һ
KeyError: 'gpt-3.5-turbo'
-chat_qa_chain_self_answer
2025-05-17 11:58:05,761 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 243, in answer
    self.total_tokens[-1] += self.llm.get_num_tokens(self.chat_history[-1][1])
IndexError: list index out of range
-chat_qa_chain_self_answer
2025-05-17 12:02:25,134 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 226, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 249, in chat_answer
    chat_chain = self.create_chat_chain(document_variable_names, **kwargs)
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 262, in create_chat_chain
    if not isinstance(self.retriever, BaseRetriever):
AttributeError: 'Chat_QA_chain_self' object has no attribute 'retriever'
-chat_qa_chain_self_answer
2025-05-17 12:08:22,835 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 228, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 251, in chat_answer
    chat_chain = self.create_chat_chain(document_variable_names, **kwargs)
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 272, in create_chat_chain
    **{document_variable_names[1]: RunnableLambda(self.get_ref_ui).with_config(
  File "B:\notebook\Rag_Assistant\project\prompt\prompt.py", line 82, in _validate_prompt
    raise ValueError(
ValueError: Prompt must accept chat_history as an input variable. Received prompt with input variables: ['context', 'input', 'rag_context']
-chat_qa_chain_self_answer
2025-05-17 12:23:19,814 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 226, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 249, in chat_answer
    chat_chain = self.create_chat_chain(document_variable_names, **kwargs)
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 275, in create_chat_chain
    _validate_prompt(document_variable_names, self.qa_prompt)
  File "B:\notebook\Rag_Assistant\project\prompt\prompt.py", line 80, in _validate_prompt
    for document_variable_name in document_variable_names:
TypeError: 'NoneType' object is not iterable
-chat_qa_chain_self_answer
2025-05-17 12:24:53,369 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 226, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 249, in chat_answer
    chat_chain = self.create_chat_chain(document_variable_names, **kwargs)
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 286, in create_chat_chain
    deal_docs_chain = RunnablePassthrough.assign(**{document_variable_names[0]: format_docs}).with_config(
TypeError: 'NoneType' object is not subscriptable
-chat_qa_chain_self_answer
2025-05-17 12:42:08,369 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 226, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 251, in chat_answer
    chat_chain = self.create_chat_chain(document_variable_names, **kwargs)
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 288, in create_chat_chain
    deal_docs_chain = RunnablePassthrough.assign(**{self.document_variable_names[0]: format_docs}).with_config(
AttributeError: 'Chat_QA_chain_self' object has no attribute 'document_variable_names'
-chat_qa_chain_self_answer
2025-05-17 12:42:57,225 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 226, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 251, in chat_answer
    chat_chain = self.create_chat_chain(document_variable_names, **kwargs)
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 293, in create_chat_chain
    **{document_variable_names[1]: retrieval_docs.with_config(run_name="retrieve_documents")},
TypeError: with_config() missing 1 required positional argument: 'self'
-chat_qa_chain_self_answer
2025-05-17 13:41:36,784 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 226, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 251, in chat_answer
    chat_chain = self.create_chat_chain(document_variable_names, **kwargs)
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 331, in create_chat_chain
    **{document_variable_names[1]: retrieval_docs.with_config(run_name="retrieve_documents")},
TypeError: with_config() missing 1 required positional argument: 'self'
-chat_qa_chain_self_answer
2025-05-17 13:47:02,555 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 226, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 257, in chat_answer
    async for info, answer in self.astream(question, chat_chain, chain_config, document_variable_names,**kwargs):
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 353, in astream
    async for chunk in chat_chain.astream(chain_config):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\branch.py", line 425, in astream
    expression_value = await condition.ainvoke(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4783, in ainvoke
    return await self._acall_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1981, in _acall_with_config
    output: Output = await coro_with_context(coro, context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4715, in _ainvoke
    output = await acall_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4685, in f
    return await run_in_executor(config, func, *args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 616, in run_in_executor
    return await asyncio.get_running_loop().run_in_executor(
  File "D:\Anaconda\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 607, in wrapper
    return func(*args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4679, in func
    return call_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
TypeError: <lambda>() takes 0 positional arguments but 1 was given
-chat_qa_chain_self_answer
2025-05-17 14:41:59,879 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 224, in answer
    self.get_retriever(**kwargs)
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 97, in get_retriever
    self.retriever = BaseRetriever()
TypeError: Can't instantiate abstract class BaseRetriever with abstract method _get_relevant_documents
-chat_qa_chain_self_answer
2025-05-17 14:44:42,328 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 226, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 257, in chat_answer
    async for info, answer in self.astream(question, chat_chain, chain_config, document_variable_names,**kwargs):
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 353, in astream
    async for chunk in chat_chain.astream(chain_config):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\branch.py", line 425, in astream
    expression_value = await condition.ainvoke(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4783, in ainvoke
    return await self._acall_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1981, in _acall_with_config
    output: Output = await coro_with_context(coro, context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4715, in _ainvoke
    output = await acall_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4685, in f
    return await run_in_executor(config, func, *args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 616, in run_in_executor
    return await asyncio.get_running_loop().run_in_executor(
  File "D:\Anaconda\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 607, in wrapper
    return func(*args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4679, in func
    return call_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
TypeError: <lambda>() takes 0 positional arguments but 1 was given
-chat_qa_chain_self_answer
2025-05-17 14:58:21,237 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 241, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 272, in chat_answer
    async for info, answer in self.astream(question, chat_chain, chain_config, document_variable_names,**kwargs):
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 368, in astream
    async for chunk in chat_chain.astream(chain_config):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1024, in astream
    yield await self.ainvoke(input, config, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 242, in ainvoke
    return await self._acall_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1981, in _acall_with_config
    output: Output = await coro_with_context(coro, context)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 195, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 183, in _validate_input
    raise KeyError(
KeyError: "Input to ChatPromptTemplate is missing variables {'rag_context'}.  Expected: ['context', 'input', 'rag_context'] Received: ['input', 'chat_history', 'context']\nNote: if you intended {rag_context} to be part of the string and not a variable, please escape it with double curly braces like: '{{rag_context}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
-chat_qa_chain_self_answer
2025-05-17 15:15:20,529 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 240, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 265, in chat_answer
    chat_chain = self.create_chat_chain(document_variable_names, **kwargs)
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 353, in create_chat_chain
    RunnablePassthrough.assign(**{document_variable_names[1]: []}).with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 233, in assign
    return RunnableAssign(RunnableParallel[dict[str, Any]](kwargs))
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3565, in __init__
    steps__={key: coerce_to_runnable(r) for key, r in merged.items()}
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3565, in <dictcomp>
    steps__={key: coerce_to_runnable(r) for key, r in merged.items()}
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5915, in coerce_to_runnable
    raise TypeError(msg)
TypeError: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'list'>
-chat_qa_chain_self_answer
2025-05-17 15:16:47,315 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 240, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 265, in chat_answer
    chat_chain = self.create_chat_chain(document_variable_names, **kwargs)
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 353, in create_chat_chain
    RunnablePassthrough.assign(**{document_variable_names[1]: []}).with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 233, in assign
    return RunnableAssign(RunnableParallel[dict[str, Any]](kwargs))
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3565, in __init__
    steps__={key: coerce_to_runnable(r) for key, r in merged.items()}
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3565, in <dictcomp>
    steps__={key: coerce_to_runnable(r) for key, r in merged.items()}
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5915, in coerce_to_runnable
    raise TypeError(msg)
TypeError: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'list'>
-chat_qa_chain_self_answer
2025-05-17 15:23:06,911 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 240, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 265, in chat_answer
    chat_chain = self.create_chat_chain(document_variable_names, **kwargs)
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 353, in create_chat_chain
    RunnablePassthrough.assign(**{document_variable_names[1]: []}).with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 233, in assign
    return RunnableAssign(RunnableParallel[dict[str, Any]](kwargs))
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3565, in __init__
    steps__={key: coerce_to_runnable(r) for key, r in merged.items()}
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3565, in <dictcomp>
    steps__={key: coerce_to_runnable(r) for key, r in merged.items()}
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5915, in coerce_to_runnable
    raise TypeError(msg)
TypeError: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'list'>
-chat_qa_chain_self_answer
2025-05-17 15:30:47,927 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 240, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 272, in chat_answer
    async for info, answer in self.astream(question, chat_chain, chain_config, document_variable_names, **kwargs):
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 372, in astream
    if chunk.get("input", False) or chunk.get(document_variable_names[0], False):  # һquestion
AttributeError: 'str' object has no attribute 'get'
-chat_qa_chain_self_answer
2025-05-17 15:33:34,924 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 240, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 272, in chat_answer
    async for info, answer in self.astream(question, chat_chain, chain_config, document_variable_names, **kwargs):
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 372, in astream
    if chunk.get("input", False) or chunk.get(document_variable_names[0], False):  # һquestion
AttributeError: 'str' object has no attribute 'get'
-chat_qa_chain_self_answer
2025-05-17 15:37:31,815 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 240, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 273, in chat_answer
    self.chat_history[-1][1] += answer
TypeError: can only concatenate str (not "list") to str
-chat_qa_chain_self_answer
2025-05-17 15:39:16,742 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 240, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 273, in chat_answer
    self.chat_history[-1][1] += answer
TypeError: can only concatenate str (not "list") to str
-chat_qa_chain_self_answer
2025-05-17 15:39:23,408 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 240, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 273, in chat_answer
    self.chat_history[-1][1] += answer
TypeError: can only concatenate str (not "list") to str
-chat_qa_chain_self_answer
2025-05-17 15:51:06,372 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 240, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 272, in chat_answer
    async for info, answer in self.astream(question, chat_chain, chain_config, document_variable_names, **kwargs):
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 371, in astream
    async for chunk in chat_chain.astream(chain_config):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 624, in _atransform
    async for chunk in for_passthrough:
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\branch.py", line 434, in astream
    async for chunk in runnable.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 671, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4988, in atransform
    async for output in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4952, in _atransform
    output = await acall_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4927, in f
    return await run_in_executor(config, func, *args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 616, in run_in_executor
    return await asyncio.get_running_loop().run_in_executor(
  File "D:\Anaconda\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 607, in wrapper
    return func(*args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4921, in func
    return call_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 297, in format_docs
    return "\n\n".join(
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 298, in <genexpr>
    format_document(doc, _document_prompt)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 446, in format_document
    return prompt.format(**_get_document_info(doc, prompt))
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 395, in _get_document_info
    base_info = {"page_content": doc.page_content, **doc.metadata}
AttributeError: 'NamedString' object has no attribute 'page_content'
-chat_qa_chain_self_answer
2025-05-17 15:57:48,486 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-1073' coro=<py_anext.<locals>.anext_impl() done, defined at D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py:71> exception=KeyError("Input to ChatPromptTemplate is missing variables {'context'}.  Expected: ['context', 'input', 'rag_context'] Received: ['input', 'chat_history', 'rag_context']\nNote: if you intended {context} to be part of the string and not a variable, please escape it with double curly braces like: '{{context}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT ")>-default_exception_handler
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1024, in astream
    yield await self.ainvoke(input, config, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 242, in ainvoke
    return await self._acall_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1981, in _acall_with_config
    output: Output = await coro_with_context(coro, context)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 195, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 183, in _validate_input
    raise KeyError(
KeyError: "Input to ChatPromptTemplate is missing variables {'context'}.  Expected: ['context', 'input', 'rag_context'] Received: ['input', 'chat_history', 'rag_context']\nNote: if you intended {context} to be part of the string and not a variable, please escape it with double curly braces like: '{{context}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-05-17 18:07:36,893 - root - ERROR - chat_process
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 101, in map_httpcore_exceptions
    yield
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 394, in handle_async_request
    resp = await self._pool.handle_async_request(req)
  File "D:\Anaconda\lib\site-packages\httpcore\_async\connection_pool.py", line 256, in handle_async_request
    raise exc from None
  File "D:\Anaconda\lib\site-packages\httpcore\_async\connection_pool.py", line 236, in handle_async_request
    response = await connection.handle_async_request(
  File "D:\Anaconda\lib\site-packages\httpcore\_async\connection.py", line 101, in handle_async_request
    raise exc
  File "D:\Anaconda\lib\site-packages\httpcore\_async\connection.py", line 78, in handle_async_request
    stream = await self._connect(request)
  File "D:\Anaconda\lib\site-packages\httpcore\_async\connection.py", line 124, in _connect
    stream = await self._network_backend.connect_tcp(**kwargs)
  File "D:\Anaconda\lib\site-packages\httpcore\_backends\auto.py", line 31, in connect_tcp
    return await self._backend.connect_tcp(
  File "D:\Anaconda\lib\site-packages\httpcore\_backends\anyio.py", line 122, in connect_tcp
    stream._raw_socket.setsockopt(*option)  # type: ignore[attr-defined] # pragma: no cover
  File "D:\Anaconda\lib\contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\Anaconda\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: All connection attempts failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1500, in _request
    response = await self._client.send(
  File "D:\Anaconda\lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
  File "D:\Anaconda\lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
  File "D:\Anaconda\lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
  File "D:\Anaconda\lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 394, in handle_async_request
    resp = await self._pool.handle_async_request(req)
  File "D:\Anaconda\lib\contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: All connection attempts failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 240, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 272, in chat_answer
    async for info, answer in self.astream(question, chat_chain, chain_config, document_variable_names, **kwargs):
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 371, in astream
    async for chunk in chat_chain.astream(chain_config):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 900, in _astream
    response = await self.async_client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 1927, in create
    return await self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1524, in _request
    return await self._retry_request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1594, in _retry_request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1524, in _request
    return await self._retry_request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1594, in _retry_request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1534, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
-chat_qa_chain_self_answer
2025-05-17 18:09:39,133 - root - ERROR - chat_process
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 101, in map_httpcore_exceptions
    yield
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 394, in handle_async_request
    resp = await self._pool.handle_async_request(req)
  File "D:\Anaconda\lib\site-packages\httpcore\_async\connection_pool.py", line 256, in handle_async_request
    raise exc from None
  File "D:\Anaconda\lib\site-packages\httpcore\_async\connection_pool.py", line 236, in handle_async_request
    response = await connection.handle_async_request(
  File "D:\Anaconda\lib\site-packages\httpcore\_async\connection.py", line 101, in handle_async_request
    raise exc
  File "D:\Anaconda\lib\site-packages\httpcore\_async\connection.py", line 78, in handle_async_request
    stream = await self._connect(request)
  File "D:\Anaconda\lib\site-packages\httpcore\_async\connection.py", line 124, in _connect
    stream = await self._network_backend.connect_tcp(**kwargs)
  File "D:\Anaconda\lib\site-packages\httpcore\_backends\auto.py", line 31, in connect_tcp
    return await self._backend.connect_tcp(
  File "D:\Anaconda\lib\site-packages\httpcore\_backends\anyio.py", line 122, in connect_tcp
    stream._raw_socket.setsockopt(*option)  # type: ignore[attr-defined] # pragma: no cover
  File "D:\Anaconda\lib\contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\Anaconda\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: All connection attempts failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1500, in _request
    response = await self._client.send(
  File "D:\Anaconda\lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
  File "D:\Anaconda\lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
  File "D:\Anaconda\lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
  File "D:\Anaconda\lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 394, in handle_async_request
    resp = await self._pool.handle_async_request(req)
  File "D:\Anaconda\lib\contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: All connection attempts failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 240, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 272, in chat_answer
    async for info, answer in self.astream(question, chat_chain, chain_config, document_variable_names, **kwargs):
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 371, in astream
    async for chunk in chat_chain.astream(chain_config):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 900, in _astream
    response = await self.async_client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 1927, in create
    return await self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1524, in _request
    return await self._retry_request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1594, in _retry_request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1524, in _request
    return await self._retry_request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1594, in _retry_request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1534, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
-chat_qa_chain_self_answer
2025-05-17 18:25:39,409 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 240, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 272, in chat_answer
    async for info, answer in self.astream(question, chat_chain, chain_config, document_variable_names, **kwargs):
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 372, in astream
    async for chunk in chat_chain.astream(chain_config):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 624, in _atransform
    async for chunk in for_passthrough:
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\branch.py", line 434, in astream
    async for chunk in runnable.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 671, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4988, in atransform
    async for output in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4952, in _atransform
    output = await acall_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4927, in f
    return await run_in_executor(config, func, *args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 616, in run_in_executor
    return await asyncio.get_running_loop().run_in_executor(
  File "D:\Anaconda\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 607, in wrapper
    return func(*args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4921, in func
    return call_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 297, in format_docs
    return "\n\n".join(
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 298, in <genexpr>
    format_document(doc, _document_prompt)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 446, in format_document
    return prompt.format(**_get_document_info(doc, prompt))
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 395, in _get_document_info
    base_info = {"page_content": doc.page_content, **doc.metadata}
AttributeError: 'NamedString' object has no attribute 'page_content'
-chat_qa_chain_self_answer
2025-05-17 18:41:40,088 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 242, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 297, in chat_answer
    async for info, answer in self.astream(question, chat_chain, chain_config, document_variable_names, **kwargs):
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 397, in astream
    async for chunk in chat_chain.astream(chain_config):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 624, in _atransform
    async for chunk in for_passthrough:
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\branch.py", line 434, in astream
    async for chunk in runnable.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5606, in astream
    async for item in self.bound.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 671, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4988, in atransform
    async for output in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4952, in _atransform
    output = await acall_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4927, in f
    return await run_in_executor(config, func, *args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 616, in run_in_executor
    return await asyncio.get_running_loop().run_in_executor(
  File "D:\Anaconda\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 607, in wrapper
    return func(*args, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 4921, in func
    return call_func_with_variable_args(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 322, in format_docs
    return "\n\n".join(
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 323, in <genexpr>
    format_document(doc, _document_prompt)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 446, in format_document
    return prompt.format(**_get_document_info(doc, prompt))
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 395, in _get_document_info
    base_info = {"page_content": doc.page_content, **doc.metadata}
AttributeError: 'tuple' object has no attribute 'page_content'
-chat_qa_chain_self_answer
2025-05-17 18:42:01,506 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-46' coro=<py_anext.<locals>.anext_impl() done, defined at D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py:71> exception=KeyError("Input to ChatPromptTemplate is missing variables {'context'}.  Expected: ['context', 'input', 'rag_context'] Received: ['input', 'chat_history', 'rag_context']\nNote: if you intended {context} to be part of the string and not a variable, please escape it with double curly braces like: '{{context}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT ")>-default_exception_handler
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1024, in astream
    yield await self.ainvoke(input, config, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 242, in ainvoke
    return await self._acall_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1981, in _acall_with_config
    output: Output = await coro_with_context(coro, context)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 195, in _aformat_prompt_with_error_handling
    _inner_input = self._validate_input(inner_input)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 183, in _validate_input
    raise KeyError(
KeyError: "Input to ChatPromptTemplate is missing variables {'context'}.  Expected: ['context', 'input', 'rag_context'] Received: ['input', 'chat_history', 'rag_context']\nNote: if you intended {context} to be part of the string and not a variable, please escape it with double curly braces like: '{{context}}'.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
2025-05-17 18:58:36,818 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 242, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 297, in chat_answer
    async for info, answer in self.astream(question, chat_chain, chain_config, document_variable_names, **kwargs):
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 397, in astream
    async for chunk in chat_chain.astream(chain_config):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 900, in _astream
    response = await self.async_client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 1927, in create
    return await self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1562, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.PermissionDeniedError: Error code: 403 - {'error': {'message': 'APIģtokenС4096и https://api.chatanywhere.tech/#/shop 򸶷APIThe number of prompt tokens for free accounts is limited to 4096. If you have additional requirements, please visit https://api.chatanywhere.tech/#/shop to purchase a premium key.(ǰʹõApiKey: sk-Hsr****fpgt)', 'type': 'chatanywhere_error', 'param': None, 'code': '403 FORBIDDEN'}}
-chat_qa_chain_self_answer
2025-05-18 10:37:14,955 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 267, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 322, in chat_answer
    async for info, answer in self.astream(question, chat_chain, chain_config, document_variable_names, **kwargs):
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 422, in astream
    async for chunk in chat_chain.astream(chain_config):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\branch.py", line 434, in astream
    async for chunk in runnable.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 624, in _atransform
    async for chunk in for_passthrough:
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\branch.py", line 434, in astream
    async for chunk in runnable.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1024, in astream
    yield await self.ainvoke(input, config, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 242, in ainvoke
    return await self._acall_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1981, in _acall_with_config
    output: Output = await coro_with_context(coro, context)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 196, in _aformat_prompt_with_error_handling
    return await self.aformat_prompt(**_inner_input)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\chat.py", line 737, in aformat_prompt
    messages = await self.aformat_messages(**kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\chat.py", line 1214, in aformat_messages
    message = await message_template.aformat_messages(**kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\message.py", line 55, in aformat_messages
    return self.format_messages(**kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\chat.py", line 184, in format_messages
    value = convert_to_messages(value)
  File "D:\Anaconda\lib\site-packages\langchain_core\messages\utils.py", line 367, in convert_to_messages
    return [_convert_to_message(m) for m in messages]
  File "D:\Anaconda\lib\site-packages\langchain_core\messages\utils.py", line 367, in <listcomp>
    return [_convert_to_message(m) for m in messages]
  File "D:\Anaconda\lib\site-packages\langchain_core\messages\utils.py", line 346, in _convert_to_message
    raise NotImplementedError(msg)
NotImplementedError: Unsupported message type: <class 'langchain_core.documents.base.Document'>
For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE 
-chat_qa_chain_self_answer
2025-05-18 11:06:13,612 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 276, in answer
    self.get_retriever(**kwargs)
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 153, in get_retriever
    self.retriever = self.create_chat_retriever(
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 160, in create_chat_retriever
    _validate_prompt(document_variable_names=["input"]+[document_variable_name], prompt=prompt)
  File "B:\notebook\Rag_Assistant\project\prompt\prompt.py", line 82, in _validate_prompt
    raise ValueError(
ValueError: Prompt must accept context as an input variable. Received prompt with input variables: ['input']
-chat_qa_chain_self_answer
2025-05-18 11:10:39,964 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 278, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 319, in chat_answer
    async for info, answer in self.astream(question, chat_chain, chain_config, document_variable_names, **kwargs):
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 419, in astream
    async for chunk in chat_chain.astream(chain_config):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\branch.py", line 434, in astream
    async for chunk in runnable.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 624, in _atransform
    async for chunk in for_passthrough:
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 5645, in atransform
    async for item in self.bound.atransform(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\branch.py", line 434, in astream
    async for chunk in runnable.astream(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1461, in atransform
    async for ichunk in input:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1024, in astream
    yield await self.ainvoke(input, config, **kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 242, in ainvoke
    return await self._acall_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1981, in _acall_with_config
    output: Output = await coro_with_context(coro, context)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\base.py", line 196, in _aformat_prompt_with_error_handling
    return await self.aformat_prompt(**_inner_input)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\chat.py", line 737, in aformat_prompt
    messages = await self.aformat_messages(**kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\chat.py", line 1214, in aformat_messages
    message = await message_template.aformat_messages(**kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\message.py", line 55, in aformat_messages
    return self.format_messages(**kwargs)
  File "D:\Anaconda\lib\site-packages\langchain_core\prompts\chat.py", line 183, in format_messages
    raise ValueError(msg)  # noqa: TRY004
ValueError: variable context should be a list of base messages, got <b>ĵ1CIEAdmission_Acceptance_Form-2025-chenxufei-T014241110407.pdfݣ</b><br>2001 Longxiang Blvd., Longgang District, Shenzhen 2001 ţ            Tel 绰: (86) 755 8427 3900         E-mail : gs@cuhk.edu.cn  
THE GRADUATE SCHOOL     Ժ 
 
 1 / 2 
 
MSc Programme in Computer and Information Engineering 
ADMISSION ACCEPTANCE FORM 
 
Congratulations on your admission to our Programme! Please complete this form to notify us if you 
will ACCEPT or DECLINE the offer of admission. 
 
 
Name in Chinese 
 
Name in Pingyin 
 
Application No. 
 
Term to Start 
1st Term, 2025-2026 
 
Please check: 
A    Yes, I accept this offer of admission.   
1. I will pay, by                      (dd/mm/yyyy), the first installment of tuition fee RMB 110, 000 via 
internet according to Payment Instructions below. 
2. I have read the document Admission Notification carefully, and accept the policy in the 
Admission Notification.  
3. For TPG students: 
1I understand, and am fully aware that On-campus housing may be provided only if it 
becomes available. In case of shortage of dormitory rooms, I am responsible to arrange and 
pay for off-campus housing. 
2All full-time TPG students are not allowed to take up any full-time employment, paid or 
unpaid, except with prior permission of the Programme Director, School Graduate Panel and 
School Dean, and with the final approval of Dean of the Graduate School. 
A    No, I do not wish to accept the offer of admission; please cancel my admission. 
Applicant Signature:  _________________________       Date:  _________________________ 
 

Chen,Xufei
T014241110407
\u2714
16/02/2025
20251202001 Longxiang Blvd., Longgang District, Shenzhen 2001 ţ              Tel 绰: (86) 755 8427 3626        E-mail : registry@cuhk.edu.cn  
THE GRADUATE SCHOOL     Ժ 
 
 2 / 2 
 
Please scan the completed form and send to: sse-tpg@cuhk.edu.cn 
Payment Instructions: 
\uf09e 
Login the Visitor Account on the official payment system: The Chinese University of Hong Kong, 
Shenzhen Payment System (https://pay.cuhk.edu.cn:3306/index.html) with your Personal ID 
Card NO.. 
\uf09e 
Select Tuition Fee in the payment category to pay tuition fee. 
\uf09e 
Accepted payment methods include: Alipay, WeChat Pay, and Citic e-bank.  
Please note:  
The Tuition Fee should be paid off in full amount with one transfer. If your bank account has a 
transfer limit, you can transfer enough amount into Alipay Balance, then use the balance to pay. of type <class 'str'>
-chat_qa_chain_self_answer
2025-05-18 12:04:24,272 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 272, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 298, in chat_answer
    raise ValueError("ϴļ")
ValueError: ϴļ
-chat_qa_chain_self_answer
2025-05-18 12:12:10,217 - root - ERROR - chat_process
Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 101, in map_httpcore_exceptions
    yield
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 394, in handle_async_request
    resp = await self._pool.handle_async_request(req)
  File "D:\Anaconda\lib\site-packages\httpcore\_async\connection_pool.py", line 256, in handle_async_request
    raise exc from None
  File "D:\Anaconda\lib\site-packages\httpcore\_async\connection_pool.py", line 236, in handle_async_request
    response = await connection.handle_async_request(
  File "D:\Anaconda\lib\site-packages\httpcore\_async\connection.py", line 101, in handle_async_request
    raise exc
  File "D:\Anaconda\lib\site-packages\httpcore\_async\connection.py", line 78, in handle_async_request
    stream = await self._connect(request)
  File "D:\Anaconda\lib\site-packages\httpcore\_async\connection.py", line 124, in _connect
    stream = await self._network_backend.connect_tcp(**kwargs)
  File "D:\Anaconda\lib\site-packages\httpcore\_backends\auto.py", line 31, in connect_tcp
    return await self._backend.connect_tcp(
  File "D:\Anaconda\lib\site-packages\httpcore\_backends\anyio.py", line 122, in connect_tcp
    stream._raw_socket.setsockopt(*option)  # type: ignore[attr-defined] # pragma: no cover
  File "D:\Anaconda\lib\contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\Anaconda\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: All connection attempts failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1500, in _request
    response = await self._client.send(
  File "D:\Anaconda\lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
  File "D:\Anaconda\lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
  File "D:\Anaconda\lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
  File "D:\Anaconda\lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 394, in handle_async_request
    resp = await self._pool.handle_async_request(req)
  File "D:\Anaconda\lib\contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\Anaconda\lib\site-packages\httpx\_transports\default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: All connection attempts failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 272, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 311, in chat_answer
    async for info, answer in self.astream(question, chat_chain, chain_config, document_variable_names, **kwargs):
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 411, in astream
    async for chunk in chat_chain.astream(chain_config):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 900, in _astream
    response = await self.async_client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 1927, in create
    return await self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1524, in _request
    return await self._retry_request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1594, in _retry_request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1524, in _request
    return await self._retry_request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1594, in _retry_request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1534, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
-chat_qa_chain_self_answer
2025-05-20 12:46:14,424 - root - ERROR - chat_process
Traceback (most recent call last):
  File "B:\notebook\Rag_Assistant\project\Agent\agent.py", line 63, in chat_qa_chain_self_answer
    async for info,answer in output_stream:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 272, in answer
    async for info, answer in chat_answer:
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 311, in chat_answer
    async for info, answer in self.astream(question, chat_chain, chain_config, document_variable_names, **kwargs):
  File "B:\notebook\Rag_Assistant\project\qa_chain\Chat_QA_chain_self.py", line 411, in astream
    async for chunk in chat_chain.astream(chain_config):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3451, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 647, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\passthrough.py", line 636, in _atransform
    yield await first_map_chunk_task
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3949, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3935, in _atransform
    chunk = AddableDict({step_name: task.result()})
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3918, in get_next_chunk
    return await py_anext(generator)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3433, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2312, in _atransform_stream_with_config
    chunk = await coro_with_context(py_anext(iterator), context)
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 3400, in _atransform
    async for output in final_pipeline:
  File "D:\Anaconda\lib\site-packages\langchain_core\output_parsers\transform.py", line 91, in atransform
    async for chunk in self._atransform_stream_with_config(
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 2272, in _atransform_stream_with_config
    final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 78, in anext_impl
    return await __anext__(iterator)
  File "D:\Anaconda\lib\site-packages\langchain_core\utils\aiter.py", line 133, in tee_peer
    item = await iterator.__anext__()
  File "D:\Anaconda\lib\site-packages\langchain_core\runnables\base.py", line 1479, in atransform
    async for output in self.astream(final, config, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_core\language_models\chat_models.py", line 580, in astream
    async for chunk in self._astream(
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 2081, in _astream
    async for chunk in super()._astream(*args, **kwargs):
  File "D:\Anaconda\lib\site-packages\langchain_openai\chat_models\base.py", line 900, in _astream
    response = await self.async_client.create(**payload)
  File "D:\Anaconda\lib\site-packages\openai\resources\chat\completions\completions.py", line 1927, in create
    return await self._post(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
  File "D:\Anaconda\lib\site-packages\openai\_base_client.py", line 1562, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - [{'error': {'code': 400, 'message': 'User location is not supported for the API use.', 'status': 'FAILED_PRECONDITION'}}]
-chat_qa_chain_self_answer
