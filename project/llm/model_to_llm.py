import sys

# from trl.commands.scripts.ppo import model_name
from utils.checkPort import getValidPort

from globals import normalize_path
sys.path.append("")
from openai import OpenAIError
#å¯¼å…¥chatopenaiï¼ŒErnieBotChat,
from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatTongyi,ChatLlamaCpp
from langchain_ollama import ChatOllama
from globals import OLLAMA_PORT_MAP
import socket
import time

from llm.call_llm import parse_llm_api_key
from llm.call_llm import parse_llm_api_base
from langchain_deepseek import ChatDeepSeek
from globals import LLM_MODEL_MAXTOKENS_DICT,SPARK_MODEL_DICT
from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import os
import re
import subprocess
from typing import Tuple, Dict
from modelscope import snapshot_download
import torch
import gradio as gr
from model.model_list import  get_model_list
import multiprocessing
from globals import models_cache
from transformers import BitsAndBytesConfig


def download_huggingface_model(model: str,model_dir) -> (bool,str):
    """
    ä¸‹è½½ HuggingFace æ¨¡å‹

    Args:
        model_name: è¦ä¸‹è½½çš„æ¨¡å‹åç§°
        model_dir: æœ¬åœ°ä¿å­˜çš„æ¨¡å‹ç›®å½•
    Returns:
        bool: ä¸‹è½½æ˜¯å¦æˆåŠŸ
    """
    try:
        # å°è¯•ä¸‹è½½æ¨¡å‹
        snapshot_download(model,cache_dir=model_dir)
        return True,f"æ¨¡å‹{model}ä¸‹è½½æˆåŠŸï¼Œä¿å­˜åœ¨{model_dir}"
    except Exception as e:
        return False,f"æ¨¡å‹{model}ä¸‹è½½å¤±è´¥: {str(e)}"


def download_ollama_model(model_name: str,model_dir:str) -> (bool,str):
    """
    ä¸‹è½½ Ollama æ¨¡å‹

    Args:
        model_name: è¦ä¸‹è½½çš„æ¨¡å‹åç§°
        model_dir: æœ¬åœ°ä¿å­˜çš„æ¨¡å‹ç›®å½•
        
    Returns:
        bool: ä¸‹è½½æ˜¯å¦æˆåŠŸ
    """

    # def pull_and_alias_model(base_model: str, alias_name: str):
    #     # 1. æ‹‰å–åŸå§‹æ¨¡å‹

    try:
        env = os.environ.copy()

        if model_dir in OLLAMA_PORT_MAP:
            port= OLLAMA_PORT_MAP[model_dir]
            env["OLLAMA_MODELS"] = model_dir
            env["OLLAMA_HOST"] = f"http://127.0.0.1:{port}"
        else:
            port = getValidPort(port=11500, max_port=11600)
            # --- Step 3. å¯åŠ¨æ–°çš„ Ollama æœåŠ¡ ---
            print(f"ğŸš€ å¯åŠ¨æ–°çš„ Ollama æœåŠ¡ï¼ˆæ¨¡å‹ç›®å½•: {model_dir}, ç«¯å£: {port}ï¼‰")
            env["OLLAMA_MODELS"] = model_dir
            env["OLLAMA_HOST"] = f"127.0.0.1:{port}"
            OLLAMA_PORT_MAP[model_dir] = port
            subprocess.Popen(["ollama", "serve"], env=env,
                             stdout=subprocess.DEVNULL,  # å±è”½æ ‡å‡†è¾“å‡º
                             stderr=subprocess.DEVNULL  # å±è”½é”™è¯¯è¾“å‡º
                             )
            time.sleep(2)  # ç­‰å¾…æœåŠ¡å¯åŠ¨



        process = subprocess.Popen(
        ["ollama", "pull", model_name],
            env=env,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        text=True,
        encoding="utf-8",
        bufsize=1
        )
        for line in process.stdout:
            line = line.strip()
            # å¦‚æœæ˜¯è¿›åº¦ä¿¡æ¯å°±åˆ·æ–°å½“å‰è¡Œ
            #  if "%" in line or "Pulling" in line or "Downloading" in line:
            print(f"\r{line}", end="", flush=True)  # å•è¡Œåˆ·æ–°
        #   else:
        # print(f"\n{line}")  # éè¿›åº¦ä¿¡æ¯æ­£å¸¸æ‰“å°

        process.wait()
        print(f"\nâœ… {model_name} ä¸‹è½½å®Œæˆ")

        return True,f"æ¨¡å‹{model_name}ä¸‹è½½æˆåŠŸï¼Œä¿å­˜åœ¨{model_dir}"
    except Exception as e:
        return False,f"ä¸‹è½½ Ollama æ¨¡å‹å¤±è´¥: {str(e)}"
def download_llamacpp_model(model_name: str, model_dir: str) -> (bool,str):
    try:
        # å°è¯•ä¸‹è½½æ¨¡å‹
        snapshot_download(model_name,cache_dir=model_dir)
        print(f"\nâœ… {model_name} ä¸‹è½½å®Œæˆ")

        return True,f"æ¨¡å‹{model_name}ä¸‹è½½æˆåŠŸï¼Œä¿å­˜åœ¨{model_dir}"
    except Exception as e:
        return False,f"ä¸‹è½½ Ollama æ¨¡å‹å¤±è´¥: {str(e)}"

def download_llm(model_type: str, model,model_dir:str,
                 ) :
    """
    ä¸‹è½½æ¨¡å‹å¹¶æ›´æ–°æ¨¡å‹åˆ—è¡¨

    Args:
        model_type: æ¨¡å‹ç±»å‹
        model: å½“å‰é€‰æ‹©çš„æ¨¡å‹
        local_llm: æœ¬åœ°æ¨¡å‹åç§°æˆ–è·¯å¾„
        model_dir:æœ¬åœ°æ¨¡å‹ä¿å­˜çš„ç›®å½•
        download_model: æ˜¯å¦ä¸‹è½½æ¨¡å‹
        llm_model_maxtokens_dict:æ¨¡å‹æœ€å¤§token
    Returns:
        Tuple[Dict, Dict]: (æ›´æ–°åçš„æ¨¡å‹åˆ—è¡¨, é€‰ä¸­çš„æ¨¡å‹)
    """
    if model_type == "HuggingFace":
        return download_huggingface_model(model, model_dir=model_dir)
    elif model_type == "Ollama":
        return download_ollama_model(model, model_dir=model_dir)
    elif model_type == "llama_cpp":
        return download_llamacpp_model(model, model_dir=model_dir)


def get_ollama_llm(model: str,
                   model_dir: str,
                   temperature: float = 0.7,
                   top_k: int = 5,
                   top_p: float = 0.95,
                   max_tokens: int = 2048,
                   reasoning: bool = False,
                 ):
    """
    å¯åŠ¨æŒ‡å®šç›®å½•ä¸‹çš„ Ollama æœåŠ¡å¹¶è¿æ¥æ¨¡å‹
    """

    env = os.environ.copy()
    if model_dir in OLLAMA_PORT_MAP:
        port= OLLAMA_PORT_MAP[model_dir]
        env["OLLAMA_HOST"] = f"127.0.0.1:{port}"
    else: #ç›®å½•æ”¹å˜
        port=getValidPort(port=11500,max_port=11600)

        # è‹¥æœªå¯åŠ¨ Ollama æœåŠ¡ï¼Œåˆ™å¯åŠ¨ä¸€ä¸ªæ–°çš„
        OLLAMA_PORT_MAP[model_dir] = port
        env["OLLAMA_MODELS"] = model_dir
        env["OLLAMA_HOST"] = f"127.0.0.1:{port}"
        print(f"å¯åŠ¨æ–°çš„ Ollama æœåŠ¡ï¼ˆæ¨¡å‹ç›®å½•: {model_dir}ï¼‰...")
        subprocess.Popen(["ollama", "serve"], env=env,
                         stdout=subprocess.DEVNULL,  # å±è”½æ ‡å‡†è¾“å‡º
                         stderr=subprocess.DEVNULL  # å±è”½é”™è¯¯è¾“å‡º
                         )
        time.sleep(2)  # ç­‰å¾…æ¨¡å‹å¯åŠ¨
    model_dir=normalize_path(model_dir)
    if model not in models_cache["Ollama"].get(model_dir,{}):
        # å¯åŠ¨ä¸€ä¸ªåå°è¿›ç¨‹å¹¶ä¿æŒè¿è¡Œ
        cmd = ["ollama", "run", model]
        proc = subprocess.Popen(
            cmd,
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            env=env
        )
        base_url=f"127.0.0.1:{port}"      #
        os.environ["OLLAMA_HOST"] = base_url  # ä¼˜å…ˆçº§æ¯”è¾“å…¥å‚æ•°base_urlæ›´é«˜
        llm_model = ChatOllama(
            model=model,
            base_url=base_url,
            temperature=temperature,
            num_predict=max_tokens,
            top_k=top_k,
            top_p=top_p,
            reasoning=reasoning
        )
        models_cache["Ollama"][model_dir][model]=[llm_model]
    else:
        models_cache["Ollama"][model_dir][model][0].temperature=temperature
        models_cache["Ollama"][model_dir][model][0].num_predict=max_tokens
        models_cache["Ollama"][model_dir][model][0].top_k=top_k
        models_cache["Ollama"][model_dir][model][0].top_p=top_p
        models_cache["Ollama"][model_dir][model][0].reasoning=reasoning
        llm_model=models_cache["Ollama"][model_dir][model][0]

    return llm_model
def get_HuggingFace_llm(model_dir,model_name,model_path,quantization,
                        max_tokens,temperature,top_k,top_p):
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    model_dir=normalize_path(model_dir)
    if model_name not in models_cache["HuggingFace"].get(model_dir, {}):
        if quantization == "q4":
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16,
            )
            llm = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                quantization_config=quantization_config,
                trust_remote_code=True,
                device_map=device,
            )
        elif quantization == "q8":
            quantization_config = BitsAndBytesConfig(
                load_in_8bit=True,
                #  llm_int8_has_fp16_weight=True  #è¿™ä¸ªä¼šä½¿å¾—æ¨¡å‹åŠ è½½å¤±è´¥

            )
            llm = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                quantization_config=quantization_config,
                trust_remote_code=True,
                device_map=device,
            )
        elif quantization == "fp16":
            llm = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16,

                trust_remote_code=True,
                device_map=device,
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„é‡åŒ–é…ç½®: {quantization}")

        tokenizer = AutoTokenizer.from_pretrained(model_path, torch_dtype=torch.float16,
                                              device_map=device, trust_remote_code=True)
        models_cache["HuggingFace"][model_dir][model_name] = [tokenizer, llm, None, None]

    else:
        tokenizer, llm = models_cache["HuggingFace"][model_dir][model_name][:2]


    pipe = pipeline(
        "text-generation",
        model=llm,
        tokenizer=tokenizer,
        max_length=max_tokens if max_tokens else 2048,
        temperature=temperature,
        top_k=top_k if top_k else 5,
        top_p=top_p if top_p else 0.95,
        #  device="cuda:0" if torch.cuda.is_available() else "cpu",
    )
    # llm_model=HuggingFacePipeline(pipeline=pipe)
    llm_model = ChatHuggingFace(llm=HuggingFacePipeline(pipeline=pipe),
                                tokenizer=tokenizer)
    models_cache["HuggingFace"][model_dir][model_name][2] = pipe
    models_cache["HuggingFace"][model_dir][model_name][3] = llm_model

    return  tokenizer,llm,pipe, llm_model

def get_llamcpp_llm(model,model_dir,quantization,
                    temperature=0.0,
                    max_tokens=4096,
                    ):
    def find_gguf_file(model_path: str, quant: str) -> str:
        quant = quant.lower()

        def is_quant_match(f, quant):
            f = f.lower()
            if not f.endswith(".gguf"):
                return False
            parts = re.split(r"[-_.]", f)
            return any(p.startswith(quant) for p in parts)

        print(f"æ¨¡å‹è·¯å¾„ä¸º{model_path}")
        candidates = [f for f in os.listdir(model_path) if is_quant_match(f, quant)]
        if not candidates:
            raise FileNotFoundError(f"æ²¡æœ‰æ‰¾åˆ°åŒ…å«é‡åŒ–æ ‡å¿— '{quant}' çš„ggufæ–‡ä»¶")

        return os.path.join(model_path, candidates[0])

    model_name = model + "_" + quantization
    model_dir = normalize_path(model_dir)
    model_path = os.path.join(model_dir, model)  # guffæ–‡ä»¶å¤¹ï¼Œä¸æ˜¯æ¨¡å‹æ–‡ä»¶è·¯å¾„
    if model_name in models_cache["llama_cpp"].get(model_dir, {}):
        model_path = models_cache["llama_cpp"][model_dir][model_name][1]
        models_cache["llama_cpp"][model_dir][model_name][0].temperature=temperature
        models_cache["llama_cpp"][model_dir][model_name][0].max_tokens=max_tokens
        models_cache["llama_cpp"][model_dir][model_name][0].n_ctx=max_tokens*2

        llm_model=models_cache["llama_cpp"][model_dir][model_name][0]
        print(f"llm_model1:{sys.getrefcount(llm_model)}")
    else:
        model_path = find_gguf_file(model_path, quantization)  # ç”Ÿæˆå…·ä½“é‡åŒ–guffæ¨¡å‹æ–‡ä»¶è·¯å¾„
        llm_model = ChatLlamaCpp(
            temperature=temperature,
            model_path=model_path,
            n_ctx=max_tokens * 2,  # ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œè¿™é‡Œç®€å•è®¾ç½®
            n_gpu_layers=-1,
            n_batch=300,  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.
            max_tokens=max_tokens,
            n_threads=multiprocessing.cpu_count() - 1,

            #   verbose=True,
        )
        models_cache["llama_cpp"][model_dir][model_name] = [llm_model, model_path]
        print(f"llm_model2:{sys.getrefcount(llm_model)}")

    return llm_model

def model_to_llm(model_type:str=None,model:str=None,model_dir:str=None,quantization:str=None,
                 temperature:float=0.0,max_tokens:int =4096,top_k:int =None,top_p:int=None,api_key:str=None,api_base:str=None,
                 is_reasoning=False):
       # model_path = os.path.join(model_dir, model)
        if model_type=="HuggingFace":
            # ä½¿ç”¨HuggingFaceæœ¬åœ°æ¨¡å‹
           # download_llm(model_type,model,local_llm,model_dir,download_model,llm_models)
            model_path = os.path.join(model_dir, model)
            model_name = model+"_"+quantization  if quantization  else model

            try:
                    # å¦‚æœæŒ‡å®šäº†é‡åŒ–é…ç½®ï¼Œä½¿ç”¨BitsAndBytesConfigåŠ è½½æ¨¡å‹
                tokenizer,llm,pipe,llm_model=get_HuggingFace_llm(model_dir,model_name,model_path,quantization,
                                              max_tokens,temperature, top_k, top_p)

                return llm_model
            except Exception as e:
                del tokenizer,llm,pipe,llm_model
                torch.cuda.empty_cache()
                raise ValueError(f"åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}")

        elif model_type=="Ollama":
           # is_reasoning= None if is_reasoning else False #Ollama Noneæ‰§è¡Œé»˜è®¤æ¨ç†ï¼Œå¯¹äºQwen3æ¨¡å‹ï¼Œé»˜è®¤æ¨ç†æ˜¯åœ¨thinkæ ‡ç­¾ä¸­
            try:

                llm_model=get_ollama_llm(model=model,
                        model_dir=model_dir,
                        temperature=temperature,
                top_k=top_k if top_k else 5,
                top_p=top_p if top_p else 0.95,
                max_tokens=max_tokens if max_tokens else 2048,
                reasoning=is_reasoning) #æŒ‡å®šç›®å½•ä¸‹ï¼Œéœ€è¦é‡æ–°ç”Ÿæˆollamaå®ä¾‹

                return llm_model
            except Exception as e:
                del llm_model
                torch.cuda.empty_cache()
                raise ValueError(f"åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}")

        elif model_type=="llama_cpp":

            try:
                llm_model=get_llamcpp_llm(model,model_dir,quantization, temperature, max_tokens)
                print(f"llm_model3:{sys.getrefcount(llm_model)}")
                return llm_model
            except Exception as e:

                torch.cuda.empty_cache()
                raise ValueError(f"åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}")

        api_key = parse_llm_api_key(model_type, model) if not api_key else api_key

        api_base = parse_llm_api_base(model_type,model) if not api_base else api_base


        if model_type=="QWEN":
            llm = ChatDeepSeek(model=model, temperature=temperature, max_tokens=max_tokens,
                             top_p=top_p,api_key=api_key,api_base=api_base,streaming=True,
                               extra_body={"enable_thinking":is_reasoning}) #æ··åˆæ¨ç†åªé€‚ç”¨äºqwen3æ¨¡å‹(å•†ç”¨ä¸å¼€æº)
        elif model_type=="ZHIPUAI":
            is_reasoning="enabled"  if is_reasoning else "disabled"
            llm = ChatDeepSeek(model=model, temperature=temperature, max_tokens=max_tokens,api_key=api_key,
                             api_base=api_base,streaming=True,extra_body = {"thinking": {
                "type": is_reasoning
            }}
                               ) #æ··åˆæ¨ç†ä»…é€‚ç”¨äºglm4.5åŠä»¥ä¸Š

        elif model_type=="BAICHUAN":
            llm = ChatOpenAI(model=model, temperature=temperature, max_tokens=max_tokens,top_p=top_p,
                   api_key=api_key,base_url=api_base,streaming=True) #æ”¯æŒtop-p,k
        else:
            if model_type == "SPARK":
                model = SPARK_MODEL_DICT[model]
            llm = ChatDeepSeek(model=model, temperature=temperature, max_tokens=max_tokens, api_key=api_key,
                             api_base=api_base, streaming=True)
        # else:
        #     if model_type == "SPARK":
        #         model = SPARK_MODEL_DICT[model]
        #     llm = ChatOpenAI(model=model, temperature=temperature, max_tokens=max_tokens,api_key=api_key,
        #                      base_url=api_base,streaming=True)

        return llm

        #ChatDeepSeekåœ¨ChatOpenAIçš„åŸºç¡€ä¸Šè¿”å›äº†æ¨ç†å†…å®¹ï¼Œæ‰€ä»¥è¿™é‡ŒåŒä¸€ä½¿ç”¨ChatDeepSeek



