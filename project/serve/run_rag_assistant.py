
import os

from langchain_core.tools import BaseTool, BaseToolkit
from llm.model_to_llm import model_to_llm
from embedding.call_embedding import get_embedding,download_embedding
from globals import normalize_path

#sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from user.log_in import login, logout, guest_login, loginToRegister
from user.sign_up import register
from user.MyBlocks import MyBlock
from database.create_db import KnowledgeDB
from globals import EMBEDDING_MODEL_DICT, SPARK_MODEL_DICT, DEFAULT_PERSIST_PATH, DEFAULT_DB_PATH, \
    LLM_MODEL_MAXTOKENS_DICT, ChatAgents
from prompt.prompt import default_template
from Agent.tools import  tools,register_tool
from globals import update_time, chat_config,default_model_dir
import gradio as gr
import traceback
import logging

from llm.model_to_llm import download_llm
from model.model_list import get_model_list
import copy
import subprocess,socket,time
from langchain_core.tools import BaseTool,Tool,StructuredTool
# ä½¿ç”¨execjs ç»å¸¸ä¼šé‡åˆ°ç¼–ç é”™è¯¯
# è§£å†³æ–¹æ³•
from user.MyBlocks import theme_block
import subprocess
from functools import partial
from globals import mixed_reason_models,models_cache
from globals import empty_gpu_memory,llama_processes
import types
# subprocess.Popen = partial(subprocess.Popen, encoding="utf-8")


# éœ€è¦å†™åœ¨ import execjs ä¹‹å‰
INIT_LLM = "gpt-3.5-turbo"

INIT_EMBEDDING_MODEL = "openai"

USER_PATH = "../figures/user.png"
AI_PATH = "../figures/AI.png"
LOGO_PATH = "../figures/logo.png"


# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºæ›´æ–° LLM ç±»å‹ä¸‹æ‹‰æ¡†çš„å†…å®¹ã€‚

button_size = {"small": 10, "medium": 50, "large": 100}
local_llm_type=["HuggingFace","Ollama","llama_cpp"]
quantization_choices={"HuggingFace":["q4","q8","fp16"],"llama_cpp":[]}
#ollamaé»˜è®¤æ‹‰å–é‡åŒ–ç‰ˆæœ¬ï¼Œä¸æä¾›é‡åŒ–
def is_quant_match(f, quant):
    import re
    f = f.lower()
    if not f.endswith(".gguf"):
        return False
    parts = re.split(r"[-_.]", f)
    return any(p.startswith(quant) for p in parts)



def update_llm_dropdown(selected_type,model_dir,llm_model_maxtokens_dict):
    #  print(f"the selected type is {selected_type}")
    model_dir= default_model_dir.get(selected_type,None)

    get_model_list(model_dir,selected_type,llm_model_maxtokens_dict)
    models = list(llm_model_maxtokens_dict.get(selected_type, {}).keys())
    print(f"åˆ—è¡¨{models}")
    if selected_type == "QWEN":
        return gr.update(choices=models, value=models[0] if models else None), gr.update(visible=False), gr.update(
            visible=True) ,gr.update(),gr.update(visible=False),gr.update(visible=False),gr.update(visible=False) # å°†LLMä¸‹æ‹‰é€‰é¡¹æ›´æ–°ä¸ºï¼š
    elif selected_type == "BAICHUAN":
        return gr.update(choices=models, value=models[0] if models else None), gr.update(visible=True), gr.update(
            visible=True) ,gr.update(),gr.update(visible=False),gr.update(visible=False),gr.update(visible=False) # å°†LLMä¸‹æ‹‰é€‰é¡¹æ›´æ–°ä¸ºï¼š
    elif selected_type in local_llm_type:


        return (gr.update(choices=models, value=models[0] if models else None), gr.update(visible=False), gr.update(
            visible=False), gr.update(value=model_dir),
                gr.update(visible=True),gr.update(visible=True),gr.update(visible=True))   #å°†LLMä¸‹æ‹‰é€‰é¡¹æ›´æ–°ä¸ºï¼š
    else:
        return gr.update(choices=models, value=models[0] if models else None), gr.update(visible=False), gr.update(
            visible=False),gr.update(),gr.update(visible=False),gr.update(visible=False),gr.update(visible=False)      # å°†LLMä¸‹æ‹‰é€‰é¡¹æ›´æ–°ä¸ºï¼š

def update_embedding_dropdown(selected_type, state,embedding_dir):


    if selected_type == "SPARK" and state["username"] != "guest":
        embeddings = EMBEDDING_MODEL_DICT.get(selected_type, [])
        return (gr.update(choices=embeddings, value=embeddings[0] if embeddings else None), gr.update(
            visible=True), gr.update(visible=True), gr.update(visible=False),
                gr.update(visible=False), gr.update(visible=False),gr.update(visible=False)),gr.update(visble=False)
    elif selected_type == "WENXIN" and state["username"] != "guest":
        embeddings = EMBEDDING_MODEL_DICT.get(selected_type, [])
        return (gr.update(choices=embeddings, value=embeddings[0] if embeddings else None), gr.update(
            visible=False), gr.update(visible=False), gr.update(visible=True), gr.update(visible=False),
                gr.update(visible=False),gr.update(visible=False)),gr.update(visible=False)
    elif selected_type in ["HuggingFaceEmbedding", "OllamaEmbedding"]:
        embedding_dir =  default_model_dir.get(selected_type,None)
        embeddings= get_model_list(embedding_dir, selected_type)
        # æœ¬åœ°embeddingæ¨¡å‹ï¼Œæ˜¾ç¤ºæœ¬åœ°æ¨¡å‹é…ç½®
        return gr.update(choices=embeddings, value=embeddings[0] if embeddings else None), gr.update(
            visible=False), gr.update(visible=False), gr.update(visible=False), gr.update(visible=True), \
                gr.update(visible=True,value=False),gr.update(visible=True),gr.update(visible=False,value=embedding_dir)
    else:
        embeddings = EMBEDDING_MODEL_DICT.get(selected_type, [])
        return gr.update(choices=embeddings, value=embeddings[0] if embeddings else None), gr.update(
            visible=False), gr.update(visible=False), gr.update(visible=False), gr.update(visible=False), gr.update(visible=False),gr.update(visible=False),\
            gr.update(visible=False)


def update_file(embedding_type,embedding,embedding_dir, knowledgeDB: KnowledgeDB):
    if embedding in knowledgeDB.files:
        print(knowledgeDB.files[embedding])
        knowledgeDB.init_doc_lists(embedding_type,embedding,embedding_dir)
        return gr.update(choices=knowledgeDB.files[embedding]), gr.update(),gr.update(value=knowledgeDB)
    else:
        return gr.update(choices=[]), gr.update(),gr.update()
    # è¿”å›ä¸€ä¸ªæ›´æ–°åçš„ä¸‹æ‹‰æ¡†å¯¹è±¡ï¼ŒåŒ…å«æ‰€é€‰ç±»å‹çš„æ¨¡å‹åˆ—è¡¨å’Œé»˜è®¤å€¼ï¼ˆç¬¬ä¸€ä¸ªæ¨¡å‹ï¼‰ã€‚



def update_llm_config(llm_type,llm,llm_maxtokens,mode,chat_mode,model_dir):
    """
    è¯¥å‡½æ•°ç”¨äºæ ¹æ® LLM ç±»å‹å’Œæ¨¡å‹åç§°æ›´æ–°æœ€å¤§ token æ•°ã€‚

    å‚æ•°:
    llm_type: LLM ç±»å‹ã€‚
    llm_model: LLM æ¨¡å‹åç§°ã€‚

    è¿”å›:
    max_tokens: æ›´æ–°åçš„æœ€å¤§ token æ•°ã€‚
    """
    # æ ¹æ® LLM ç±»å‹å’Œæ¨¡å‹åç§°ï¼Œè·å–å¯¹åº”çš„æœ€å¤§ token æ•°ã€‚
    print(f"æ¨¡å‹ï¼š{llm}")
    if not llm:
        return gr.update(),gr.update(),gr.update()
    max_tokens = llm_maxtokens[llm_type][llm][0]  #å¦‚æœæ¨¡å‹åˆ—è¡¨é‡Œæ²¡æœ‰æ¨¡å‹ï¼Œè¾“å…¥ä¸ºNoneï¼Œè¿™é‡ŒæŒ‡å®šä¸€ä¸ªå€¼ï¼Œé˜²æ­¢æŠ¥é”™

    if llm_type=="llama_cpp" :
        model_path = os.path.join(model_dir, llm) if llm else model_dir
        quantization_choices["llama_cpp"]=[] #æ¯æ¬¡æ›´æ–°å‰æ¸…ç©º
        for quant in ["q4","q6","q8"]:
            candidates = [f for f in os.listdir(model_path) if is_quant_match(f, quant)]
            if candidates:
                quantization_choices["llama_cpp"].append(quant)
    update=gr.update(choices=quantization_choices[llm_type],value=quantization_choices[llm_type][0] if quantization_choices[llm_type] else None,visible=True )\
        if llm_type in ["HuggingFace","llama_cpp"] else gr.update(visible=False)
    if mode=="æ™ºèƒ½ä½“æ¨¡å¼":
        if llm in mixed_reason_models or "qwen3".lower() in llm.lower() :
            return gr.update(maximum=max_tokens, value=max_tokens / 2),gr.update(choices=[chat_config[-1]],visible=True),update
        else:
            if "æ·±åº¦æ€è€ƒ" in chat_mode:
                chat_mode.remove("æ·±åº¦æ€è€ƒ")
            return gr.update(maximum=max_tokens, value=max_tokens/2),gr.update(visible=False,value=chat_mode),update #ä¸æ”¯æŒæ·±åº¦æ€è€ƒ
    else:
        if llm in mixed_reason_models or "qwen3".lower() in llm.lower()  :
            return gr.update(maximum=max_tokens, value=max_tokens / 2),gr.update(choices=chat_config),update
        else:
            if "æ·±åº¦æ€è€ƒ" in chat_mode:
                chat_mode.remove("æ·±åº¦æ€è€ƒ")
            return gr.update(maximum=max_tokens, value=max_tokens/2),gr.update(choices=chat_config[:-1],value=chat_mode),update
            #ä¸æ”¯æŒæ·±åº¦æ€è€ƒ


def create_db_info(files=DEFAULT_DB_PATH, embedding_type="OPENAI", embeddings="text-embedding-ada-002",
                   DB: KnowledgeDB = None, embedding_key: str = None, embedding_base: str = None,
                   spark_app_id: str = None, spark_api_secret: str = None, wenxin_secret: str = None,
                   local_embedding_model: str = None, embedding_dir: str = None,
                   ):
    try:
        embeddings=local_embedding_model if local_embedding_model  else embeddings
        embedding_dir=normalize_path(embedding_dir)
        if embedding_type in ["OllamaEmbedding","HuggingFaceEmbedding"] and embeddings not in models_cache[embedding_type].get(embedding_dir,{}):
            return f"æ¨¡å‹{embeddings}æœªåŠ è½½",gr.update(),gr.update()

        DB.reset(embedding_key=embedding_key, embedding_base=embedding_base,
                 spark_app_id=spark_app_id, spark_api_secret=spark_api_secret, wenxin_secret=wenxin_secret)
        # ä¼ é€’æœ¬åœ°embeddingç›¸å…³å‚æ•°
        return DB.create_db_info(files, embedding_type, embeddings, embedding_dir
                               ), \
               gr.update(choices=DB.files.get(embeddings, []), value=None), DB


    except Exception as e:
        error_info = traceback.format_exc()  # å°†å®Œæ•´çš„é”™è¯¯è¾“å‡ºä¸ºå­—ç¬¦ä¸²
        logging.basicConfig(filename="log.txt",
                            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s-%(funcName)s',
                            level=logging.ERROR)  # è®¾ç½®æ—¥å¿—çº§åˆ«ï¼Œåªæœ‰è°ƒç”¨è¯¥å‡½æ•°æ‰ä¼šå°†å¤§äºç­‰äºlevelçš„æ—¥å¿—ä¿¡æ¯ä¿å­˜åˆ°é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„logæ–‡ä»¶ä¸­;ä¸è°ƒç”¨ï¼Œåˆ™è¾“å‡ºåˆ°æ§åˆ¶å°
        logging.error(f"chat_processå‘ç”Ÿé”™è¯¯ï¼š\n{error_info}")  # é”™è¯¯è¾“å‡ºåˆ°æ§åˆ¶å°ï¼Œå­˜åœ¨äºé¡¹ç›®æ ¹ç›®å½•
        print(error_info)
        return f"æ–‡ä»¶ä¸Šä¼ çŸ¥è¯†åº“å¤±è´¥ï¼š{str(e)}", gr.update(choices=DB.files.get(embeddings, [])), DB


def delete_db(db_file, embedding_type, embedding, DB,local_embedding,embedding_dir):
    embedding = local_embedding if local_embedding else embedding
    embedding_dir=normalize_path(embedding_dir)
    if embedding_type in ["OllamaEmbedding", "HuggingFaceEmbedding"] and embedding not in models_cache[embedding_type].get(embedding_dir,{}):
        return f"æ¨¡å‹{embedding}æœªåŠ è½½", gr.update(), gr.update()
    return DB.del_file(db_file, embedding_type, embedding,embedding_dir), gr.update(choices=DB.files.get(embedding, []),
                                                                      value=None), DB


def update_db(files, new_files, embedding_type, embedding, DB,local_embedding,embedding_dir):
    embedding = local_embedding if local_embedding else embedding
    embedding_dir = normalize_path(embedding_dir)
    if embedding_type in ["OllamaEmbedding", "HuggingFaceEmbedding"] and embedding not in models_cache[embedding_type].get(embedding_dir,{}):
        return f"æ¨¡å‹{embedding}æœªåŠ è½½", gr.update(), gr.update()
    return DB.update_file(files, new_files, embedding_type, embedding,embedding_dir), gr.update(choices=DB.files.get(embedding, []),
                                                                                  value=None), DB


def search_db(file, embedding,local_embedding,DB):
    embedding=local_embedding if local_embedding else embedding
    search_file = [aim_file for aim_file in DB.files.get(embedding,[]) if aim_file.startswith(file)]  # å‰ç¼€åŒ¹é…æœç´¢
    if not search_file:
        return "æ£€ç´¢å¤±è´¥ï¼", gr.update(value=None)
    else:
        return "æ£€ç´¢æˆåŠŸï¼", gr.update(value=search_file)


def update_embedding_key(embedding_key, DB):
    DB.embedding_key = embedding_key
    return DB


def update_spark_app_id(spark_app_id, DB):
    DB.spark_app_id = spark_app_id
    return DB


def update_spark_api_secret(spark_api_secret, DB):
    DB.spark_api_secret = spark_api_secret
    return DB


def update_wenxin_secret(wenxin_secret, DB):
    DB.wenxin_secret = wenxin_secret
    return DB


def choose_all(DB, embedding):
    return gr.update(value=DB.files[embedding])


def cancel_all():
    return gr.update(value=[])


# def update_template(template_type, template, state):
#     ChatAgents[state["session_id"]].prompts[template_type] = template

def update_template(template, state):
    ChatAgents[state["session_id"]].prompts = template


# def show_template(template_type, state):
#     return gr.update(value=ChatAgents[state["session_id"]].prompts[template_type])

def update_mode(request:gr.Request,mode,llm,tool_choose):
    tool_list=ChatAgents[request.session_hash].tools
    tool_choose=[] if tool_choose is None else tool_choose
    if mode=="èŠå¤©æ¨¡å¼":
        tool_list.pop("RagTool",None) #èŠå¤©æ¨¡å¼æ˜¯å¦RAGç”¨æˆ·æŒ‡å®š ï¼Œä¸ä½œä¸ºå·¥å…·
        if tool_choose and  "RagTool" in tool_choose:
            tool_choose.remove("RagTool")
        if llm in mixed_reason_models or llm.startswith("qwen3"):
            return (gr.update(visible=True,choices=chat_config),gr.update(visible=False,value=None),gr.update(visible=True),
                    gr.update(choices=tool_list,visible=True,value=tool_choose),gr.update(visible=False))
        else:
            return gr.update(visible=True, choices=chat_config[:-1]), gr.update(visible=False, value=None), gr.update(
                visible=True), gr.update(choices=tool_list, visible=True,value=tool_choose), gr.update(visible=False)
    elif mode=="æ™ºèƒ½ä½“æ¨¡å¼":
        tool_list["RagTool"]=tool_list.get("RagTool",None)

        if llm in mixed_reason_models or llm.startswith("qwen3"):
            return (gr.update(visible=True,choices=[chat_config[-1]],value=None),gr.update(visible=False,value=None),
                    gr.update(visible=True),gr.update(choices=tool_list,visible=True),gr.update(visible=True))
        else:
            return gr.update(visible=False,value=None), gr.update(visible=False,
                                                            value=None), gr.update(
                visible=True), gr.update(choices=tool_list, visible=True), gr.update(visible=True)
    elif mode=="æŒ‡å®šæ¨¡å¼":
        return gr.update(visible=False,value=None),gr.update(visible=True,value="æ–‡æœ¬æ‘˜è¦"),gr.update(visible=False),gr.update(visible=False),gr.update(visible=False)
    return None



def update_chat_mode(chat_mode: list[str],mode:str):

    if "ä½¿ç”¨çŸ¥è¯†åº“" in chat_mode and mode=="èŠå¤©æ¨¡å¼" :
        return gr.update(visible=True)
    else:
        return  gr.update(
                visible=False)


def update_tool_list(code,choosed_tools,state):
    if not code:
        return gr.update()
    local_vars = {}
    session_id=state["session_id"]
    user_tools=ChatAgents[session_id].tools
    try:
        # å®‰å…¨æ‰§è¡Œç”¨æˆ·è¾“å…¥çš„å‡½æ•°ä»£ç 
       # exec(textwrap.dedent(code) , {}, local_vars)
        exec(code, {"BaseTool":BaseTool,"BaseToolkit":BaseToolkit,"os":os,
                    "Tool":Tool,"StructuredTool":StructuredTool}, local_vars)
        #å¦‚æœæ˜¯è‡ªå®šä¹‰å‡½æ•°ä½œä¸ºå·¥å…·,å¯¼å…¥åŒ…éœ€è¦åœ¨å‡½æ•°å†…éƒ¨
        tool_names=[] #è¯»å–çš„ç”¨æˆ·è‡ªå®šä¹‰å·¥å…·

        for func_tool in local_vars.values():
            #å®šä¹‰å·¥å…·ç±»å‡ºé”™,å¯èƒ½æ˜¯ç¼ºå¤±APIå‚æ•°,æ­¤æ—¶æ”¹ä¸ºä½¿ç”¨å®ä¾‹å®šä¹‰
            if isinstance(func_tool, type):
                # æ”¯æŒè‡ªå®šä¹‰ BaseTool æˆ– BaseToolkit ç±»ï¼Œæˆ–å¯¼å…¥ LangChain è‡ªå¸¦å·¥å…·
                flag = issubclass(func_tool, BaseTool) or issubclass(func_tool, BaseToolkit)
                if flag:
                    try:
                        tool = func_tool()  # å°è¯•å®ä¾‹åŒ–
                    except Exception as e:
                        # âŒ å¦‚æœå®ä¾‹åŒ–å¤±è´¥ï¼ˆå¦‚ç¼º api_keyï¼‰ï¼Œä»…è·³è¿‡ï¼Œä¸æ³¨å†Œã€ä¸æŠ¥é”™
                        print(f"[è·³è¿‡æœªåˆå§‹åŒ–çš„å·¥å…·ç±»] {func_tool.__name__}: {e}")
                        continue

                    # âœ… æˆåŠŸå®ä¾‹åŒ–æ‰æ³¨å†Œ
                    name = getattr(tool, "name", tool.__class__.__name__)
                    if name not in user_tools:
                        register_tool(tool, user_tools=user_tools)
                        tool_names.append(name)
                    else:
                        del tool
                        gr.Warning(f"æ³¨å†Œå¤±è´¥ï¼Œå­˜åœ¨åŒåå·¥å…·ï¼š{name}ï¼")

            # âœ… å¦‚æœ func_tool æœ¬èº«å°±æ˜¯å®ä¾‹ï¼ˆå¸¦å¥½ api_keyï¼‰
            elif isinstance(func_tool, BaseTool) or isinstance(func_tool, BaseToolkit):
                name = getattr(func_tool, "name", func_tool.__class__.__name__)
                if name not in user_tools:
                    register_tool(func_tool, user_tools=user_tools)
                    tool_names.append(name)
                else:
                    gr.Warning(f"æ³¨å†Œå¤±è´¥ï¼Œå­˜åœ¨åŒåå·¥å…·ï¼š{name}ï¼")

            elif isinstance(func_tool,types.FunctionType) or isinstance(func_tool,types.MethodType):
                ##æ”¯æŒpythonå‡½æ•°æ„é€ å·¥å…·ï¼Œä½†æ˜¯éœ€è¦æœ‰docå¯¹å‡½æ•°ä½œç”¨ä»‹ç»
                    if not func_tool.__name__ in user_tools:
                        if not func_tool.__doc__:
                            gr.Warning("æ³¨å†Œå¤±è´¥ï¼Œæ²¡æœ‰ä¸ºå·¥å…·æ·»åŠ æè¿°ï¼›è¯·åœ¨å‡½æ•°ä¸­ï¼Œç”¨æ–‡æœ¬å­—ç¬¦ä¸²æè¿°å·¥å…·ä½œç”¨")
                        else:
                            register_tool(func_tool,name=func_tool.__name__,description=func_tool.__doc__,
                              user_tools=user_tools)
                            tool_names.append(func_tool.__name__)
                    else:
                        gr.Warning(f"æ³¨å†Œå¤±è´¥ï¼šå­˜åœ¨åŒåå·¥å…·ï¼")


        if not tool_names:
            gr.Error("âŒ æ²¡æœ‰æ£€æµ‹åˆ°Toolå®šä¹‰ï¼Œè¯·æ£€æŸ¥æ ¼å¼ã€‚")
            return gr.update()
        else:
            gr.Info(f"æˆåŠŸå®šä¹‰{len(tool_names)}ä¸ªå·¥å…·æˆ–å·¥å…·åŒ…,åˆ†åˆ«ä¸ºï¼š{', '.join(tool_names)}")
            if choosed_tools is None:
               choosed_tools=[]
            merged = list(dict.fromkeys( tool_names+choosed_tools)) #å°†é€‰ä¸­çš„å·¥å…·å’Œåˆšå®šä¹‰çš„å·¥å…·åˆå¹¶ï¼Œä½œä¸ºé€‰ä¸­çš„å·¥å…·å€¼ï¼›
            #user_toolsæ˜¯å·¥å…·åˆ—è¡¨

            return gr.update(choices=user_tools.keys(),value=merged)


    except Exception as e:
        gr.Warning(f"âŒ æ³¨å†Œå¤±è´¥ï¼š{e}")


# def update_rag_config(chat_mode):
#     if "ä½¿ç”¨çŸ¥è¯†åº“" in chat_mode:
#         return gr.update(visible=True)
#     else:
#         return gr.update(visible=False)

def check_legal_quantized(model_type,quantization_config):
    if model_type=="llama_cpp" and not quantization_config:#å¿…é¡»é‡åŒ–
        return False
    else:
        return True

def update_model_list(model_dir,model_type,llm_model_maxtokens_dict):
    if not model_dir:
        return gr.update()
    try:
        if  os.path.exists(model_dir):
            default_model_dir[model_type]=model_dir
            llm_model_maxtokens_dict=get_model_list(model_dir,model_type,llm_model_maxtokens_dict)
            model_list=list(llm_model_maxtokens_dict.get(model_type,[]).keys())
            model_dir=normalize_path(model_dir)
            if model_dir not in models_cache[model_type]:
                models_cache[model_type][model_dir]={}
            return gr.update(choices=model_list,value=model_list[0] if model_list else None)
        else:
            gr.Warning(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥ï¼")
            return gr.update()
    except Exception as e:
        gr.Error(f"âŒ å½“å‰è·¯å¾„ä¸‹è¯»å–æ¨¡å‹åˆ—è¡¨å¤±è´¥ï¼Œé”™è¯¯æ˜¯ï¼š{e}")

def update_embedding_list(embedding_dir,embedding_type):
    if not embedding_dir:
        return gr.update()
    try:
        if os.path.exists(embedding_dir):
            default_model_dir[embedding_type] = embedding_dir
            model_list=get_model_list(embedding_dir,embedding_type)
            embedding_dir = normalize_path(embedding_dir)
            if embedding_dir not in models_cache[embedding_type]:
                models_cache[embedding_type][embedding_dir]={}
            return gr.update(choices=model_list,value=model_list[0] if model_list else None)
        else:
            gr.Warning(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥ï¼")
            return gr.update()
    except Exception as e:
        gr.Error(f"âŒ å½“å‰è·¯å¾„ä¸‹è¯»å–æ¨¡å‹åˆ—è¡¨å¤±è´¥ï¼Œé”™è¯¯æ˜¯ï¼š{e}")
        return gr.update()
def chat_qa_chain_answer(question_input, mode,chatbot, model_type,model, model_dir,llm_model_maxtokens_dict,quantization_config,
                               embedding_type, embedding,embedding_dir,
                               temperature, top_k_llm, history_len, max_tokens, top_p,
                               top_k_query, score_threshold, fecth_k, lambda_mult, search_type, chat_mode, special_mode,
                               is_with_history,
                               llm_key, llm_base, DB, upload_files, rag_files, rag_config,web_config,  websearch_max_results, tools,state,agent_output_mode,
                               reranker_config):
    context_window=llm_model_maxtokens_dict[model_type][model][1]
    agent = ChatAgents[state["session_id"]]
    if not check_legal_quantized(model_type,quantization_config):
        yield "guffæ ¼å¼å¿…é¡»æŒ‡å®šé‡åŒ–æ¨¡å‹",agent.chat_history
        return
    model_name = model + "_" + quantization_config if model_type in ["HuggingFace", "llama_cpp"] \
        else model
    model_dir=normalize_path(model_dir)
    if model_type in ["HuggingFace","llama_cpp","Ollama"]  \
            and model_name not in  models_cache[model_type].get(model_dir,{}):

        yield f"æœ¬åœ°æ¨¡å‹{model}æœªåŠ è½½!",agent.chat_history
        return



    chat_mode = {"is_rag": "ä½¿ç”¨çŸ¥è¯†åº“" in chat_mode,
                 "is_web_search": "è”ç½‘æœç´¢" in chat_mode,
                 "is_reasoning":"æ·±åº¦æ€è€ƒ" in chat_mode
                 }
    embedding_dir = normalize_path(embedding_dir)
    if chat_mode["is_rag"] and embedding_type in ["HuggingFaceEmbedding","OllamaEmbedding"] and embedding not in  models_cache[embedding_type].get(embedding_dir,{}):
        yield f"æœ¬åœ°åµŒå…¥æ¨¡å‹{embedding}æœªåŠ è½½",agent.chat_history
        return

    search_type: str = "similarity_score_threshold" if search_type == "ä½™å¼¦ç›¸ä¼¼åº¦" else "mmr"
    is_abstract=  (special_mode=="æ–‡æœ¬æ‘˜è¦" and mode=="æŒ‡å®šæ¨¡å¼")
    if is_abstract:
        if not upload_files and not rag_files:
            yield "é”™è¯¯ï¼ç”Ÿæˆæ–‡æœ¬æ‘˜è¦æ¨¡å¼ä¸‹æœªæŒ‡å®šæ–‡ä»¶ï¼", agent.chat_history
            return
        elif question_input:
            yield "é”™è¯¯ï¼ç”Ÿæˆæ–‡æœ¬æ‘˜è¦æ¨¡å¼ä¸‹ä¸æ”¯æŒç”¨æˆ·è¾“å…¥é—®é¢˜ï¼", agent.chat_history
            return


    if mode=="èŠå¤©æ¨¡å¼" or mode=="æŒ‡å®šæ¨¡å¼":
        output_stream = agent.chat_qa_chain_self_answer(question_input, chatbot, model_type, model, model_dir, context_window,quantization_config,
                                                        embedding_type, embedding,embedding_dir,
                                                        temperature, top_k_llm, history_len, max_tokens, top_p,
                                                        top_k_query, score_threshold, fecth_k, lambda_mult, search_type,
                                                        chat_mode, is_abstract, is_with_history,
                                                        llm_key, llm_base, DB, upload_files, rag_files, rag_config,web_config,websearch_max_results,tools,
                                                        reranker_config,)

    elif mode=="æ™ºèƒ½ä½“æ¨¡å¼":
        output_stream = agent.agent_answer(question_input, chatbot,
                                           model_type=model_type,
                                           model=model,
                                           model_dir=model_dir,
                                           embedding_dir=embedding_dir,# åªåœ¨æœ¬åœ°æ¨¡å‹æ—¶èµ·ä½œç”¨ ?
                                           quantization_config=quantization_config,  # åªåœ¨æœ¬åœ°æ¨¡å‹æ—¶èµ·ä½œç”¨ ?
                                           temperature=temperature,
                                           top_k_llm=top_k_llm,
                                           top_p=top_p,
                                           max_tokens=max_tokens,
                                           api_key=llm_key,
                                           api_base=llm_base,
                                           tools=tools,
                                           is_with_history=is_with_history,
                                           embedding_type=embedding_type,
                                           embedding=embedding,
                                           DB=DB,
                                           context_window=context_window,
                                           search_type=search_type,
                                           top_k_query=top_k_query,
                                           score_threshold=score_threshold,
                                           fetch_k=fecth_k,
                                           lambda_mult=lambda_mult,
                                           rag_files=rag_files,
                                           upload_files=upload_files,
                                           agent_output_mode=agent_output_mode,
                                           chat_mode=chat_mode,
                                           history_len=history_len

                                           )
    for info, chat_history in output_stream:
        yield info, chat_history

def load_llm(model_type,model_dir,model,local_llm,quantization,llm_max_tokens_dict):
    model=local_llm if local_llm else model
    model_dir=model_dir if model_dir else default_model_dir[model_type]
    model_dir=normalize_path(model_dir)
  #  model_path=os.path.join(model_dir,model)
    model_name=model+"_"+quantization if model_type in ["HuggingFace","llama_cpp" ] else model
    if model_name in models_cache[model_type].get(model_dir,{}):
        return f"æ¨¡å‹{model}åŠ è½½æˆåŠŸ",gr.update()
    else:
        try:
            flag=True
            model_list=list(get_model_list(model_dir,model_type,llm_max_tokens_dict)[model_type].keys())
            if model not in model_list:
                flag,info=download_llm(model_type,model,model_dir)
                if not flag:
                    return gr.update(value=info), gr.update()
                model_list = list(get_model_list(model_dir, model_type, llm_max_tokens_dict)[model_type].keys())
            model_to_llm(model_type,model,model_dir,quantization)

            if flag:
                info=f"æ¨¡å‹{model}åŠ è½½æˆåŠŸ"

            return gr.update(
                value=info), gr.update(choices=model_list, value=model if model_list else None),
        except Exception as e:
            print(e)
            return gr.update(value=f"æ¨¡å‹åŠ è½½å‡ºé”™ï¼Œé”™è¯¯æ˜¯ï¼š{e}"),gr.update()

def  load_embedding(embedding_type,embedding_dir,embedding,local_embedding):
    embedding=  local_embedding if local_embedding  else embedding
    embedding_dir = embedding_dir if embedding_dir else default_model_dir[embedding_type]
    embedding_dir = normalize_path(embedding_dir)
    embedding_path = os.path.join(embedding_dir,embedding)

    if embedding_type in ["OllamaEmbedding","HuggingFaceEmbedding"] and embedding in models_cache[embedding_type].get(embedding_dir, {}):
        return gr.update(value=f"åµŒå…¥æ¨¡å‹{embedding}åŠ è½½æˆåŠŸ"),gr.update(value=embedding )
    else:
        try:
            flag=True
            model_list=get_model_list(embedding_dir,embedding_type)
            if embedding not in model_list: #ä¸‹è½½
                flag,info = download_embedding(embedding_type,embedding,embedding_dir)
                if not flag:
                    return gr.update(value=info), gr.update()
                model_list.append(embedding)
            # print("-2")
            embedding_model,*_=get_embedding(embedding_type=embedding_type,embedding=embedding,embedding_dir=embedding_dir)

            embedding_model.embed_query("1")  # çœŸæ­£çš„é¢„åŠ è½½ï¼Œåœ¨get_embeddingé¢„åŠ è½½ï¼Œä¼šå¯¼è‡´ä½¿ç”¨Chroma ä¼ å…¥embeddingFunctionå°±åˆå§‹åŒ–
            #æˆ‘ä»¬éœ€è¦è°ƒç”¨æ‰åˆå§‹åŒ–ï¼Œé™ä½æ˜¾å­˜å ç”¨
            models_cache[embedding_type][embedding_dir][embedding] = [embedding_model]
            if flag:
                info=f"æ¨¡å‹{embedding}åŠ è½½æˆåŠŸ"

            # model_list=get_model_list(embedding_dir,embedding_type)
            return gr.update(value=info),gr.update(choices=model_list,value=embedding if model_list else None)


        except Exception as error:
            return gr.update(value=f"æ¨¡å‹åŠ è½½å‡ºé”™ï¼Œé”™è¯¯æ˜¯ï¼š{error}"),gr.update()


def clear_history(state):

    ChatAgents[state["session_id"]].clear_history()

# def keep_alive(request:gr.Request):
#     if request:
#         ChatAgents[request.session_hash].keep_alive()
def update_search_config(search_type: str):
    if search_type == "ä½™å¼¦ç›¸ä¼¼åº¦":
        return gr.update(visible=True), gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)
    elif search_type == "MMRç®—æ³•":
        return gr.update(visible=True), gr.update(visible=False), gr.update(visible=True), gr.update(visible=True)

def update_model_dir(is_model_dir):
    if is_model_dir:
        return gr.update(visible=True)
    else:
        return gr.update(visible=False)
def update_embedding_model_dir(is_embedding_model_dir):
    if is_embedding_model_dir:
        return gr.update(visible=True)
    else:
        return gr.update(visible=False)
def start_llamafactory_webui(request:gr.Request,fine_tuning):
    #éœ€è¦æŒ‡å®šç«¯å£ï¼Œé¿å…å’Œragåº”ç”¨å†²çª
    # å¯åŠ¨ llamafactory-cli webui å‘½ä»¤
    def is_port_open(host, port, timeout=1.0):
        try:
            with socket.create_connection((host, port), timeout=timeout):
                return True
        except Exception:
            return False

    session_id=request.session_hash
    if fine_tuning=="å¯åŠ¨llamaFactoryå¾®è°ƒ":
        try:
            gr.Info("âœ… LLaMA-Factory WebUI æ­£åœ¨åå°å¯åŠ¨",duration=20)
            os.environ["GRADIO_SERVER_PORT"]="7880"  # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ŒæŒ‡å®šç«¯å£

            llama_processes[session_id]=subprocess.Popen(
                ["llamafactory-cli", "webui"],
                stdout=open("fine_tuning/llama_webui.log", "w"),
                stderr=subprocess.STDOUT
            )
            for _ in range(60): #20så†…åˆ¤æ–­æ˜¯å¦æˆåŠŸå¯åŠ¨
                if is_port_open("127.0.0.1", 7880):
                    gr.Info("âœ… LLaMA-Factory WebUI å·²åœ¨åå°å¯åŠ¨ï¼šhttp://localhost:7880")
                    print("æˆåŠŸ")
                    return gr.update(value="å…³é—­llamaFactoryå¾®è°ƒ")
                time.sleep(1)
            print("è¶…æ—¶")
            gr.Error("âŒ WebUI å¯åŠ¨è¶…æ—¶ï¼Œè¯·æ£€æŸ¥æ—¥å¿— llama_webui.log")
            return gr.update(value="å¯åŠ¨llamaFactoryå¾®è°ƒ")
        except Exception as e:
            gr.Error(f"âŒ LLaMA-Factoryå¯åŠ¨å‡ºé”™ï¼š{str(e)}")
            return gr.update(value="å¯åŠ¨llamaFactoryå¾®è°ƒ")
    elif fine_tuning == "å…³é—­llamaFactoryå¾®è°ƒ":
        try:
            proc = llama_processes.get(session_id)
            if proc:
                proc.terminate()  # æˆ–ï¼šproc.send_signal(signal.SIGINT)
                proc.wait() #é˜»å¡å½“å‰ä¸»çº¿ç¨‹ï¼Œç›´åˆ°å­çº¿ç¨‹procé€€å‡º
                gr.Info(f"âœ… LLaMA-Factory WebUI å·²æˆåŠŸå…³é—­")
                del llama_processes[session_id]
            else:
                gr.Warning(f"âš ï¸  æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„ WebUI è¿›ç¨‹")
            return gr.update(value="å¯åŠ¨llamaFactoryå¾®è°ƒ")
        except Exception as e:
            gr.Error(f"âŒ  å…³é—­llamaFactory Web UIå¤±è´¥ï¼š{e}")
            return gr.update(value="å…³é—­llamaFactoryå¾®è°ƒ")
    return None


def Rag_assistant_block(state):
    with (gr.Column(visible=False) as demo):
        knowledgeDB = KnowledgeDB()
        DB = gr.State(knowledgeDB)  # åˆ›å»ºä¸€ä¸ªçŠ¶æ€å˜é‡ï¼ˆä¸ºæ¯ä¸ªä¼šè¯ç»´æŠ¤ä¸€ä¸ªçŸ¥è¯†åº“ï¼‰ï¼Œç”¨äºå­˜å‚¨çŸ¥è¯†åº“å¯¹è±¡

        llm_model_maxtokens_dict = gr.State( copy.deepcopy(LLM_MODEL_MAXTOKENS_DICT) )

        # åˆ›å»ºç•Œé¢
        with gr.Row(equal_height=True):
            with gr.Column(scale=2):

                theme_block(scale=2)
                gr.Row(scale=2)
            gr.Column(scale=6)          #ä¸ºäº†ä½¿å›¾ç‰‡å‡ºç°åœ¨åˆé€‚çš„ä½ç½®ï¼Œç”¨columnå ä½
            with gr.Column(scale=2):
                gr.Image(value=LOGO_PATH, height=100, scale=1, show_label=False, show_download_button=False,
                         container=False)
                title = gr.Markdown(f"""<center><b>ç”¨æˆ·</b>ï¼š</center>    """,)

            gr.Column(scale=5)
            with gr.Column(scale=2):

                logout_button = gr.Button("é€€å‡º", scale=2,  # min_width=button_size["small"],
                                          variant="huggingface")
                gr.Row(scale=2)

        with gr.Row():
            with gr.Column(scale=4):
                chatbot = gr.Chatbot(
                    height=800,
                    show_copy_button=True,
                    show_share_button=True,
                    avatar_images=(USER_PATH, AI_PATH),
                    latex_delimiters=[
                        {"left": "$$", "right": "$$", "display": True},
                        {"left": "$", "right": "$", "display": False},
                        {"left": "\\[", "right": "\\]", "display": True},
                        {"left": "\\(", "right": "\\)", "display": False}
                    ]
                )

                # åˆ›å»ºä¸€ä¸ªæ–‡æœ¬æ¡†ç»„ä»¶ï¼Œç”¨äºè¾“å…¥ promptã€‚
                with gr.Row():
                    with gr.Column(scale=8):
                        question_input = gr.Textbox(label="ç”¨æˆ·è¾“å…¥", placeholder="è¾“å…¥ä½ çš„é—®é¢˜...")
                        with gr.Row():
                           # ä¸»è¦å†…å®¹å®½ä¸€äº›
                           with gr.Column(scale=5):
                               gr.Markdown("ğŸ’¡**æ”¯æŒpythonè‡ªå®šä¹‰å‡½æ•°ã€å®ä¾‹æ–¹æ³•ã€å’Œlangchainå·¥å…·ç±»(BaseTool, BaseToolkit)**")
                           with gr.Column(scale=1):  # å³ä¾§æŒ‰é’®å ç”¨è¾ƒå°ç©ºé—´
                                registerToolButton=gr.Button("ğŸ“Œæ³¨å†Œå·¥å…·", scale=1,size="sm")  # å°æŒ‰é’®
                        function_call = gr.Code(label="å‡½æ•°è°ƒç”¨(é€‰å¡«)",
                                               # info="",
                                                   interactive=True,lines=5,language="python",show_line_numbers=True,autocomplete=True)

                        inside_function=gr.Dropdown(label="å¯é€‰çš„å·¥å…·è°ƒç”¨(é€‰å¡«)",interactive=True,multiselect=True,
                                                   choices=list(tools.keys()))
                        info_box = gr.Textbox(label="æç¤ºä¿¡æ¯", interactive=False)
                    with gr.Column(scale=2):
                        with gr.Row():
                            mode=gr.Radio(label="æ¨¡å¼",choices=["èŠå¤©æ¨¡å¼","æ™ºèƒ½ä½“æ¨¡å¼","æŒ‡å®šæ¨¡å¼"],value="èŠå¤©æ¨¡å¼")
                        with gr.Row():
                            is_with_history = gr.Checkbox(label="ä½¿ç”¨è®°å¿†", value=False)
                        with gr.Row():
                            agent_output_mode=gr.Radio(label="è¾“å‡ºæ¨¡å¼",choices=["æ ‡å‡†è¾“å‡º","Reactè¾“å‡º"],value="æ ‡å‡†è¾“å‡º",visible=False)
                        with gr.Row():
                            chat_mode = gr.CheckboxGroup(label="èŠå¤©é…ç½®", choices=chat_config[:-1], value=None)
                            special_mode = gr.Radio(label="å¯é€‰åŠŸèƒ½",choices=["æ–‡æœ¬æ‘˜è¦"], value=None,visible=False)


                        rag_config = gr.Radio(label="æœç´¢é…ç½®", choices=["å¿«é€Ÿæœç´¢", "é«˜çº§æœç´¢"], value="å¿«é€Ÿæœç´¢",
                                              visible=False)
                        reranker_config=gr.Radio(label="é‡æ’åºè®¾ç½®",choices=["äº¤å‰ç¼–ç å™¨é‡æ’","LLMé‡æ’"],value="LLMé‡æ’",visible=False)
                        web_config=gr.Radio(label="è”ç½‘æœç´¢å¼•æ“",choices=["duckduckgo","serperapi"],value="serperapi",visible=False)#å¼ƒç”¨ä¸é€‰æ‹©æœç´¢apié»˜è®¤é€‰æ‹©duduckGoï¼Œä½¿å…¶ä¸å¯è§
                        # abstract_mode = gr.Radio(label="æ‘˜è¦ç”Ÿæˆæ–¹å¼", value="çŸ­æ–‡æœ¬ç²¾è¯»",
                        #                          choices=["çŸ­æ–‡æœ¬ç²¾è¯»", "é•¿æ–‡æœ¬ç•¥è¯»", "é•¿æ–‡æœ¬ç²¾è¯»"], visible=False)
                        #

                # abstract_button=gr.Button(value="ç”Ÿæˆæ–‡æ¡£æ‘˜è¦",variant="primary")
                with gr.Row():
                    chat_button = gr.Button("Chat", variant="primary")
                    # åˆ›å»ºä¸€ä¸ªæ¸…é™¤æŒ‰é’®ï¼Œç”¨äºæ¸…é™¤èŠå¤©æœºå™¨äººç»„ä»¶çš„å†…å®¹ã€‚
                    clear = gr.ClearButton(
                        components=[chatbot], value="Clear", variant="primary")


            with gr.Column(scale=2):
                model_argument = gr.Accordion("LLMå‚æ•°é…ç½®", open=True)

                with model_argument:
                    temperature = gr.Slider(0,
                                            1,
                                            value=0.01,
                                            step=0.01,
                                            label="llm temperature",
                                            interactive=True)

                    top_k_llm = gr.Slider(1,
                                          10,
                                          value=3,
                                          step=1,
                                          label="top_k",
                                          interactive=True,
                                          visible=False)
                    top_p = gr.Slider(0,
                                      1,
                                      value=0.95,
                                      step=0.01,
                                      label="top_p",
                                      interactive=True,
                                      visible=False)
                    max_tokens = gr.Slider(256,
                                           4096,  # éšæ¨¡å‹è¾“å‡ºèƒ½åŠ›ä¸åŒè€Œæ”¹å˜ï¼Œå…·ä½“çœ‹globals
                                           value=2048,
                                           step=256,
                                           label="æ¨¡å‹æœ€å¤§è¾“å‡ºtokens",
                                           interactive=True
                                           )
                    history_len = gr.Slider(0,
                                            10,
                                            value=3,
                                            step=1,
                                            label="history length",
                                            interactive=True)

                chat_model_select = gr.Accordion("èŠå¤©æ¨¡å‹é€‰æ‹©")  #

                with chat_model_select:
                    model_type = gr.Dropdown(
                        list(llm_model_maxtokens_dict.value.keys()),
                        label="companys",
                        value="OPENAI",
                        interactive=True)

                    llm = gr.Dropdown(choices=llm_model_maxtokens_dict.value["OPENAI"].keys(),
                                      label="large language model",
                                      value=INIT_LLM, #gpt3.5-turbo
                                      interactive=True)

                    llm_dir = gr.Textbox(label="æ¨¡å‹ç›®å½•", visible=False, interactive=True,lines=2,
                                         value=None)
                    local_llm = gr.Textbox(label="æœ¬åœ°æ¨¡å‹",
                                           info="æ”¯æŒæ¨¡å‹åç§°\næ¨¡å‹ä¸å­˜åœ¨æ—¶ï¼Œå¯ä»¥é€‰æ‹©æ˜¯å¦æ ¹æ®æ¨¡å‹åç§°ä¸‹è½½",
                                           value="", interactive=True)
                    quantization_config = gr.Radio(label="é‡åŒ–é…ç½®",choices=quantization_choices["HuggingFace"],value="fp16",visible=False,interactive=True)

                    with gr.Row():

                        is_model_dir=gr.Checkbox(label="æŒ‡å®šæœ¬åœ°æ¨¡å‹ç›®å½•",value=False,visible=False)
                        is_download_llm = gr.Button(value="åŠ è½½æ¨¡å‹",visible=False)
                        is_empty_cache=gr.Button(value="æ¸…ç©ºæ˜¾å­˜",visible=True
                                                 #size="sm",
                                                 #min_width=button_size["small"]
                                                 )

                    llm_key = gr.Textbox(label="llm key", value=None, type="password", visible=False)  # åªå¯¹ç”¨æˆ·å¯è§
                    base_url = gr.Textbox(label="llm base url", value=None, visible=False)

                template = gr.Textbox(label="æç¤ºè¯æ¨¡æ¿", info="System Message",
                                      placeholder="ä½¿ç”¨få­—ç¬¦ä¸²çš„æ ¼å¼å¡«å†™æç¤ºè¯æ¨¡æ¿",
                                      value=default_template, interactive=True, lines=15)  # å…è®¸ç”¨æˆ·è‡ªå®šä¹‰æç¤ºè¯
              #
                fine_tuning=gr.Button(value="å¯åŠ¨llamaFactoryå¾®è°ƒ",variant="huggingface")

        with gr.Row():
            # ä¸Šä¼ æ–‡ä»¶
            with gr.Column(scale=3):
                file = gr.File(label='è¯·é€‰æ‹©çŸ¥è¯†åº“ç›®å½•', file_count='multiple',
                               file_types=['.txt', '.md', '.docx','.doc','.pdf'])
                with gr.Row():
                    init_db = gr.Button("ğŸ’¨æ·»åŠ æ–‡ä»¶")  # åˆæ¬¡æ·»åŠ æ–‡ä»¶
                    upd_db = gr.Button("æ›´æ–°æ–‡ä»¶")  # æ›´æ–°å·²æœ‰æ–‡ä»¶
                    del_db = gr.Button("ğŸ—‘ï¸ åˆ é™¤æ–‡ä»¶")  # åˆ é™¤æ–‡ä»¶
                msg_db = gr.Textbox(label="æç¤ºä¿¡æ¯", value=None, interactive=False, visible=True, scale=3)

            # æ•°æ®åº“é€‰æ‹©å’Œæ–‡ä»¶æœç´¢ï¼š
            with gr.Column(scale=2):
                # æ—¢æ˜¯è¾“å‡ºä¹Ÿæ˜¯è¾“å‡ºç»„ä»¶ï¼Œå¯ä»¥æ˜¾ç¤ºå½“å‰å·²ç»å­˜åœ¨çš„çŸ¥è¯†åº“ï¼Œä¹Ÿå¯ä»¥æŒ‡å®šæ£€ç´¢çš„æ–‡ä»¶
                embedding_model_select = gr.Accordion("Embeddingæ¨¡å‹")
                with embedding_model_select:
                    embedding_type = gr.Dropdown(EMBEDDING_MODEL_DICT,
                                                 label="companys",
                                                 value="OPENAI")
                    embedding = gr.Dropdown(EMBEDDING_MODEL_DICT["OPENAI"],
                                            label="Embedding model",
                                            value="text-embedding-ada-002",
                                            interactive=True)
                    embedding_model_dir = gr.Textbox(label="embeddingæ¨¡å‹ç›®å½•", visible=False, interactive=True,
                                                     value=None)
                    local_embedding_model = gr.Textbox(label="æœ¬åœ°embeddingæ¨¡å‹",
                                                       info="æ”¯æŒæ¨¡å‹åç§°\næ¨¡å‹ä¸å­˜åœ¨æ—¶ï¼Œå¯ä»¥é€‰æ‹©æ˜¯å¦æ ¹æ®æ¨¡å‹åç§°ä¸‹è½½",
                                                       value="", interactive=True, visible=False)

                    with gr.Row():
                        is_download_embedding = gr.Button(value="åŠ è½½æ¨¡å‹", visible=False)
                        is_embedding_model_dir = gr.Checkbox(label="æŒ‡å®šæœ¬åœ°æ¨¡å‹ç›®å½•", value=False,
                                                             visible=False)
                search_file = gr.Textbox(label="æ–‡ä»¶å", value=None)
                search_button = gr.Button("æŸ¥è¯¢æ–‡ä»¶")

            with gr.Column(scale=2):
                embedding_config = gr.Accordion("Embeddingå‚æ•°é…ç½®", open=True)
                with embedding_config:
                    top_k_query = gr.Slider(1,
                                            10,
                                            value=3,
                                            step=1,
                                            label="top_k",
                                            interactive=True)
                    score_threshold = gr.Slider(0,
                                                1,
                                                value=0.4,
                                                step=0.01,
                                                label="score threshold",
                                                interactive=True)
                    websearch_max_results=gr.Slider(1,
                                                   10,
                                                   value=3,
                                                   step=1,
                                                   label="websearch_max_results",
                                                   interactive=True,
                                                   visible=True)
                    fetch_k = gr.Slider(5,
                                        30,
                                        value=20,
                                        step=1,
                                        label="fetch_k",
                                        interactive=True,
                                        visible=False)
                    lambda_mult = gr.Slider(0,
                                            1,
                                            value=0.5,
                                            step=0.01,
                                            label="lambda_mult",
                                            visible=False,
                                            interactive=True)
                search_type = gr.Radio(choices=["ä½™å¼¦ç›¸ä¼¼åº¦", "MMRç®—æ³•"], value="ä½™å¼¦ç›¸ä¼¼åº¦", label="æ£€ç´¢æ–¹å¼",
                                       interactive=True)
                key_input = gr.Accordion(label="embeddingæ¨¡å‹API_KEY")
                with key_input:
                    embedding_key = gr.Textbox(label="embedding key", value=None, type="password", visible=False)
                    # åªå¯¹spark embeddingå¯è§
                    spark_app_id = gr.Textbox(label="spark app id", value=None, type="password", visible=False)
                    spark_api_secret = gr.Textbox(label="spark api secret", value=None, type="password", visible=False)
                    wenxin_secret = gr.Textbox(label="wenxin api secret", value=None, type="password", visible=False)




        with gr.Row():  # æ–‡ä»¶åˆ—è¡¨

            db_file = gr.CheckboxGroup(
                choices=DB.value.files.get("text-embedding-ada-002", []),
                label="ğŸ—‚ï¸æ–‡ä»¶åˆ—è¡¨",
                interactive=True,
                scale=8
            )

        with gr.Row():  # ç¿»é¡µåŠŸèƒ½
            all_button = gr.Button(value="âœ”å…¨é€‰", scale=1,
                                   #size="sm",
                                 #  min_width=button_size["small"]
                                   )
            cancel_button = gr.Button(value="âŒå–æ¶ˆ",scale=1,
                                      #size="sm", min_width=button_size["small"]
                                      )
            gr.Column(scale=8)
            last_page = gr.Button(value="ğŸ”¼ä¸Šä¸€é¡µ", scale=1,
                                #  size="sm",
                                 # min_width=button_size["small"]
                                  )
            page = gr.Textbox(label="é¡µæ•°", scale=1,
                             # min_width=button_size["small"]
                              )  #

            next_page = gr.Button(value="ğŸ”½ä¸‹ä¸€é¡µ", scale=1,
                                  #size="sm",
                                  #min_width=button_size["small"]
                                  )


        model_type.change(update_llm_dropdown, inputs=[model_type,llm_dir,llm_model_maxtokens_dict] ,outputs=[llm, top_p, top_k_llm,llm_dir,
                                                                                            local_llm,is_model_dir,is_download_llm])  # æ›´æ–°LLMä¸‹æ‹‰é€‰é¡¹
        llm.change(update_llm_config, inputs=[model_type, llm,llm_model_maxtokens_dict,mode,chat_mode,llm_dir], outputs=[max_tokens,chat_mode,quantization_config])
        embedding_type.change(update_embedding_dropdown, inputs=[embedding_type, state,embedding_model_dir],
                              outputs=[embedding, spark_app_id, spark_api_secret, wenxin_secret, local_embedding_model,
                                       is_embedding_model_dir,is_download_embedding,embedding_model_dir])
        embedding.change(update_file, inputs=[embedding_type,embedding,embedding_model_dir, DB], outputs=[db_file, msg_db,DB])
        # ç”¨æˆ·æ‰æœ‰keyæ˜¾ç¤ºï¼Œå½“keyå˜åŒ–äº†å‚¨å­˜åœ¨çŸ¥è¯†åº“å†…ä¾›APIä½¿ç”¨ï¼›æ¸¸å®¢ä¸æ˜¾ç¤ºé»˜è®¤ä¸ä¼šchangeï¼Œæ‰€ä»¥DBå‚¨å­˜çš„keyæ˜¯ç¼ºçœçš„Noneï¼Œè§£æenvçš„key
        embedding_key.change(fn=update_embedding_key, inputs=[embedding_key, DB], outputs=[DB])
        template.change(fn=update_template, inputs=[template, state])

        wenxin_secret.change(fn=update_wenxin_secret, inputs=[wenxin_secret, DB], outputs=[DB])
        spark_app_id.change(fn=update_spark_app_id, inputs=[spark_app_id, DB], outputs=[DB])
        spark_api_secret.change(fn=update_spark_api_secret, inputs=[spark_api_secret, DB], outputs=[DB])

        search_type.change(update_search_config, inputs=[search_type], outputs=[top_k_query, score_threshold,
                                                                                fetch_k, lambda_mult])

        llm_dir.submit(update_model_list,inputs =[llm_dir,model_type,llm_model_maxtokens_dict],
                       outputs=[llm])
        embedding_model_dir.submit(update_embedding_list,inputs=[embedding_model_dir,embedding_type],outputs=[embedding])
        is_empty_cache.click(empty_gpu_memory,outputs=[info_box]) #æ¸…ç©ºæ˜¾å­˜æ—¶æ˜¾ç¤ºæ¨¡å‹ç›®å½•
        all_button.click(choose_all, inputs=[DB, embedding], outputs=[db_file])
        cancel_button.click(cancel_all, inputs=[], outputs=[db_file])
        is_model_dir.change(update_model_dir,inputs=[is_model_dir],outputs=[llm_dir])
        is_embedding_model_dir.change(update_embedding_model_dir,inputs=[is_embedding_model_dir],outputs=[embedding_model_dir])

        is_download_llm.click(load_llm,inputs=[model_type,llm_dir,llm,local_llm,quantization_config,llm_model_maxtokens_dict],outputs=[info_box,llm])
        is_download_embedding.click(load_embedding,inputs=[embedding_type,embedding_model_dir,embedding,local_embedding_model]
                                    ,outputs=[msg_db,embedding])
        mode.change(update_mode, inputs=[mode, llm,inside_function],
                    outputs=[chat_mode, special_mode, function_call, inside_function, agent_output_mode]) \
            .then(lambda mode, chat_mode, rag_config:
                  (gr.update(visible=(mode == "èŠå¤©æ¨¡å¼" and "ä½¿ç”¨çŸ¥è¯†åº“" in chat_mode)),
                   gr.update(
                       visible=(mode == "èŠå¤©æ¨¡å¼" and "ä½¿ç”¨çŸ¥è¯†åº“" in chat_mode and rag_config == "é«˜çº§æœç´¢"))
                   ), inputs=[mode, chat_mode, rag_config], outputs=[rag_config, reranker_config])

        chat_mode.change(lambda mode, chat_mode, rag_config:
                  (gr.update(visible=(mode == "èŠå¤©æ¨¡å¼" and "ä½¿ç”¨çŸ¥è¯†åº“" in chat_mode)),
                   gr.update(
                       visible=(mode == "èŠå¤©æ¨¡å¼" and "ä½¿ç”¨çŸ¥è¯†åº“" in chat_mode and rag_config == "é«˜çº§æœç´¢"))
                   ), inputs=[mode, chat_mode, rag_config], outputs=[rag_config, reranker_config])
        rag_config.change(
            lambda mode, chat_mode, rag_config: gr.update(visible=(mode == "èŠå¤©æ¨¡å¼" and "ä½¿ç”¨çŸ¥è¯†åº“" in chat_mode
                                                                   and rag_config == "é«˜çº§æœç´¢")),
            inputs=[mode, chat_mode, rag_config], outputs=[reranker_config])




        init_db.click(create_db_info,  #
                      inputs=[file, embedding_type, embedding, DB,
                              embedding_key, base_url, spark_app_id, spark_api_secret, wenxin_secret,local_embedding_model,embedding_model_dir],
                      outputs=[msg_db, db_file, DB])
        fine_tuning.click(start_llamafactory_webui,inputs=[fine_tuning],outputs=[fine_tuning])#å¯åŠ¨llamaFactoryåº”ç”¨
        del_db.click(delete_db, inputs=[db_file, embedding_type, embedding, DB,local_embedding_model,embedding_model_dir], outputs=[msg_db, db_file, DB])
        upd_db.click(update_db, inputs=[db_file, file, embedding_type, embedding, DB,local_embedding_model,embedding_model_dir], outputs=[msg_db, db_file, DB])
        search_button.click(search_db, inputs=[search_file, embedding, local_embedding_model,DB], outputs=[msg_db, db_file])
        # è®¾ç½®æŒ‰é’®çš„ç‚¹å‡»äº‹ä»¶ã€‚å½“ç‚¹å‡»æ—¶ï¼Œè°ƒç”¨ä¸Šé¢å®šä¹‰çš„ chat_qa_chain_self_answer å‡½æ•°ï¼Œå¹¶ä¼ å…¥ç”¨æˆ·çš„æ¶ˆæ¯å’ŒèŠå¤©å†å²è®°å½•ï¼Œç„¶åæ›´æ–°æ–‡æœ¬æ¡†å’ŒèŠå¤©æœºå™¨äººç»„ä»¶ã€‚


        temp_msg=gr.State("")#å‘é€æ¶ˆæ¯åé©¬ä¸Šæ¸…ç©ºï¼Œåç»­answerè¾“å…¥ä»temp_msgï¼Œå®ç°â€œå‘é€â€æ–‡æœ¬çš„å‰ç«¯æ•ˆæœ
        registerToolButton.click(lambda fc,ins,states: update_tool_list(fc,ins,states),inputs=[function_call,inside_function,state],
                              outputs=[inside_function])
        chat_button.click(lambda msg: (msg,""),inputs=[question_input],outputs=[temp_msg,question_input]).then(lambda fc,ins,states: update_tool_list(fc,ins,states),inputs=[function_call,inside_function,state],
                              outputs=[inside_function]
                          ).then(chat_qa_chain_answer, inputs=[
            temp_msg, mode,chatbot, model_type, llm, llm_dir,llm_model_maxtokens_dict,quantization_config,
            embedding_type, embedding,embedding_model_dir,
            temperature, top_k_llm, history_len, max_tokens, top_p,
            top_k_query, score_threshold, fetch_k, lambda_mult, search_type, chat_mode, special_mode, is_with_history,
            llm_key, base_url, DB, file, db_file, rag_config, web_config,  websearch_max_results,inside_function,state,agent_output_mode,
        reranker_config],
                          outputs=[info_box, chatbot])

        question_input.submit(lambda msg: (msg,""),inputs=[question_input],outputs=[temp_msg,question_input]).then(lambda fc,ins,states: update_tool_list(fc,ins,states),
                                                                    inputs=[function_call,inside_function,state],outputs=[inside_function]).then(chat_qa_chain_answer, inputs=[
            temp_msg, mode,chatbot, model_type, llm, llm_dir,llm_model_maxtokens_dict,quantization_config,
            embedding_type, embedding,embedding_model_dir,temperature, top_k_llm, history_len, max_tokens, top_p,
            top_k_query, score_threshold, fetch_k, lambda_mult, search_type, chat_mode, special_mode, is_with_history,
            llm_key, base_url, DB, file, db_file, rag_config, web_config,  websearch_max_results,inside_function,state,agent_output_mode
        ,reranker_config],
                          outputs=[info_box, chatbot])

        # ç‚¹å‡»åæ¸…ç©ºåç«¯å­˜å‚¨çš„èŠå¤©è®°å½•
        clear.click(clear_history, inputs=[ state])  # ä¸åŒæ¸¸å®¢çš„agentä¹Ÿæ˜¯è¦éš”ç¦»çš„



        gr.Markdown("""æé†’ï¼š<br>    
                         1. ä½¿ç”¨çŸ¥è¯†åº“æ£€ç´¢æ—¶è¯·å…ˆä¸Šä¼ æ–‡ä»¶ã€‚
                         2. OpenAI APIé‡‡ç”¨çš„æ˜¯ç¬¬ä¸‰æ–¹æœåŠ¡å™¨ï¼Œå¯èƒ½å­˜åœ¨æ— æ³•ä½¿ç”¨çš„é—®é¢˜<br>
                         3.Gemini å’ŒGROK APIè°ƒç”¨éœ€è¦ç§‘å­¦ä¸Šç½‘ <br>
                         4.APIè°ƒç”¨å¤±è´¥å¯èƒ½æ˜¯é¢åº¦ç”¨å®Œæˆ–æœåŠ¡å™¨ä¸å†ç»´æŠ¤è¯¥æ¨¡å‹
                         """)

    return MyBlock(demo, ("logout_button", logout_button),
                   ("llm_key", llm_key),
                   ("base_url", base_url),
                   ("embedding_key", embedding_key),
                   ("spark_app_id", spark_app_id),
                   ("spark_api_secret", spark_api_secret),
                   ("title", title),
                   ("knowledgeDB", DB),
                   ("db_file", db_file),
                   ("wenxin_secret", wenxin_secret),
                   )  # è¿”å›æ‰€æœ‰å­˜åœ¨valueçš„ç»„ä»¶ï¼Œåœ¨é¡µé¢è·³è½¬è¿‡ç¨‹ä¹‹ä¸­ï¼Œéœ€è¦æ¸…ç©ºvalue


def run_rag_assistant(state, login_block: MyBlock, register_block: MyBlock, app_block: MyBlock):
    #  state=gr.State({"username":{},"logged_in":False})
    log_button2 = register_block["log_button2"]
    login_button1: gr.Button = login_block["login_button1"]
    reg_button1 = login_block["reg_button1"]
    guest_button = login_block["guest_button"]
    reg_button2 = register_block["reg_button2"]
    logout_button = app_block["logout_button"]

    # è¿™é‡Œä»£ç å¯ä»¥ä¼˜åŒ–ä¸ºä»¥blockä¸ºå•ä½æ›´æ–°å†…éƒ¨ç»„ä»¶ï¼Œç„¶åå†è¿”å›æ–°çš„blockï¼Œè€Œä¸æ˜¯ç»„ä»¶æ›´æ–°ã€‚
    login_button1.click(
        login,
        inputs=[login_block["login_username"], login_block["login_password"], state, app_block["knowledgeDB"]],
        outputs=[login_block["login_output"], state, login_block.block, app_block.block,
                 app_block["llm_key"], app_block["base_url"], app_block["embedding_key"], app_block["title"],
                 app_block["knowledgeDB"], app_block["db_file"]]
    )  # ç™»å½•

    log_button2.click(loginToRegister, inputs=[],
                      outputs=[register_block.block, login_block.block, login_block["login_output"]])  # è¿”å›ç™»å½•ç•Œé¢

    guest_button.click(guest_login, inputs=[state, app_block["knowledgeDB"]],
                       outputs=[login_block["login_output"]
                           , state, login_block.block, app_block.block, app_block["title"], app_block["knowledgeDB"],
                                app_block["db_file"]])

    reg_button1.click(loginToRegister, inputs=[],
                      outputs=[login_block.block, register_block.block, login_block["login_output"]])  # ç™»å½•è·³è½¬åˆ°æ³¨å†Œç•Œé¢
    reg_button2.click(register, inputs=[register_block["reg_username"], register_block["reg_password1"],
                                        register_block["reg_password2"]],
                      outputs=[register_block["reg_output"], register_block.block, login_block.block,
                               login_block["login_output"]])  # æ³¨å†ŒæˆåŠŸåè·³è½¬åˆ°ç™»å½•ç•Œé¢

    logout_button.click(
        logout,
        inputs=[state],
        outputs=[state, app_block.block, login_block.block, login_block["login_output"]]
    )
