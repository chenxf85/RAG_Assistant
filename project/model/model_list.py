from globals import  LLM_MODEL_MAXTOKENS_DICT,OLLAMA_PORT_MAP
import os
import json
import copy
import subprocess
import ollama

# def read_model_files(root,model_name: str,model_type,llm_maxtokens_dict: dict = None):
#     config_path = os.path.join(root, "config.json")
#     with open(config_path, "r", encoding="utf-8") as f:
#         config_data = json.load(f)
#     max_tokens = config_data.get("max_position_embeddings", 8192)
#     llm_maxtokens_dict[model_type][model_name] = (max_tokens, max_tokens)


import subprocess
from utils.checkPort import getValidPort


def get_ollama_list(model_dir):
    """
    ä½¿ç”¨ subprocess.Popen è·å–æŒ‡å®šç«¯å£ä¸Šçš„ Ollama æ¨¡å‹åˆ—è¡¨ï¼ˆå¼‚æ­¥æ–¹å¼ï¼‰

    Args:
        port: Ollama æœåŠ¡ç«¯å£ï¼ˆé»˜è®¤11434ï¼‰

    Returns:
        list[str]: å½“å‰å¯ç”¨æ¨¡å‹åç§°åˆ—è¡¨
    """
    try:
        # é€šè¿‡ Popen å¯åŠ¨ ollama list å­è¿›ç¨‹
        env=os.environ.copy()
        if model_dir in OLLAMA_PORT_MAP:
            port = OLLAMA_PORT_MAP[model_dir]
            env["OLLAMA_MODELS"] = model_dir
            env["OLLAMA_HOST"] = f"127.0.0.1:{port}"
        else:
            port = getValidPort(port=11500, max_port=11600)
            # --- Step 3. å¯åŠ¨æ–°çš„ Ollama æœåŠ¡ ---
            print(f"ğŸš€ å¯åŠ¨æ–°çš„ Ollama æœåŠ¡ï¼ˆæ¨¡å‹ç›®å½•: {model_dir}, ç«¯å£: {port}ï¼‰")
            env["OLLAMA_MODELS"] = model_dir
            env["OLLAMA_HOST"] = f"127.0.0.1:{port}"
            OLLAMA_PORT_MAP[model_dir] = port
            subprocess.Popen(["ollama", "serve"], env=env,
                             stdout=subprocess.DEVNULL,  # å±è”½æ ‡å‡†è¾“å‡º
                             stderr=subprocess.DEVNULL  # å±è”½é”™è¯¯è¾“å‡º
                             )

        process = subprocess.Popen(
            ["ollama", "list"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            env=env
        )

        # è¯»å–è¾“å‡ºç»“æœ
        stdout, stderr = process.communicate(timeout=5)

        if process.returncode != 0:
            print(f"[è­¦å‘Š] ç«¯å£ {port} çš„ Ollama æœåŠ¡è¿æ¥å¤±è´¥: {stderr.strip()}")
            return []

        # è§£æè¾“å‡º
        lines = stdout.strip().splitlines()
        if not lines or len(lines) < 2:
            return []

        # ollama list è¾“å‡ºç¤ºä¾‹:
        # MODEL             ID              SIZE    MODIFIED
        # mistral:latest    a1b2c3d4        4.1GB   2024-04-01 10:30:00
        models = []
        for line in lines[1:]:
            parts = line.split()
            if parts:
                models.append(parts[0])

        return models

    except subprocess.TimeoutExpired:
        process.kill()
        print(f"[é”™è¯¯] è·å– Ollama æ¨¡å‹åˆ—è¡¨è¶…æ—¶ï¼ˆç«¯å£ {port}ï¼‰")
        return []
    except FileNotFoundError:
        print("[é”™è¯¯] æœªæ‰¾åˆ° ollama å‘½ä»¤ï¼Œè¯·ç¡®è®¤ Ollama å·²æ­£ç¡®å®‰è£…ã€‚")
        return []
    except Exception as e:
        print(f"[é”™è¯¯] è·å– Ollama æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
        return []


def get_model_list(model_dir: str,model_type: str = "HuggingFace",llm_maxtokens_dict: dict = None):
    '''
    è¯»å–model_dirä¸‹æœ€åä¸€çº§ç›®å½•çš„æ¨¡å‹åç§°ï¼Œè¿”å›ä¸€ä¸ªæ¨¡å‹åˆ—è¡¨ï¼Œ
    å¹¶ç”Ÿæˆ model_list.json ç¼“å­˜ max_position_embeddings ä¿¡æ¯
    '''
    if model_type=="Ollama":
        llm_maxtokens_dict[model_type]={}
        for model in get_ollama_list(model_dir):
            llm_maxtokens_dict[model_type][model] = (8192,16384) #Ollmaæ¨¡å‹ä¿¡æ¯æ²¡æœ‰åŒ…å«æœ€å¤§çš„è¾“å‡ºå’Œçª—å£é•¿åº¦ï¼Œè¿™é‡Œç»™ä¸€ä¸ªé»˜è®¤å€¼8192
        return llm_maxtokens_dict
    elif model_type=="HuggingFace" :
        llm_maxtokens_dict[model_type] = {}
        for root, dirs, files in os.walk(model_dir):
            config_path = os.path.join(root, "config.json")
            configuration_path=os.path.join(root, "configuration.json")
            model_name = os.path.relpath(root, model_dir).replace("\\", "/")
            if os.path.exists(config_path) :  # safetensors
                try:
                    with open(config_path, "r", encoding="utf-8") as f:
                        config_data = json.load(f)
                    max_tokens = config_data.get("max_position_embeddings", 8192)
                    llm_maxtokens_dict[model_type][model_name] = (max_tokens, max_tokens)
                except Exception as e:
                    print(f"[è­¦å‘Š] è¯»å– {config_path} å¤±è´¥: {e}")
                    continue
            elif os.path.exists(configuration_path):  # llama.cpp
                llm_maxtokens_dict[model_type][model_name] = (4096, 8192)
        return llm_maxtokens_dict
    elif model_type=="llama_cpp":
        llm_maxtokens_dict[model_type] = {}
        for root, dirs, files in os.walk(model_dir):
            config_path = os.path.join(root, "params")
            configuration_path = os.path.join(root, "configuration.json")
            model_name = os.path.relpath(root, model_dir).replace("\\", "/")
            if os.path.exists(config_path):  # safetensors
                try:
                    with open(config_path, "r", encoding="utf-8") as f:
                        config_data = json.load(f)

                    max_tokens = config_data.get("num_predict", 4096)
                    context_window=config_data.get("num_ctx",8192)

                    llm_maxtokens_dict[model_type][model_name] = (max_tokens,context_window, )
                except Exception as e:
                    print(f"[è­¦å‘Š] è¯»å– {config_path} å¤±è´¥: {e}")
                    continue
            elif os.path.exists(configuration_path):  # llama.cpp
                llm_maxtokens_dict[model_type][model_name] = (4096, 8192)
        return llm_maxtokens_dict
    elif model_type=="OllamaEmbedding":
        return get_ollama_list(model_dir)
    elif model_type=="HuggingFaceEmbedding":
        model_list=[]
        for root, dirs, files in os.walk(model_dir):
            config_path = os.path.join(root, "config.json")
            configuration_path = os.path.join(root, "configuration.json")
            model_name = os.path.relpath(root, model_dir).replace("\\", "/")
            if os.path.exists(config_path) and os.path.exists(configuration_path):  # safetensors
                model_list.append(model_name)

        return model_list








